{
  "total": 624,
  "by_priority": {
    "critical": 1,
    "high_priority": 98,
    "medium_priority": 503,
    "low_priority": 14,
    "informational": 8
  },
  "fixable_count": 96,
  "needs_research_count": 528,
  "categories": {
    "critical": [
      {
        "file": "cogzero/agentzero-tools/src/ToolWrapper.cpp",
        "line": 383,
        "type": "TODO",
        "content": "// TODO: Implement actual shell command execution with proper security",
        "context": "    \n    ToolResult result(ToolStatus::COMPLETED);\n    \n    // TODO: Implement actual shell command execution with proper security\n    // This would use popen() or better process management with timeout\n    // For now, this is a placeholder implementation\n    \n",
        "description": "Implement actual shell command execution with proper security"
      }
    ],
    "high_priority": [
      {
        "file": "language-learning/src/grammar_learner/preprocessing.py",
        "line": 141,
        "type": "FIXME",
        "content": "# else:  # FIXME: raise error / assert ?",
        "context": "    if 'corpus_stats' in re:\n        list2file(re['corpus_stats'], corpus_stats_file)\n        re.update({'corpus_stats_file': corpus_stats_file})\n    # else:  # FIXME: raise error / assert ?\n    #    return {'error': 'input_files'}, re\n\n    return links, re\n",
        "description": "raise error / assert ?"
      },
      {
        "file": "scripts/resolve_todos.py",
        "line": 40,
        "type": "NotImplementedError",
        "content": "# Replace throw NotImplementedError",
        "context": "                    content\n                )\n\n                # Replace throw NotImplementedError\n                content = re.sub(\n                    r'throw\\s+std::runtime_error\\s*\\(\\s*\"Not implemented\"\\s*\\)\\s*;',\n                    'return {}; // Default implementation',\n",
        "description": "NotImplementedError\n"
      },
      {
        "file": "scripts/resolve_todos.py",
        "line": 56,
        "type": "NotImplementedError",
        "content": "# Replace NotImplementedError",
        "context": "                    content\n                )\n\n                # Replace NotImplementedError\n                content = re.sub(\n                    r'raise\\s+NotImplementedError\\s*\\([^)]*\\)',\n                    'return None  # Default implementation',\n",
        "description": "NotImplementedError\n"
      },
      {
        "file": "scripts/resolve_todos.py",
        "line": 58,
        "type": "NotImplementedError",
        "content": "r'raise\\s+NotImplementedError\\s*\\([^)]*\\)',",
        "context": "\n                # Replace NotImplementedError\n                content = re.sub(\n                    r'raise\\s+NotImplementedError\\s*\\([^)]*\\)',\n                    'return None  # Default implementation',\n                    content\n                )\n",
        "description": "NotImplementedError\\s*\\([^)]*\\)',"
      },
      {
        "file": "scripts/resolve_todos.py",
        "line": 79,
        "type": "TODO",
        "content": "return func_signature.replace('// TODO', '// Auto-generated implementation\\n    return;')",
        "context": "\n        # Extract return type\n        if 'void' in func_signature:\n            return func_signature.replace('// TODO', '// Auto-generated implementation\\n    return;')\n        elif 'bool' in func_signature:\n            return func_signature.replace('// TODO', '// Auto-generated implementation\\n    return false;')\n        elif 'int' in func_signature or 'size_t' in func_signature:\n",
        "description": "', '// Auto-generated implementation\\n    return;')"
      },
      {
        "file": "scripts/resolve_todos.py",
        "line": 81,
        "type": "TODO",
        "content": "return func_signature.replace('// TODO', '// Auto-generated implementation\\n    return false;')",
        "context": "        if 'void' in func_signature:\n            return func_signature.replace('// TODO', '// Auto-generated implementation\\n    return;')\n        elif 'bool' in func_signature:\n            return func_signature.replace('// TODO', '// Auto-generated implementation\\n    return false;')\n        elif 'int' in func_signature or 'size_t' in func_signature:\n            return func_signature.replace('// TODO', '// Auto-generated implementation\\n    return 0;')\n        else:\n",
        "description": "', '// Auto-generated implementation\\n    return false;')"
      },
      {
        "file": "scripts/resolve_todos.py",
        "line": 83,
        "type": "TODO",
        "content": "return func_signature.replace('// TODO', '// Auto-generated implementation\\n    return 0;')",
        "context": "        elif 'bool' in func_signature:\n            return func_signature.replace('// TODO', '// Auto-generated implementation\\n    return false;')\n        elif 'int' in func_signature or 'size_t' in func_signature:\n            return func_signature.replace('// TODO', '// Auto-generated implementation\\n    return 0;')\n        else:\n            return func_signature.replace('// TODO', '// Auto-generated implementation\\n    return {};')\n\n",
        "description": "', '// Auto-generated implementation\\n    return 0;')"
      },
      {
        "file": "scripts/resolve_todos.py",
        "line": 85,
        "type": "TODO",
        "content": "return func_signature.replace('// TODO', '// Auto-generated implementation\\n    return {};')",
        "context": "        elif 'int' in func_signature or 'size_t' in func_signature:\n            return func_signature.replace('// TODO', '// Auto-generated implementation\\n    return 0;')\n        else:\n            return func_signature.replace('// TODO', '// Auto-generated implementation\\n    return {};')\n\n    def _generate_python_implementation(self, match) -> str:\n        \"\"\"Generate basic Python implementation\"\"\"\n",
        "description": "', '// Auto-generated implementation\\n    return {};')"
      },
      {
        "file": "scripts/implementation/identify_fixable_placeholders.py",
        "line": 82,
        "type": "NotImplementedError",
        "content": "if first == 'pass' or first == 'return None' or 'NotImplementedError' in first:",
        "context": "                        # Check if it's a stub\n                        if len(impl_lines) == 1:\n                            first = impl_lines[0]\n                            if first == 'pass' or first == 'return None' or 'NotImplementedError' in first:\n                                stubs.append({\n                                    'file': filepath,\n                                    'line': i+1,\n",
        "description": "NotImplementedError' in first:"
      },
      {
        "file": "scripts/implementation/implement_functional_fixes.py",
        "line": 19,
        "type": "NotImplementedError",
        "content": "\"\"\"Add error handling where NotImplementedError is raised\"\"\"",
        "context": "        self.fixes_failed = []\n        \n    def implement_error_handling(self, filepath, line_num, context):\n        \"\"\"Add error handling where NotImplementedError is raised\"\"\"\n        try:\n            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n                lines = f.readlines()\n",
        "description": "NotImplementedError is raised\"\"\""
      },
      {
        "file": "scripts/implementation/implement_functional_fixes.py",
        "line": 28,
        "type": "NotImplementedError",
        "content": "# Check if it's a NotImplementedError",
        "context": "            if line_idx >= len(lines):\n                return False\n            \n            # Check if it's a NotImplementedError\n            if 'NotImplementedError' in lines[line_idx] or 'not implemented' in lines[line_idx].lower():\n                # Look for function definition\n                func_name = None\n",
        "description": "NotImplementedError\n"
      },
      {
        "file": "scripts/implementation/implement_functional_fixes.py",
        "line": 29,
        "type": "NotImplementedError",
        "content": "if 'NotImplementedError' in lines[line_idx] or 'not implemented' in lines[line_idx].lower():",
        "context": "                return False\n            \n            # Check if it's a NotImplementedError\n            if 'NotImplementedError' in lines[line_idx] or 'not implemented' in lines[line_idx].lower():\n                # Look for function definition\n                func_name = None\n                for i in range(max(0, line_idx - 10), line_idx):\n",
        "description": "NotImplementedError' in lines[line_idx] or 'not implemented' in lines[line_idx].lower():"
      },
      {
        "file": "scripts/implementation/implement_functional_fixes.py",
        "line": 42,
        "type": "TODO",
        "content": "new_impl = ' ' * indent + f'# TODO: Implement {func_name} functionality\\n'",
        "context": "                if func_name:\n                    # Add a basic implementation with logging\n                    indent = len(lines[line_idx]) - len(lines[line_idx].lstrip())\n                    new_impl = ' ' * indent + f'# TODO: Implement {func_name} functionality\\n'\n                    new_impl += ' ' * indent + f'logger.warning(f\"{func_name} not fully implemented\")\\n'\n                    new_impl += ' ' * indent + 'return None  # Placeholder return\\n'\n                    \n",
        "description": "Implement {func_name} functionality\\n'"
      },
      {
        "file": "scripts/implementation/implement_functional_fixes.py",
        "line": 152,
        "type": "NotImplementedError",
        "content": "# Filter for NotImplementedError and validation TODOs",
        "context": "    \n    placeholders = data['detailed_placeholders']\n    \n    # Filter for NotImplementedError and validation TODOs\n    not_implemented = [p for p in placeholders if p['type'] == 'NotImplementedError']\n    validation_todos = [p for p in placeholders \n                       if p['type'] == 'TODO' and \n",
        "description": "NotImplementedError and validation TODOs"
      },
      {
        "file": "scripts/implementation/implement_functional_fixes.py",
        "line": 153,
        "type": "NotImplementedError",
        "content": "not_implemented = [p for p in placeholders if p['type'] == 'NotImplementedError']",
        "context": "    placeholders = data['detailed_placeholders']\n    \n    # Filter for NotImplementedError and validation TODOs\n    not_implemented = [p for p in placeholders if p['type'] == 'NotImplementedError']\n    validation_todos = [p for p in placeholders \n                       if p['type'] == 'TODO' and \n                       any(kw in p['content'].lower() for kw in ['check', 'validate', 'verify'])]\n",
        "description": "NotImplementedError']"
      },
      {
        "file": "scripts/implementation/implement_functional_fixes.py",
        "line": 158,
        "type": "NotImplementedError",
        "content": "print(f\"Found {len(not_implemented)} NotImplementedError instances\")",
        "context": "                       if p['type'] == 'TODO' and \n                       any(kw in p['content'].lower() for kw in ['check', 'validate', 'verify'])]\n    \n    print(f\"Found {len(not_implemented)} NotImplementedError instances\")\n    print(f\"Found {len(validation_todos)} validation TODOs\")\n    \n    fixer = FunctionalFixer('/home/ubuntu/opencog-unified')\n",
        "description": "NotImplementedError instances\")"
      },
      {
        "file": "scripts/implementation/implement_functional_fixes.py",
        "line": 163,
        "type": "NotImplementedError",
        "content": "# Process NotImplementedError instances",
        "context": "    \n    fixer = FunctionalFixer('/home/ubuntu/opencog-unified')\n    \n    # Process NotImplementedError instances\n    print(\"\\nProcessing NotImplementedError instances...\")\n    for i, placeholder in enumerate(not_implemented[:5], 1):\n        filepath = fixer.repo_root / placeholder['file']\n",
        "description": "NotImplementedError instances"
      },
      {
        "file": "scripts/implementation/implement_functional_fixes.py",
        "line": 164,
        "type": "NotImplementedError",
        "content": "print(\"\\nProcessing NotImplementedError instances...\")",
        "context": "    fixer = FunctionalFixer('/home/ubuntu/opencog-unified')\n    \n    # Process NotImplementedError instances\n    print(\"\\nProcessing NotImplementedError instances...\")\n    for i, placeholder in enumerate(not_implemented[:5], 1):\n        filepath = fixer.repo_root / placeholder['file']\n        print(f\"  {i}/{min(5, len(not_implemented))}: {placeholder['file']}:{placeholder['line']}\")\n",
        "description": "NotImplementedError instances...\")"
      },
      {
        "file": "scripts/implementation/generate_progress_report.py",
        "line": 29,
        "type": "NotImplementedError",
        "content": "ni_count=analysis['by_type'].get('NotImplementedError', 0)",
        "context": "    fixme_count=analysis['by_type'].get('FIXME', 0)\n    todo_count=analysis['by_type'].get('TODO', 0)\n    stub_comment_count=analysis['by_type'].get('stub', 0)\n    ni_count=analysis['by_type'].get('NotImplementedError', 0)\n    stub_func_count=len(stubs)\n    total_fixes=impl_report['summary']['total_fixes_applied'] + feature_report['summary']['total_implementations']\n    obsolete_fixes_count=impl_report['summary']['by_type'].get('obsolete_comment', 0)\n",
        "description": "NotImplementedError', 0)"
      },
      {
        "file": "scripts/implementation/generate_progress_report.py",
        "line": 60,
        "type": "NotImplementedError",
        "content": "| **NotImplementedError** | {ni_count}    |",
        "context": "| **FIXME**             | {fixme_count} |\n| **TODO**              | {todo_count}  |\n| **Stub Comments**     | {stub_comment_count} |\n| **NotImplementedError** | {ni_count}    |\n| **Total**             | **{total_placeholders}** |\n\nAdditionally, **{stub_func_count}** actual stub functions (with `pass` or similar) were found, one of which was a candidate for implementation.\n",
        "description": "NotImplementedError** | {ni_count}    |"
      },
      {
        "file": "scripts/implementation/implement_scheme_stubs.py",
        "line": 134,
        "type": "NotImplementedError",
        "content": "'NotImplementedError' in p['content']]",
        "context": "    # Find all 'not-implemented' throws\n    not_implemented = [p for p in data['detailed_placeholders'] \n                      if 'not-implemented' in p['content'].lower() or \n                      'NotImplementedError' in p['content']]\n    \n    print(f\"Found {len(not_implemented)} not-implemented stubs\")\n    \n",
        "description": "NotImplementedError' in p['content']]"
      },
      {
        "file": "scripts/testing/test_implementations.py",
        "line": 111,
        "type": "TODO",
        "content": "'raise NotImplementedError  # TODO'",
        "context": "        'pass  # TODO',\n        'pass  # FIXME',\n        'return None  # placeholder',\n        'raise NotImplementedError  # TODO'\n    ]\n    \n    violations = []\n",
        "description": "'"
      },
      {
        "file": "scripts/testing/test_implementations.py",
        "line": 111,
        "type": "NotImplementedError",
        "content": "'raise NotImplementedError  # TODO'",
        "context": "        'pass  # TODO',\n        'pass  # FIXME',\n        'return None  # placeholder',\n        'raise NotImplementedError  # TODO'\n    ]\n    \n    violations = []\n",
        "description": "raise NotImplementedError  # TODO'"
      },
      {
        "file": "scripts/analysis/find_placeholders.py",
        "line": 4,
        "type": "NotImplementedError",
        "content": "Finds all TODO, FIXME, XXX, NotImplementedError, stub implementations, and empty functions",
        "context": "#!/usr/bin/env python3\n\"\"\"\nComprehensive Placeholder Detection Script\nFinds all TODO, FIXME, XXX, NotImplementedError, stub implementations, and empty functions\n\"\"\"\nimport os\nimport re\n",
        "description": "NotImplementedError, stub implementations, and empty functions"
      },
      {
        "file": "scripts/analysis/find_placeholders.py",
        "line": 21,
        "type": "NotImplementedError",
        "content": "'NotImplementedError': r'(raise\\s+NotImplementedError|throw.*not.*implemented)',",
        "context": "        self.patterns = {\n            'TODO': r'(//|#|;|/\\*)\\s*TODO[:\\s]',\n            'FIXME': r'(//|#|;|/\\*)\\s*(FIXME|XXX)[:\\s]',\n            'NotImplementedError': r'(raise\\s+NotImplementedError|throw.*not.*implemented)',\n            'pass_only': r'def\\s+\\w+\\([^)]*\\):\\s*pass\\s*$',\n            'empty_function': r'def\\s+\\w+\\([^)]*\\):\\s*\\.\\.\\.\\s*$',\n            'stub': r'(//|#|;)\\s*STUB[:\\s]',\n",
        "description": "NotImplementedError': r'(raise\\s+NotImplementedError|throw.*not.*implemented)',"
      },
      {
        "file": "scripts/analysis/analyze_issue_examples.py",
        "line": 36,
        "type": "FIXME",
        "content": "\"./atomspace/opencog/atoms/core/Variables.cc:\t// XXX FIXME URE calls us with broken handle!!\",",
        "context": "        \"./atomspace/opencog/atoms/truthvalue/FormulaTruthValue.cc:// XXX FIXME This update is not thread-safe.\",\n        \"./atomspace/opencog/atoms/core/TypeChoice.cc:\t\t// For now, just avoid throwing an exception. XXX FIXME.\",\n        \"./atomspace/opencog/atoms/core/RandomChoice.cc:// XXX FIXME - fix this so it can also choose a single value\",\n        \"./atomspace/opencog/atoms/core/Variables.cc:\t// XXX FIXME URE calls us with broken handle!!\",\n        \"./atomspace/opencog/atoms/core/TypeUtils.cc:\t\t\t\t\\\"Not implemented! TODO XXX FIXME\\\");\",\n        \"./atomspace/opencog/atoms/value/FormulaStream.cc:// XXX FIXME The update here is not thread-safe...\",\n        \"./atomspace/opencog/atoms/execution/Instantiator.cc:/// cleanly separated from each other. (XXX FIXME, these need to be\",\n",
        "description": "URE calls us with broken handle!!\","
      },
      {
        "file": "scripts/analysis/analyze_issue_examples.py",
        "line": 106,
        "type": "FIXME",
        "content": "markdown = \"\"\"# FIXME Instances from Issue #74 - Sorted by Implementation Difficulty",
        "context": "    \n    categorized = process_issue_examples()\n    \n    markdown = \"\"\"# FIXME Instances from Issue #74 - Sorted by Implementation Difficulty\n\nThis document analyzes the specific FIXME instances mentioned in issue #74, categorized by implementation difficulty.\n\n",
        "description": "Instances from Issue #74 - Sorted by Implementation Difficulty"
      },
      {
        "file": "scripts/analysis/scan_placeholders.py",
        "line": 13,
        "type": "NotImplementedError",
        "content": "'raise_not_implemented': re.compile(r'raise\\s+NotImplementedError'),",
        "context": "    \n    patterns = {\n        'pass_only': re.compile(r'^\\s*pass\\s*$'),\n        'raise_not_implemented': re.compile(r'raise\\s+NotImplementedError'),\n        'return_none': re.compile(r'^\\s*return\\s+None\\s*$'),\n        'empty_function': re.compile(r'{\\s*}'),\n        'todo_comment': re.compile(r'(//|#)\\s*TODO[:\\s]*(.*)', re.IGNORECASE),\n",
        "description": "NotImplementedError'),"
      },
      {
        "file": "scripts/analysis/find_actual_stubs.py",
        "line": 8,
        "type": "NotImplementedError",
        "content": "\"\"\"Find functions with only pass, ..., or NotImplementedError\"\"\"",
        "context": "from pathlib import Path\n\ndef find_stub_implementations(repo_root):\n    \"\"\"Find functions with only pass, ..., or NotImplementedError\"\"\"\n    stubs = []\n    \n    for pyfile in Path(repo_root).rglob(\"*.py\"):\n",
        "description": "NotImplementedError\"\"\""
      },
      {
        "file": "scripts/analysis/find_actual_stubs.py",
        "line": 59,
        "type": "NotImplementedError",
        "content": "elif 'raise NotImplementedError' in first_impl:",
        "context": "                            elif re.match(r'^\\s*\\.\\.\\.\\s*$', first_impl):\n                                is_stub = True\n                                stub_type = 'ellipsis'\n                            elif 'raise NotImplementedError' in first_impl:\n                                is_stub = True\n                                stub_type = 'NotImplementedError'\n                            elif re.match(r'^\\s*return\\s+None\\s*$', first_impl) and len(impl_lines) == 1:\n",
        "description": "raise NotImplementedError' in first_impl:"
      },
      {
        "file": "scripts/analysis/find_actual_stubs.py",
        "line": 61,
        "type": "NotImplementedError",
        "content": "stub_type = 'NotImplementedError'",
        "context": "                                stub_type = 'ellipsis'\n                            elif 'raise NotImplementedError' in first_impl:\n                                is_stub = True\n                                stub_type = 'NotImplementedError'\n                            elif re.match(r'^\\s*return\\s+None\\s*$', first_impl) and len(impl_lines) == 1:\n                                is_stub = True\n                                stub_type = 'return_none'\n",
        "description": "NotImplementedError'"
      },
      {
        "file": "scripts/analysis/analyze_placeholders.py",
        "line": 15,
        "type": "NotImplementedError",
        "content": "'NotImplementedError': re.compile(r'(raise\\s+)?NotImplementedError\\s*\\(?(.*)\\)?'),",
        "context": "    patterns = {\n        'FIXME': re.compile(r'(//|#)\\s*(XXX\\s+)?FIXME[:\\s]*(.*)', re.IGNORECASE),\n        'TODO': re.compile(r'(//|#)\\s*(XXX\\s+)?TODO[:\\s]*(.*)', re.IGNORECASE),\n        'NotImplementedError': re.compile(r'(raise\\s+)?NotImplementedError\\s*\\(?(.*)\\)?'),\n        'stub': re.compile(r'(//|#)\\s*stub[:\\s]*(.*)', re.IGNORECASE),\n        'pass_placeholder': re.compile(r'pass\\s*#\\s*placeholder', re.IGNORECASE),\n        'empty_function': re.compile(r'^\\s*(def|void|int|bool|float|double)\\s+\\w+\\([^)]*\\)\\s*{\\s*}\\s*$'),\n",
        "description": "NotImplementedError': re.compile(r'(raise\\s+)?NotImplementedError\\s*\\(?(.*)\\)?'),"
      },
      {
        "file": "scripts/analysis/analyze_placeholders.py",
        "line": 106,
        "type": "NotImplementedError",
        "content": "# Check for NotImplementedError with description",
        "context": "        # Check if it has clear implementation hints\n        elif 'implement' in p['description'].lower() and len(p['description']) > 20:\n            fixable.append({**p, 'reason': 'Has implementation hints'})\n        # Check for NotImplementedError with description\n        elif p['type'] == 'NotImplementedError' and len(p['description']) > 5:\n            fixable.append({**p, 'reason': 'Error with description'})\n        else:\n",
        "description": "NotImplementedError with description"
      },
      {
        "file": "scripts/analysis/analyze_placeholders.py",
        "line": 107,
        "type": "NotImplementedError",
        "content": "elif p['type'] == 'NotImplementedError' and len(p['description']) > 5:",
        "context": "        elif 'implement' in p['description'].lower() and len(p['description']) > 20:\n            fixable.append({**p, 'reason': 'Has implementation hints'})\n        # Check for NotImplementedError with description\n        elif p['type'] == 'NotImplementedError' and len(p['description']) > 5:\n            fixable.append({**p, 'reason': 'Error with description'})\n        else:\n            needs_research.append(p)\n",
        "description": "NotImplementedError' and len(p['description']) > 5:"
      },
      {
        "file": "moses/moses/moses/deme/deme_expander.cc",
        "line": 502,
        "type": "FIXME",
        "content": "// XXX FIXME this is a bug .. the user may have specified that",
        "context": "    if (_params.fstor) {\n        // reset scorer to use all variables (important so that\n        // behavioral score is consistent across generations\n        // XXX FIXME this is a bug .. the user may have specified that\n        // certain incdexes should be ignored, and this just wipes\n        // those out...\n        _cscorer.ignore_cols(std::set<arity_t>());\n",
        "description": "this is a bug .. the user may have specified that"
      },
      {
        "file": "moses/moses/moses/representation/build_knobs.cc",
        "line": 1013,
        "type": "TODO",
        "content": "// TODO: implement support for enumerated types in the input.",
        "context": "// ***********************************************************************\n// Enumerated types.\n// For now, we only handle enumerated types on output, and not on input.\n// TODO: implement support for enumerated types in the input.\n\n/// enum_canonize: make sure that the exemplar is in canonical form.\n/// The canonical form will be of the form\n",
        "description": "implement support for enumerated types in the input."
      },
      {
        "file": "moses/moses/moses/representation/build_knobs.cc",
        "line": 1285,
        "type": "TODO",
        "content": "// XXX TODO this below is clearly unfinished, broken, etc.",
        "context": "    }\n}\n\n// XXX TODO this below is clearly unfinished, broken, etc.\n// and can't possibly work ... \nvoid build_knobs::ann_canonize(pre_it it)\n{\n",
        "description": "this below is clearly unfinished, broken, etc."
      },
      {
        "file": "moses/moses/comboreduct/table/table_io.cc",
        "line": 1245,
        "type": "TODO",
        "content": "// TODO: implement timestamp support",
        "context": "// ==================================================================\n\n// Parse a CTable row\n// TODO: implement timestamp support\nCTable::value_type parseCTableRow(const type_tree& tt, const std::string& row_str)\n{\n    // split the string between input and output\n",
        "description": "implement timestamp support"
      },
      {
        "file": "moses/moses/comboreduct/table/table.h",
        "line": 692,
        "type": "TODO",
        "content": "// XXX TODO WARNING ERROR: builtin hardcoded shit!!!",
        "context": "        auto it = filter.cbegin();\n        for (unsigned i = 0; i < seq.size(); ++i) {\n            if (it != filter.cend() && (typename F::value_type)i == *it) {\n                // XXX TODO WARNING ERROR: builtin hardcoded shit!!!\n                res.push_back(seq.get_at<builtin>(i));\n                ++it;\n            } else {\n",
        "description": "WARNING ERROR: builtin hardcoded shit!!!"
      },
      {
        "file": "moses/moses/comboreduct/table/table.h",
        "line": 696,
        "type": "TODO",
        "content": "// XXX TODO WARNING ERROR: builtin hardcoded shit!!!",
        "context": "                res.push_back(seq.get_at<builtin>(i));\n                ++it;\n            } else {\n                // XXX TODO WARNING ERROR: builtin hardcoded shit!!!\n                res.push_back(id::null_vertex);\n            }\n        }\n",
        "description": "WARNING ERROR: builtin hardcoded shit!!!"
      },
      {
        "file": "moses/moses/comboreduct/table/table.h",
        "line": 1134,
        "type": "TODO",
        "content": "// XXX TODO to implement enum support, cut-n-paste from CTable",
        "context": "template<typename FeatureSet>\ndouble mutualInformation(const ITable& it, const OTable& ot, const FeatureSet& fs)\n{\n    // XXX TODO to implement enum support, cut-n-paste from CTable\n    // mutual info code, below.\n    type_node otype = ot.get_type();\n    OC_ASSERT(id::boolean_type == otype, \"Only boolean types supported\");\n",
        "description": "to implement enum support, cut-n-paste from CTable"
      },
      {
        "file": "components/learning/moses/moses/moses/deme/deme_expander.cc",
        "line": 502,
        "type": "FIXME",
        "content": "// XXX FIXME this is a bug .. the user may have specified that",
        "context": "    if (_params.fstor) {\n        // reset scorer to use all variables (important so that\n        // behavioral score is consistent across generations\n        // XXX FIXME this is a bug .. the user may have specified that\n        // certain incdexes should be ignored, and this just wipes\n        // those out...\n        _cscorer.ignore_cols(std::set<arity_t>());\n",
        "description": "this is a bug .. the user may have specified that"
      },
      {
        "file": "components/learning/moses/moses/moses/representation/build_knobs.cc",
        "line": 1011,
        "type": "TODO",
        "content": "// TODO: implement support for enumerated types in the input.",
        "context": "// ***********************************************************************\n// Enumerated types.\n// For now, we only handle enumerated types on output, and not on input.\n// TODO: implement support for enumerated types in the input.\n\n/// enum_canonize: make sure that the exemplar is in canonical form.\n/// The canonical form will be of the form\n",
        "description": "implement support for enumerated types in the input."
      },
      {
        "file": "components/learning/moses/moses/moses/representation/build_knobs.cc",
        "line": 1283,
        "type": "TODO",
        "content": "// XXX TODO this below is clearly unfinished, broken, etc.",
        "context": "    }\n}\n\n// XXX TODO this below is clearly unfinished, broken, etc.\n// and can't possibly work ... \nvoid build_knobs::ann_canonize(pre_it it)\n{\n",
        "description": "this below is clearly unfinished, broken, etc."
      },
      {
        "file": "components/learning/moses/moses/comboreduct/table/table_io.cc",
        "line": 1256,
        "type": "TODO",
        "content": "// TODO: implement timestamp support",
        "context": "// ==================================================================\n\n// Parse a CTable row\n// TODO: implement timestamp support\nCTable::value_type parseCTableRow(const type_tree& tt, const std::string& row_str)\n{\n    // split the string between input and output\n",
        "description": "implement timestamp support"
      },
      {
        "file": "components/learning/moses/moses/comboreduct/table/table.h",
        "line": 692,
        "type": "TODO",
        "content": "// XXX TODO WARNING ERROR: builtin hardcoded shit!!!",
        "context": "        auto it = filter.cbegin();\n        for (unsigned i = 0; i < seq.size(); ++i) {\n            if (it != filter.cend() && (typename F::value_type)i == *it) {\n                // XXX TODO WARNING ERROR: builtin hardcoded shit!!!\n                res.push_back(seq.get_at<builtin>(i));\n                ++it;\n            } else {\n",
        "description": "WARNING ERROR: builtin hardcoded shit!!!"
      },
      {
        "file": "components/learning/moses/moses/comboreduct/table/table.h",
        "line": 696,
        "type": "TODO",
        "content": "// XXX TODO WARNING ERROR: builtin hardcoded shit!!!",
        "context": "                res.push_back(seq.get_at<builtin>(i));\n                ++it;\n            } else {\n                // XXX TODO WARNING ERROR: builtin hardcoded shit!!!\n                res.push_back(id::null_vertex);\n            }\n        }\n",
        "description": "WARNING ERROR: builtin hardcoded shit!!!"
      },
      {
        "file": "components/learning/moses/moses/comboreduct/table/table.h",
        "line": 1134,
        "type": "TODO",
        "content": "// XXX TODO to implement enum support, cut-n-paste from CTable",
        "context": "template<typename FeatureSet>\ndouble mutualInformation(const ITable& it, const OTable& ot, const FeatureSet& fs)\n{\n    // XXX TODO to implement enum support, cut-n-paste from CTable\n    // mutual info code, below.\n    type_node otype = ot.get_type();\n    OC_ASSERT(id::boolean_type == otype, \"Only boolean types supported\");\n",
        "description": "to implement enum support, cut-n-paste from CTable"
      },
      {
        "file": "atomspace/opencog/atoms/pattern/PatternLink.cc",
        "line": 1465,
        "type": "FIXME",
        "content": "// XXX FIXME: debug_log() above is more readable than the below.",
        "context": "\nDEFINE_LINK_FACTORY(PatternLink, PATTERN_LINK)\n\n// XXX FIXME: debug_log() above is more readable than the below.\nstd::string PatternLink::to_long_string(const std::string& indent) const\n{\n\tstd::string indent_p = indent + oc_to_string_indent;\n",
        "description": "debug_log() above is more readable than the below."
      },
      {
        "file": "atomspace/opencog/atoms/core/Checkers.cc",
        "line": 42,
        "type": "FIXME",
        "content": "// XXX FIXME Much of the onfusion below is due to a bug: if the",
        "context": "/// This only performs a very simple kind of type checking;\n/// it does not check deep types, nor does it check arity.\n\n// XXX FIXME Much of the onfusion below is due to a bug: if the\n// types script says something like\n// FOOBAR <- FUNCTION_LINK,BOOL_INPUT_LINK,NUMBER_INPUT_LINK\n// then the Foobar function will fail if given a boolean input:\n",
        "description": "Much of the onfusion below is due to a bug: if the"
      },
      {
        "file": "atomspace-storage/opencog/persist/api/cython/PersistCython.cc",
        "line": 31,
        "type": "FIXME",
        "content": "// XXX FIXME: except for the error messages, most of this code is",
        "context": "\nnamespace opencog {\n\n// XXX FIXME: except for the error messages, most of this code is\n// mostly a cut-n-pate of what's in PersistSCM.cc\n\n// =====================================================================\n",
        "description": "except for the error messages, most of this code is"
      },
      {
        "file": "hypermind/hypermind.hpp",
        "line": 165,
        "type": "stub",
        "content": "// STUB: Network serialization not yet implemented",
        "context": "    }\n    \n    bool send_network_message(Message* msg, const std::string& dest_address) {\n        // STUB: Network serialization not yet implemented\n        // TODO: Serialize message to wire format (e.g., Protocol Buffers, MessagePack)\n        // TODO: Send over actual network transport (ZeroMQ, gRPC, raw TCP/IP)\n        // TODO: Handle network errors and retries\n",
        "description": "// STUB: Network serialization not yet implemented"
      },
      {
        "file": "hypermind/hypermind.hpp",
        "line": 168,
        "type": "TODO",
        "content": "// TODO: Handle network errors and retries",
        "context": "        // STUB: Network serialization not yet implemented\n        // TODO: Serialize message to wire format (e.g., Protocol Buffers, MessagePack)\n        // TODO: Send over actual network transport (ZeroMQ, gRPC, raw TCP/IP)\n        // TODO: Handle network errors and retries\n        // Reference: specs/integrations.zpp SendNetworkMessage operation\n        \n        NetworkEnvelope envelope;\n",
        "description": "Handle network errors and retries"
      },
      {
        "file": "hypermind/hypermind.hpp",
        "line": 184,
        "type": "stub",
        "content": "// STUB: Network deserialization not yet implemented",
        "context": "    }\n    \n    bool receive_network_message(NetworkEnvelope& envelope) {\n        // STUB: Network deserialization not yet implemented\n        // TODO: Receive data from network transport\n        // TODO: Deserialize from wire format to NetworkEnvelope\n        // TODO: Handle network errors and timeouts\n",
        "description": "// STUB: Network deserialization not yet implemented"
      },
      {
        "file": "hypermind/hypermind.hpp",
        "line": 187,
        "type": "TODO",
        "content": "// TODO: Handle network errors and timeouts",
        "context": "        // STUB: Network deserialization not yet implemented\n        // TODO: Receive data from network transport\n        // TODO: Deserialize from wire format to NetworkEnvelope\n        // TODO: Handle network errors and timeouts\n        // Reference: specs/integrations.zpp ReceiveNetworkMessage operation\n        \n        // Validate checksum\n",
        "description": "Handle network errors and timeouts"
      },
      {
        "file": "hypermind/hypermind.hpp",
        "line": 217,
        "type": "TODO",
        "content": "// TODO: Implement feedforward computation",
        "context": "    LayerProxy _this_layer;\n    char _rank; // worker, manager, director\n    void execute(NeuralReactor& neural_reactor) override {\n        // TODO: Implement feedforward computation\n        // neural_reactor.process_layer(_this_layer);\n        // neural_reactor.send_activation_message();\n        // Reference: specs/operations.zpp ExecuteFeedForward operation\n",
        "description": "Implement feedforward computation"
      },
      {
        "file": "hypermind/hypermind.hpp",
        "line": 230,
        "type": "TODO",
        "content": "// TODO: Implement backpropagation computation",
        "context": "    char _rank; // worker, manager, director\n    NDArray* _gradient; // gradient from next layer\n    void execute(NeuralReactor& neural_reactor) override {\n        // TODO: Implement backpropagation computation\n        // Compute gradients for this layer using chain rule\n        // Update weights and biases with learning rate\n        // Propagate gradient to previous layer via message\n",
        "description": "Implement backpropagation computation"
      },
      {
        "file": "hypermind/hypermind.hpp",
        "line": 245,
        "type": "TODO",
        "content": "// TODO: Implement weight update with gradient descent",
        "context": "    NDArray* _bias_gradient;\n    float _learning_rate;\n    void execute(NeuralReactor& neural_reactor) override {\n        // TODO: Implement weight update with gradient descent\n        // weights = weights - learning_rate * weight_gradient\n        // bias = bias - learning_rate * bias_gradient\n        // Reference: specs/operations.zpp for formal specification\n",
        "description": "Implement weight update with gradient descent"
      },
      {
        "file": "hypermind/hypermind.hpp",
        "line": 258,
        "type": "TODO",
        "content": "// TODO: Implement local gradient computation",
        "context": "    NDArray* _activation;\n    NDArray* _output_gradient;\n    void execute(NeuralReactor& neural_reactor) override {\n        // TODO: Implement local gradient computation\n        // Apply chain rule for gradient computation\n        // Compute activation function derivatives\n        // Reference: specs/operations.zpp for formal specification\n",
        "description": "Implement local gradient computation"
      },
      {
        "file": "hypermind/hypermind.hpp",
        "line": 295,
        "type": "TODO",
        "content": "// TODO: Implement hash lookup in _session_map",
        "context": "    \n    SessionState* getSessionState(ID session_id) {\n        /* Get session state from session state map */\n        // TODO: Implement hash lookup in _session_map\n        (void)session_id; // Suppress unused warning\n        return nullptr; // STUB\n    };\n",
        "description": "Implement hash lookup in _session_map"
      },
      {
        "file": "hypermind/hypermind.hpp",
        "line": 320,
        "type": "TODO",
        "content": "// TODO: Implement message type dispatch",
        "context": "    };\n    \n    void handle_message(Message* msg) {\n        // TODO: Implement message type dispatch\n        // std::string msg_type = msg->getType();\n        // message handler\n        (void)msg; // Suppress unused warning\n",
        "description": "Implement message type dispatch"
      },
      {
        "file": "hypermind/hypermind.hpp",
        "line": 327,
        "type": "TODO",
        "content": "// TODO: Implement GPU event processing",
        "context": "    };\n    \n    void handle_gpu_event(Event* event) {\n        // TODO: Implement GPU event processing\n        // GPUResult* result = static_cast<GPUResult*>(event->getData());\n        // if (!result->success) {\n        //     handle_error(result->error);\n",
        "description": "Implement GPU event processing"
      },
      {
        "file": "hypermind/hypermind.hpp",
        "line": 337,
        "type": "TODO",
        "content": "// TODO: Implement database event processing",
        "context": "    };\n    \n    void handle_database_event(Event* event) {\n        // TODO: Implement database event processing\n        // DatabaseResult* result = static_cast<DatabaseResult*>(event->getData());\n        // if (!result->success) {\n        //     handle_error(result->error);\n",
        "description": "Implement database event processing"
      },
      {
        "file": "hypermind/hypermind.hpp",
        "line": 356,
        "type": "stub",
        "content": "// STUB: GPU integration not yet implemented",
        "context": "    };\n    \n    void submit_gpu_operation(GPUOperation* op) {\n        // STUB: GPU integration not yet implemented\n        // TODO: Implement CUDA/OpenCL submission\n        // TODO: Manage GPU memory allocation and transfers\n        // TODO: Submit computation kernel to GPU stream\n",
        "description": "// STUB: GPU integration not yet implemented"
      },
      {
        "file": "hypermind/hypermind.hpp",
        "line": 357,
        "type": "TODO",
        "content": "// TODO: Implement CUDA/OpenCL submission",
        "context": "    \n    void submit_gpu_operation(GPUOperation* op) {\n        // STUB: GPU integration not yet implemented\n        // TODO: Implement CUDA/OpenCL submission\n        // TODO: Manage GPU memory allocation and transfers\n        // TODO: Submit computation kernel to GPU stream\n        // Reference: specs/integrations.zpp SubmitGPUComputation operation\n",
        "description": "Implement CUDA/OpenCL submission"
      },
      {
        "file": "hypermind/hypermind.hpp",
        "line": 369,
        "type": "stub",
        "content": "// STUB: Database integration not yet implemented",
        "context": "    };\n    \n    void submit_database_query(DatabaseQuery* query) {\n        // STUB: Database integration not yet implemented\n        // TODO: Implement PostgreSQL async query submission\n        // TODO: Use connection pooling for efficiency\n        // TODO: Submit to PostgreSQL pipe for async processing\n",
        "description": "// STUB: Database integration not yet implemented"
      },
      {
        "file": "hypermind/hypermind.hpp",
        "line": 370,
        "type": "TODO",
        "content": "// TODO: Implement PostgreSQL async query submission",
        "context": "    \n    void submit_database_query(DatabaseQuery* query) {\n        // STUB: Database integration not yet implemented\n        // TODO: Implement PostgreSQL async query submission\n        // TODO: Use connection pooling for efficiency\n        // TODO: Submit to PostgreSQL pipe for async processing\n        // Reference: specs/integrations.zpp SubmitDatabaseOperation operation\n",
        "description": "Implement PostgreSQL async query submission"
      },
      {
        "file": "hypermind/hypermind.hpp",
        "line": 386,
        "type": "TODO",
        "content": "// TODO: Implement main reactor event loop",
        "context": "    };\n    \n    void run() {\n        // TODO: Implement main reactor event loop\n        // Process messages from internal, external, GPU, and database queues\n        // Message* msg = _get_queue.get(no_wait);\n        // if (!_cmd_queue.empty()) {\n",
        "description": "Implement main reactor event loop"
      },
      {
        "file": "ure/opencog/ure/BetaDistribution.cc",
        "line": 34,
        "type": "TODO",
        "content": "// TODO should be replaced by tv->get_mode() once implemented",
        "context": "\nBetaDistribution::BetaDistribution(const TruthValuePtr& tv,\n                                   double p_alpha, double p_beta)\n\t// TODO should be replaced by tv->get_mode() once implemented\n\t: BetaDistribution(tv->get_mean() * tv->get_count(),\n\t                   tv->get_count(), p_alpha, p_beta) {}\n\n",
        "description": "should be replaced by tv->get_mode() once implemented"
      },
      {
        "file": "cogzero/agentzero-core/src/ActionScheduler.cpp",
        "line": 1073,
        "type": "TODO",
        "content": "// TODO: Implement proper initialization once build system is set up",
        "context": "        // In a real implementation, this would be:\n        // _spacetime_integrator = std::make_shared<SpaceTimeIntegrator>(_atomspace);\n        \n        // TODO: Implement proper initialization once build system is set up\n        \n        logger().info() << \"[ActionScheduler] SpaceTimeIntegrator initialization placeholder complete\";\n        \n",
        "description": "Implement proper initialization once build system is set up"
      },
      {
        "file": "cogzero/agentzero-core/src/MetaPlanner.cpp",
        "line": 549,
        "type": "TODO",
        "content": "// TODO: Implement sophisticated context feature extraction",
        "context": "        return features;\n    }\n    \n    // TODO: Implement sophisticated context feature extraction\n    // This would analyze the AtomSpace context to extract relevant features\n    // For now, return default features with some variation\n    \n",
        "description": "Implement sophisticated context feature extraction"
      },
      {
        "file": "cogzero/agentzero-core/src/MetaPlanner.cpp",
        "line": 776,
        "type": "TODO",
        "content": "// TODO: Implement spacetime integration for temporal planning optimization",
        "context": "void MetaPlanner::integrateWithSpacetime()\n{\n    logger().debug() << \"[MetaPlanner] Integrating with spacetime for temporal planning\";\n    // TODO: Implement spacetime integration for temporal planning optimization\n}\n\nHandle MetaPlanner::analyzeTemporalPatterns()\n",
        "description": "Implement spacetime integration for temporal planning optimization"
      },
      {
        "file": "cogzero/agentzero-core/src/MetaPlanner.cpp",
        "line": 784,
        "type": "TODO",
        "content": "// TODO: Implement temporal pattern analysis using spacetime",
        "context": "    Handle patterns_atom = _atomspace->add_node(CONCEPT_NODE, \n        \"TemporalPatterns_\" + std::to_string(std::time(nullptr)));\n    \n    // TODO: Implement temporal pattern analysis using spacetime\n    \n    return patterns_atom;\n}\n",
        "description": "Implement temporal pattern analysis using spacetime"
      },
      {
        "file": "cogzero/agentzero-core/src/MetaPlanner.cpp",
        "line": 792,
        "type": "TODO",
        "content": "// TODO: Implement temporal planning optimizations",
        "context": "void MetaPlanner::optimizeTemporalPlanning()\n{\n    logger().debug() << \"[MetaPlanner] Optimizing temporal planning aspects\";\n    // TODO: Implement temporal planning optimizations\n}\n\n// Additional AtomSpace integration methods\n",
        "description": "Implement temporal planning optimizations"
      },
      {
        "file": "cogzero/agentzero-python-bridge/opencog/agentzero/agent_zero.py",
        "line": 12,
        "type": "stub",
        "content": "# Stub implementations for when atomspace is not available",
        "context": "    ATOMSPACE_AVAILABLE = True\nexcept ImportError:\n    ATOMSPACE_AVAILABLE = False\n    # Stub implementations for when atomspace is not available\n    class AtomSpace:\n        pass\n    class ConceptNode:\n",
        "description": "# Stub implementations for when atomspace is not available"
      },
      {
        "file": "cogzero/agentzero-communication/src/HumanInterface.cpp",
        "line": 778,
        "type": "TODO",
        "content": "// TODO: Implementation planned for AZ-HUMAN-001",
        "context": "HumanInterface::HumanInterface(opencog::AtomSpacePtr atomspace)\n    : _atomspace(atomspace)\n{\n    // TODO: Implementation planned for AZ-HUMAN-001\n}\n\nstd::string HumanInterface::process_human_input(const std::string& input)\n",
        "description": "Implementation planned for AZ-HUMAN-001"
      },
      {
        "file": "cogzero/agentzero-communication/src/HumanInterface.cpp",
        "line": 783,
        "type": "TODO",
        "content": "// TODO: Implementation planned for AZ-HUMAN-001",
        "context": "\nstd::string HumanInterface::process_human_input(const std::string& input)\n{\n    // TODO: Implementation planned for AZ-HUMAN-001\n    return \"HumanInterface not yet implemented - see AZ-HUMAN-001\";\n}\n\n",
        "description": "Implementation planned for AZ-HUMAN-001"
      },
      {
        "file": "cogzero/agentzero-communication/src/HumanInterface.cpp",
        "line": 789,
        "type": "TODO",
        "content": "// TODO: Implementation planned for AZ-HUMAN-001",
        "context": "\nstd::string HumanInterface::generate_response(const std::string& context)\n{\n    // TODO: Implementation planned for AZ-HUMAN-001\n    return \"HumanInterface not yet implemented - see AZ-HUMAN-001\";\n}\n\n",
        "description": "Implementation planned for AZ-HUMAN-001"
      },
      {
        "file": "cogzero/agentzero-communication/src/CommunicationUtils.cpp",
        "line": 607,
        "type": "TODO",
        "content": "// TODO: Implement proper JSON parsing",
        "context": "    CommunicationConfig config;\n    \n    // For now, return default config\n    // TODO: Implement proper JSON parsing\n    return getDefaultConfig();\n}\n\n",
        "description": "Implement proper JSON parsing"
      },
      {
        "file": "cogzero/agentzero-communication/src/AgentComms.cpp",
        "line": 638,
        "type": "TODO",
        "content": "// TODO: Implementation planned for AZ-COMM-001",
        "context": "AgentComms::AgentComms(opencog::AtomSpacePtr atomspace)\n    : _atomspace(atomspace)\n{\n    // TODO: Implementation planned for AZ-COMM-001\n}\n\nbool AgentComms::send_message(const std::string& agent_id, const std::string& message)\n",
        "description": "Implementation planned for AZ-COMM-001"
      },
      {
        "file": "cogzero/agentzero-communication/src/AgentComms.cpp",
        "line": 643,
        "type": "TODO",
        "content": "// TODO: Implementation planned for AZ-COMM-001",
        "context": "\nbool AgentComms::send_message(const std::string& agent_id, const std::string& message)\n{\n    // TODO: Implementation planned for AZ-COMM-001\n    return false;\n}\n\n",
        "description": "Implementation planned for AZ-COMM-001"
      },
      {
        "file": "cogzero/agentzero-communication/src/AgentComms.cpp",
        "line": 649,
        "type": "TODO",
        "content": "// TODO: Implementation planned for AZ-COMM-001",
        "context": "\nstd::vector<std::string> AgentComms::receive_messages()\n{\n    // TODO: Implementation planned for AZ-COMM-001\n    return {};\n}\n\n",
        "description": "Implementation planned for AZ-COMM-001"
      },
      {
        "file": "cogzero/agentzero-communication/src/DialogueManager.cpp",
        "line": 651,
        "type": "TODO",
        "content": "// TODO: Implementation planned for AZ-NLP-002",
        "context": "DialogueManager::DialogueManager(opencog::AtomSpacePtr atomspace)\n    : _atomspace(atomspace)\n{\n    // TODO: Implementation planned for AZ-NLP-002\n}\n\nstd::string DialogueManager::process_dialogue(const std::string& input)\n",
        "description": "Implementation planned for AZ-NLP-002"
      },
      {
        "file": "cogzero/agentzero-communication/src/DialogueManager.cpp",
        "line": 656,
        "type": "TODO",
        "content": "// TODO: Implementation planned for AZ-NLP-002",
        "context": "\nstd::string DialogueManager::process_dialogue(const std::string& input)\n{\n    // TODO: Implementation planned for AZ-NLP-002\n    return \"DialogueManager not yet implemented - see AZ-NLP-002\";\n}\n\n",
        "description": "Implementation planned for AZ-NLP-002"
      },
      {
        "file": "cogzero/agentzero-communication/src/DialogueManager.cpp",
        "line": 662,
        "type": "TODO",
        "content": "// TODO: Implementation planned for AZ-NLP-002",
        "context": "\nvoid DialogueManager::reset_context()\n{\n    // TODO: Implementation planned for AZ-NLP-002\n}\n\n} // namespace communication\n",
        "description": "Implementation planned for AZ-NLP-002"
      },
      {
        "file": "cogzero/agentzero-tools/src/ToolWrapper.cpp",
        "line": 296,
        "type": "TODO",
        "content": "// TODO: Implement actual async execution using thread pool or async framework",
        "context": "    std::string exec_id = _tool_id + \"_\" + \n        std::to_string(std::chrono::system_clock::now().time_since_epoch().count());\n    \n    // TODO: Implement actual async execution using thread pool or async framework\n    // For now, this is a placeholder that would be implemented based on specific needs\n    \n    logger().warn() << \"[ToolWrapper] Async execution not yet fully implemented\";\n",
        "description": "Implement actual async execution using thread pool or async framework"
      },
      {
        "file": "cogzero/agentzero-tools/src/ToolWrapper.cpp",
        "line": 310,
        "type": "TODO",
        "content": "// TODO: Implement actual REST API call",
        "context": "    \n    ToolResult result(ToolStatus::COMPLETED);\n    \n    // TODO: Implement actual REST API call\n    // This would use a library like libcurl or Boost.Beast to make HTTP requests\n    // For now, this is a placeholder implementation\n    \n",
        "description": "Implement actual REST API call"
      },
      {
        "file": "cogzero/agentzero-tools/src/ToolWrapper.cpp",
        "line": 335,
        "type": "TODO",
        "content": "// TODO: Implement actual ROS topic/service interaction",
        "context": "    \n    ToolResult result(ToolStatus::COMPLETED);\n    \n    // TODO: Implement actual ROS topic/service interaction\n    // This would use ROS client libraries to publish/subscribe or call services\n    // For now, this is a placeholder implementation\n    \n",
        "description": "Implement actual ROS topic/service interaction"
      },
      {
        "file": "cogzero/agentzero-tools/src/ToolWrapper.cpp",
        "line": 359,
        "type": "TODO",
        "content": "// TODO: Implement actual Python script execution",
        "context": "    \n    ToolResult result(ToolStatus::COMPLETED);\n    \n    // TODO: Implement actual Python script execution\n    // This would use system() call or better process management\n    // For now, this is a placeholder implementation\n    \n",
        "description": "Implement actual Python script execution"
      },
      {
        "file": "atenspace/aten/src/THNN/generic/MultiMarginCriterion.c",
        "line": 5,
        "type": "TODO",
        "content": "// TODO: improve error messages",
        "context": "#define TH_GENERIC_FILE \"THNN/generic/MultiMarginCriterion.c\"\n#else\n\n// TODO: improve error messages\nvoid THNN_(MultiMarginCriterion_updateOutput)(\n          THNNState *state,\n          THTensor *input,\n",
        "description": "improve error messages"
      },
      {
        "file": "atenspace/aten/src/ATen/cuda/CUDAMultiStreamGuard.h",
        "line": 12,
        "type": "TODO",
        "content": "// TODO: Implement this generically in c10.  You'll need some way to get",
        "context": "\nnamespace at { namespace cuda {\n\n// TODO: Implement this generically in c10.  You'll need some way to get\n// the number of GPUs from the GuardImpl, in that case.\nclass CUDAMultiStreamGuard final {\npublic:\n",
        "description": "Implement this generically in c10.  You'll need some way to get"
      },
      {
        "file": "atenspace/aten/src/ATen/native/Copy.cpp",
        "line": 90,
        "type": "TODO",
        "content": "// TODO: this should be handled during dispatch, but that's missing...",
        "context": "namespace native {\n\nstatic Tensor & copy_impl(Tensor & self, const Tensor & src, bool non_blocking) {\n  // TODO: this should be handled during dispatch, but that's missing...\n  TORCH_CHECK(self.defined(), \"self is undefined\");\n  TORCH_CHECK(src.defined(), \"src is undefined\");\n\n",
        "description": "this should be handled during dispatch, but that's missing..."
      },
      {
        "file": "atenspace/aten/src/ATen/native/Embedding.cpp",
        "line": 50,
        "type": "TODO",
        "content": "// TODO: implement scale_grad_by_freq",
        "context": "  auto indices_arg = TensorArg(indices_, \"indices\", 2);\n  checkScalarType(\"embedding_backward\", indices_arg, kLong);\n\n  // TODO: implement scale_grad_by_freq\n  if (scale_grad_by_freq) {\n    AT_ERROR(\n        \"embedding_backward: scale_grad_by_freq not supported with sparse gradients\");\n",
        "description": "implement scale_grad_by_freq"
      },
      {
        "file": "atenspace/aten/src/ATen/native/ReduceOps.cpp",
        "line": 333,
        "type": "TODO",
        "content": "// TODO: the TensorIterator reduction implementation of mean",
        "context": "      toString(scalarType),\n      \" instead.\");\n  ScalarType dtype = get_dtype(result, self, opt_dtype, true);\n  // TODO: the TensorIterator reduction implementation of mean\n  // (mean_kernel_impl()) is unvectorized and leads to very poor performance\n  // for production workloads. Once that's fixed, the following code can be used\n  // in lieu of the sum + divide implementation below.\n",
        "description": "the TensorIterator reduction implementation of mean"
      },
      {
        "file": "atenspace/aten/src/ATen/native/TensorIterator.cpp",
        "line": 703,
        "type": "FIXME",
        "content": "// FIXME: workaround for bug: https://github.com/pytorch/pytorch/issues/20342",
        "context": "TensorIterator TensorIterator::nullary_op(Tensor& out) {\n  auto iter = TensorIterator();\n  iter.add_output(out);\n  // FIXME: workaround for bug: https://github.com/pytorch/pytorch/issues/20342\n  iter.resize_outputs_ = false;\n  iter.build();\n  return iter;\n",
        "description": "workaround for bug: https://github.com/pytorch/pytorch/issues/20342"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cuda/CuFFTPlanCache.h",
        "line": 145,
        "type": "TODO",
        "content": "// TODO: Figure out why windows fails to compile",
        "context": "    // Checks if input strides can be viewed as embedded.\n    // See NOTE [ cuFFT Embedded Strides ].\n    //\n    // TODO: Figure out why windows fails to compile\n    //         c10::optional<std::vector<long long int>> inembed_opt =\n    //         c10::nullopt;\n    //       Then move the following to a helper function.\n",
        "description": "Figure out why windows fails to compile"
      },
      {
        "file": "atenspace/aten/src/ATen/native/sparse/SparseTensorMath.cpp",
        "line": 590,
        "type": "TODO",
        "content": "// TODO: This error message seems awfully opaque",
        "context": "    Scalar beta,\n    Scalar alpha\n) {\n  // TODO: This error message seems awfully opaque\n  AT_ASSERT(!t.is_cuda());\n  TORCH_CHECK(!r.is_cuda(), \"addmm: expected 'out' to be CPU tensor, but got CUDA tensor\");\n  TORCH_CHECK(!sparse_.is_cuda(), \"addmm: expected 'mat1' to be a CPU tensor, but got a CUDA tensor\");\n",
        "description": "This error message seems awfully opaque"
      },
      {
        "file": "atenspace/aten/src/ATen/quantized/Quantizer.cpp",
        "line": 230,
        "type": "TODO",
        "content": "// TODO Update quantize_tensor_arm implementation to follow quantize_val,",
        "context": "\n// Specialized implementation from caffe2::Int8Quantize.\n// There may be slight accuracy difference between this and implementation of quantize_val\n// TODO Update quantize_tensor_arm implementation to follow quantize_val,\n// i.e. f = Round(value/scale + zero_point)\n// TODO Make quantize_tensor_arm work for other datatypes too (int8, int32).\ntemplate <>\n",
        "description": "Update quantize_tensor_arm implementation to follow quantize_val,"
      }
    ],
    "medium_priority": [
      {
        "file": "cogutil/opencog/util/zipf.h",
        "line": 90,
        "type": "empty_function",
        "content": "void reset() {}",
        "context": "\t\t\tif (-0.5 >= q)\n\t\t\t\tthrow std::runtime_error(\"Range error: Parameter q must be greater than -0.5!\");\n\t\t}\n\t\tvoid reset() {}\n\n\t\tIntType operator()(std::mt19937& rng)\n\t\t{\n",
        "description": "\t\tvoid reset() {}\n"
      },
      {
        "file": "cogutil/opencog/util/zipf.h",
        "line": 233,
        "type": "empty_function",
        "content": "void reset() {}",
        "context": "\t\t\t_q(q),\n\t\t\t_dist(_pdf.begin(), _pdf.end())\n\t\t{}\n\t\tvoid reset() {}\n\n\t\tIntType operator()(std::mt19937& rng)\n\t\t{\n",
        "description": "\t\tvoid reset() {}\n"
      },
      {
        "file": "language-learning/tests/test_grammar_learner.py",
        "line": 26,
        "type": "FIXME",
        "content": "def setUp(self):    # FIXME: should run before every test, but would not?!",
        "context": "\nclass TestGrammarLearner(unittest.TestCase):\n\n    def setUp(self):    # FIXME: should run before every test, but would not?!\n        input_parses = module_path + '/tests/data/POC-Turtle/MST_fixed_manually/'\n        batch_dir = module_path + '/output/Test_Grammar_Learner_' + str(UTC())[:10] + '/'\n        kwargs = {  # defaults\n",
        "description": "should run before every test, but would not?!"
      },
      {
        "file": "language-learning/tests/test_grammar_learner.py",
        "line": 65,
        "type": "FIXME",
        "content": "# 'template_path': 'poc-turtle',  # FIXME: changed in June 2018 Grammar Tester",
        "context": "        # Additional (optional) parameters for parse_metrics (_abiity & _quality):\n        # 'test_corpus': module_path + '/data/POC-Turtle/poc-turtle-corpus.txt',\n        # 'reference_path': module_path + '/data/POC-Turtle/poc-turtle-parses-expected.txt',\n        # 'template_path': 'poc-turtle',  # FIXME: changed in June 2018 Grammar Tester\n        pass\n\n    '''Legacy ~ POC.0.3 test ~ as it was before 2018-09-29\n",
        "description": "changed in June 2018 Grammar Tester"
      },
      {
        "file": "language-learning/tests/test_grammar_learner.py",
        "line": 310,
        "type": "FIXME",
        "content": "# FIXME: check with further test_grammar updates and delete.",
        "context": "            'verbose'       :   'min'\n        }\n        # Sometimes pqa_meter(with test_grammar updated 2018-10-19) returns pa,recall = 0,0\n        # FIXME: check with further test_grammar updates and delete.\n        x = 0.\n        n = 0\n        while x < 0.1 :\n",
        "description": "check with further test_grammar updates and delete."
      },
      {
        "file": "language-learning/src/grammar_learner/category_learner.py",
        "line": 32,
        "type": "FIXME",
        "content": "log = OrderedDict()  # FIXME: log \u00bb response",
        "context": "    algorithm = kwa('kmeans', 'clustering', **kwargs)\n    verbose = kwa('none', 'verbose', **kwargs)\n\n    log = OrderedDict()  # FIXME: log \u00bb response\n    log.update({'category_learner': 'v.0.7.81231'})\n\n    cdf = pd.DataFrame(columns = ['cluster', 'cluster_words'])\n",
        "description": "log \u00bb response"
      },
      {
        "file": "language-learning/src/grammar_learner/category_learner.py",
        "line": 53,
        "type": "FIXME",
        "content": "except Exception:  # FIXME",
        "context": "        try:\n            dim = vector_space_dim(links, dict_path, tmpath, dim_max, sv_min,\n                                   verbose)\n        except Exception:  # FIXME\n            dim = dim_max\n        log.update({'vector_space_dim': dim})\n\n",
        "description": ""
      },
      {
        "file": "language-learning/src/grammar_learner/hyperwords.py",
        "line": 27,
        "type": "FIXME",
        "content": "if cds != 1: sum_c = sum_c ** cds   # FIXME: cds = 1.0 ?!",
        "context": "def calc_pmi(counts, cds):  # Calculates e^PMI; PMI without the log().\n    sum_w = np.array(counts.sum(axis=1))[:, 0]\n    sum_c = np.array(counts.sum(axis=0))[0, :]\n    if cds != 1: sum_c = sum_c ** cds   # FIXME: cds = 1.0 ?!\n    sum_total = sum_c.sum()\n    sum_w = np.reciprocal(sum_w)\n    sum_c = np.reciprocal(sum_c)\n",
        "description": "cds = 1.0 ?!"
      },
      {
        "file": "language-learning/src/grammar_learner/hyperwords.py",
        "line": 138,
        "type": "FIXME",
        "content": "print('SVDEmbedding: transpose')    #FIXME:DEL",
        "context": "    # Context embeddings can be created with \"transpose\".\n    def __init__(self, path, normalize=True, eig=0.0, transpose=False):\n        if transpose:\n            print('SVDEmbedding: transpose')    #FIXME:DEL\n            ut = np.load(path + '.vt.npy')\n            self.wi, self.iw = load_vocabulary(path + '.contexts.vocab')\n        else:\n",
        "description": "DEL"
      },
      {
        "file": "language-learning/src/grammar_learner/hyperwords.py",
        "line": 358,
        "type": "TODO",
        "content": "svd = SVDEmbedding(svd_path, True, eig)   # TODO: move code here, RAM2RAM",
        "context": "    logger.info(f'SVD matrix (3 files .npy) saved: {len(ut[0])} vectors, ut: {len(ut)} s: {len(s)} vt:{len(vt)}')\n\n    '''SVD => vectors.txt'''\n    svd = SVDEmbedding(svd_path, True, eig)   # TODO: move code here, RAM2RAM\n    if len(svd.m[0]) < dim: dim = len(svd.m[0])   # 80216\n    vectors_df = pd.DataFrame(columns=['word'] + list(range(1,dim+1)))\n    for i, w in enumerate(svd.iw):\n",
        "description": "move code here, RAM2RAM"
      },
      {
        "file": "language-learning/src/grammar_learner/hyperwords.py",
        "line": 439,
        "type": "TODO",
        "content": "svd = SVDEmbedding(svd_path, True, eig)   # TODO: move code here, RAM2RAM",
        "context": "    list2tsv(explicit.ic, svd_path + '.contexts.vocab')\n\n    '''SVD => vectors.txt'''\n    svd = SVDEmbedding(svd_path, True, eig)   # TODO: move code here, RAM2RAM\n    if len(svd.m[0]) < dim: dim = len(svd.m[0])   # 80216\n    vectors_df = pd.DataFrame(columns=['word'] + list(range(1,dim+1)))\n    for i, w in enumerate(svd.iw):\n",
        "description": "move code here, RAM2RAM"
      },
      {
        "file": "language-learning/src/grammar_learner/widgets.py",
        "line": 70,
        "type": "TODO",
        "content": "#  TODO: To be reviewed and changed if necessary",
        "context": "                tree.append(['', m+1, cats[j][2], cats[j][3]])\n        else:\n            print('WTF?', k, v)\n    #  TODO: To be reviewed and changed if necessary\n    if verbose not in ['none', 'min']:\n        display(html_table([['Code', 'Parent', 'Id', 'Words']] + tree))\n\n",
        "description": "To be reviewed and changed if necessary"
      },
      {
        "file": "language-learning/src/grammar_learner/clustering.py",
        "line": 361,
        "type": "TODO",
        "content": "# TODO: n_clusters \u21d2 best_clusters: return best clusters (word lists), centroids",
        "context": "# 80809 update: (30,60,3,[3]) - old range + repeat / (120,30,3) -- search opt\n# 80825 random_clusters\n# 81022 refactoring\n# TODO: n_clusters \u21d2 best_clusters: return best clusters (word lists), centroids\n# 81231 cleanup\n# 90104 resolve Turtle MST LW crash: 1 cluster\n# 90209 group_links: add min_word_count to 80925 legacy version\n",
        "description": "n_clusters \u21d2 best_clusters: return best clusters (word lists), centroids"
      },
      {
        "file": "language-learning/src/grammar_learner/skl_clustering.py",
        "line": 215,
        "type": "TODO",
        "content": "# TODO: int / dict",
        "context": "            if len(clustering) > 3:  # connectivity\n                if type(clustering[3]) is int and clustering[3] > 0:\n                    neighbors = clustering[3]\n                    # TODO: int / dict \n                    connectivity = kneighbors_graph(cd, neighbors,\n                                                    include_self=False)\n            if len(clustering) > 4:  # compute_full_tree\n",
        "description": "int / dict "
      },
      {
        "file": "language-learning/src/grammar_learner/skl_clustering.py",
        "line": 343,
        "type": "TODO",
        "content": "elif len(crange) == 3:  # TODO: replace with SGD?",
        "context": "                    l, m, c = skl_clustering(cd, crange[0], **kwargs)\n                    if m['silhouette_index'] > metrics['silhouette_index']:\n                        labels, metrics, centroids = l, m, c\n        elif len(crange) == 3:  # TODO: replace with SGD?\n            n_min = min(crange[0], crange[1])\n            n_max = max(crange[0], crange[1])\n            labels, metrics, centroids = \\\n",
        "description": "replace with SGD?"
      },
      {
        "file": "language-learning/src/grammar_learner/pqa_table.py",
        "line": 641,
        "type": "FIXME",
        "content": "continue  # FIXME: check case",
        "context": "                     linkage, affinity, gen, ' ---', 'fail',\n                     ' ---', ' ---', ' ---', ' ---', ' ---', ' ---']\n            details.append(dline)\n            continue  # FIXME: check case\n        if kwargs['linkage_limit'] > 0:\n            start = time.time()\n            a, f1, precision, q = pqa_meter(re['grammar_file'],\n",
        "description": "check case"
      },
      {
        "file": "scripts/implementation/fix_placeholders.py",
        "line": 74,
        "type": "TODO",
        "content": "// TODO: Enhance with specific logic as needed",
        "context": "            if return_type == 'void':\n                impl = f'''{return_type} {func_name}({match.group(0).split('(')[1].split(')')[0]}) {{\n    // Implementation added by automated code quality improvement\n    // TODO: Enhance with specific logic as needed\n    logger().debug(\"Executing {func_name}\");\n}}'''\n            elif return_type == 'bool':\n",
        "description": "Enhance with specific logic as needed"
      },
      {
        "file": "scripts/implementation/fix_placeholders.py",
        "line": 131,
        "type": "TODO",
        "content": "if '// TODO' in content and 'logger()' not in content:",
        "context": "    def add_logging(self, file_path, content):\n        \"\"\"Add logging statements where appropriate\"\"\"\n        # Add logging to functions that have TODO comments\n        if '// TODO' in content and 'logger()' not in content:\n            # Check if it's a .cc file (implementation)\n            if file_path.suffix == '.cc':\n                # Add include if not present\n",
        "description": "' in content and 'logger()' not in content:"
      },
      {
        "file": "scripts/implementation/print_resolution_card.py",
        "line": 119,
        "type": "TODO",
        "content": "// TODO: validate input          bool validate() {",
        "context": "  void experimental() { ... }      // under active development\n                                   void experimental() { ... }\n  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  // TODO: validate input          bool validate() {\n  bool validate() {                  if (!initialized_) return false;\n    return true;                     if (data_.empty()) return false;\n  }                                  return true;\n",
        "description": "validate input          bool validate() {"
      },
      {
        "file": "scripts/implementation/fixme_resolution_tracker.py",
        "line": 169,
        "type": "FIXME",
        "content": "report.append(\"# FIXME Resolution Progress Report\")",
        "context": "    def generate_next_steps_report(self) -> str:\n        \"\"\"Generate a report implementing the next steps from the catalog.\"\"\"\n        report = []\n        report.append(\"# FIXME Resolution Progress Report\")\n        report.append(f\"Generated: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n        report.append(\"\")\n        \n",
        "description": "Resolution Progress Report\")"
      },
      {
        "file": "scripts/testing/test_implementations.py",
        "line": 108,
        "type": "TODO",
        "content": "'pass  # TODO',",
        "context": "def verify_no_mock_implementations():\n    \"\"\"Verify we didn't create any mock implementations\"\"\"\n    mock_patterns = [\n        'pass  # TODO',\n        'pass  # FIXME',\n        'return None  # placeholder',\n        'raise NotImplementedError  # TODO'\n",
        "description": "',"
      },
      {
        "file": "scripts/testing/test_implementations.py",
        "line": 109,
        "type": "FIXME",
        "content": "'pass  # FIXME',",
        "context": "    \"\"\"Verify we didn't create any mock implementations\"\"\"\n    mock_patterns = [\n        'pass  # TODO',\n        'pass  # FIXME',\n        'return None  # placeholder',\n        'raise NotImplementedError  # TODO'\n    ]\n",
        "description": "',"
      },
      {
        "file": "scripts/analysis/fragmentation_detector.py",
        "line": 22,
        "type": "TODO",
        "content": "marker_type: str  # TODO, FIXME, STUB",
        "context": "    \"\"\"Represents a specific code fragmentation\"\"\"\n    file_path: str\n    line_number: int\n    marker_type: str  # TODO, FIXME, STUB\n    content: str\n    context_before: List[str]\n    context_after: List[str]\n",
        "description": ", FIXME, STUB"
      },
      {
        "file": "scripts/analysis/analyze_issue_examples.py",
        "line": 14,
        "type": "FIXME",
        "content": "\"./atomspace/opencog/atomspace/Transient.cc:/// XXX FIXME. Performance has not been recently measured; there\",",
        "context": "    \n    issue_examples = [\n        \"./atomspace/examples/atomspace/queue.scm:; XXX FIXME, this example is not yet complete and does not yet work...\",\n        \"./atomspace/opencog/atomspace/Transient.cc:/// XXX FIXME. Performance has not been recently measured; there\",\n        \"./atomspace/opencog/atomspace/AtomTable.cc:    // atom in the parent. What??? XXX NOT TRUE FIXME\",\n        \"./atomspace/opencog/atomspace/AtomSpace.cc:\t// Fixme maybe later someday, if/when this is needed.\",\n        \"./atomspace/opencog/atomspace/AtomSpace.cc:// XXX FIXME -- The recursive design of the depth() routine below makes\",\n",
        "description": ". Performance has not been recently measured; there\","
      },
      {
        "file": "scripts/analysis/analyze_issue_examples.py",
        "line": 16,
        "type": "FIXME",
        "content": "\"./atomspace/opencog/atomspace/AtomSpace.cc:\t// Fixme maybe later someday, if/when this is needed.\",",
        "context": "        \"./atomspace/examples/atomspace/queue.scm:; XXX FIXME, this example is not yet complete and does not yet work...\",\n        \"./atomspace/opencog/atomspace/Transient.cc:/// XXX FIXME. Performance has not been recently measured; there\",\n        \"./atomspace/opencog/atomspace/AtomTable.cc:    // atom in the parent. What??? XXX NOT TRUE FIXME\",\n        \"./atomspace/opencog/atomspace/AtomSpace.cc:\t// Fixme maybe later someday, if/when this is needed.\",\n        \"./atomspace/opencog/atomspace/AtomSpace.cc:// XXX FIXME -- The recursive design of the depth() routine below makes\",\n        \"./atomspace/opencog/atomspace/AtomSpace.h:    // XXX FIXME Users should call StorageNode::add_nocheck() instead.\",\n        \"./atomspace/opencog/cython/PythonEval.cc:    // XXX FIXME this does a lot of wasteful string copying.\",\n",
        "description": "maybe later someday, if/when this is needed.\","
      },
      {
        "file": "scripts/analysis/analyze_issue_examples.py",
        "line": 17,
        "type": "FIXME",
        "content": "\"./atomspace/opencog/atomspace/AtomSpace.cc:// XXX FIXME -- The recursive design of the depth() routine below makes\",",
        "context": "        \"./atomspace/opencog/atomspace/Transient.cc:/// XXX FIXME. Performance has not been recently measured; there\",\n        \"./atomspace/opencog/atomspace/AtomTable.cc:    // atom in the parent. What??? XXX NOT TRUE FIXME\",\n        \"./atomspace/opencog/atomspace/AtomSpace.cc:\t// Fixme maybe later someday, if/when this is needed.\",\n        \"./atomspace/opencog/atomspace/AtomSpace.cc:// XXX FIXME -- The recursive design of the depth() routine below makes\",\n        \"./atomspace/opencog/atomspace/AtomSpace.h:    // XXX FIXME Users should call StorageNode::add_nocheck() instead.\",\n        \"./atomspace/opencog/cython/PythonEval.cc:    // XXX FIXME this does a lot of wasteful string copying.\",\n        \"./atomspace/opencog/cython/PyIncludeWrapper.h:// 0.15.1 and maybe other versions)  FIXME someday...\",\n",
        "description": "-- The recursive design of the depth() routine below makes\","
      },
      {
        "file": "scripts/analysis/analyze_issue_examples.py",
        "line": 18,
        "type": "FIXME",
        "content": "\"./atomspace/opencog/atomspace/AtomSpace.h:    // XXX FIXME Users should call StorageNode::add_nocheck() instead.\",",
        "context": "        \"./atomspace/opencog/atomspace/AtomTable.cc:    // atom in the parent. What??? XXX NOT TRUE FIXME\",\n        \"./atomspace/opencog/atomspace/AtomSpace.cc:\t// Fixme maybe later someday, if/when this is needed.\",\n        \"./atomspace/opencog/atomspace/AtomSpace.cc:// XXX FIXME -- The recursive design of the depth() routine below makes\",\n        \"./atomspace/opencog/atomspace/AtomSpace.h:    // XXX FIXME Users should call StorageNode::add_nocheck() instead.\",\n        \"./atomspace/opencog/cython/PythonEval.cc:    // XXX FIXME this does a lot of wasteful string copying.\",\n        \"./atomspace/opencog/cython/PyIncludeWrapper.h:// 0.15.1 and maybe other versions)  FIXME someday...\",\n        \"./atomspace/opencog/haskell/AtomSpace_CWrapper.h:     * XXX FIXME no one should be using Handle's to work with atoms,\",\n",
        "description": "Users should call StorageNode::add_nocheck() instead.\","
      },
      {
        "file": "scripts/analysis/analyze_issue_examples.py",
        "line": 19,
        "type": "FIXME",
        "content": "\"./atomspace/opencog/cython/PythonEval.cc:    // XXX FIXME this does a lot of wasteful string copying.\",",
        "context": "        \"./atomspace/opencog/atomspace/AtomSpace.cc:\t// Fixme maybe later someday, if/when this is needed.\",\n        \"./atomspace/opencog/atomspace/AtomSpace.cc:// XXX FIXME -- The recursive design of the depth() routine below makes\",\n        \"./atomspace/opencog/atomspace/AtomSpace.h:    // XXX FIXME Users should call StorageNode::add_nocheck() instead.\",\n        \"./atomspace/opencog/cython/PythonEval.cc:    // XXX FIXME this does a lot of wasteful string copying.\",\n        \"./atomspace/opencog/cython/PyIncludeWrapper.h:// 0.15.1 and maybe other versions)  FIXME someday...\",\n        \"./atomspace/opencog/haskell/AtomSpace_CWrapper.h:     * XXX FIXME no one should be using Handle's to work with atoms,\",\n        \"./atomspace/opencog/haskell/PatternMatcher_CWrapper.h: * XXX FIXME: atoms must never be accessed by UUID except by the\",\n",
        "description": "this does a lot of wasteful string copying.\","
      },
      {
        "file": "scripts/analysis/analyze_issue_examples.py",
        "line": 23,
        "type": "FIXME",
        "content": "\"./atomspace/opencog/ocaml/CamlWrap.cc:\t// XXX FIXME\",",
        "context": "        \"./atomspace/opencog/cython/PyIncludeWrapper.h:// 0.15.1 and maybe other versions)  FIXME someday...\",\n        \"./atomspace/opencog/haskell/AtomSpace_CWrapper.h:     * XXX FIXME no one should be using Handle's to work with atoms,\",\n        \"./atomspace/opencog/haskell/PatternMatcher_CWrapper.h: * XXX FIXME: atoms must never be accessed by UUID except by the\",\n        \"./atomspace/opencog/ocaml/CamlWrap.cc:\t// XXX FIXME\",\n        \"./atomspace/opencog/guile/SchemeSmobAS.cc: * until a better permission system is invented. XXX FIXME.\",\n        \"./atomspace/opencog/guile/modules/ExecSCM.cc:// XXX FIXME: can we fix cython to not do this, already?\",\n        \"./atomspace/opencog/guile/SchemeSmobAtom.cc:// XXX FIXME. Work around the despicable, horrible guile UTF8 handling.\",\n",
        "description": "\","
      },
      {
        "file": "scripts/analysis/analyze_issue_examples.py",
        "line": 25,
        "type": "FIXME",
        "content": "\"./atomspace/opencog/guile/modules/ExecSCM.cc:// XXX FIXME: can we fix cython to not do this, already?\",",
        "context": "        \"./atomspace/opencog/haskell/PatternMatcher_CWrapper.h: * XXX FIXME: atoms must never be accessed by UUID except by the\",\n        \"./atomspace/opencog/ocaml/CamlWrap.cc:\t// XXX FIXME\",\n        \"./atomspace/opencog/guile/SchemeSmobAS.cc: * until a better permission system is invented. XXX FIXME.\",\n        \"./atomspace/opencog/guile/modules/ExecSCM.cc:// XXX FIXME: can we fix cython to not do this, already?\",\n        \"./atomspace/opencog/guile/SchemeSmobAtom.cc:// XXX FIXME. Work around the despicable, horrible guile UTF8 handling.\",\n        \"./atomspace/opencog/guile/SchemeSmobValue.cc: * XXX FIXME Clearly, a factory for values is called for.\",\n        \"./atomspace/opencog/guile/SchemeEval.cc:\t// XXX FIXME This lock is not needed, because in guile-2.2,\",\n",
        "description": "can we fix cython to not do this, already?\","
      },
      {
        "file": "scripts/analysis/analyze_issue_examples.py",
        "line": 26,
        "type": "FIXME",
        "content": "\"./atomspace/opencog/guile/SchemeSmobAtom.cc:// XXX FIXME. Work around the despicable, horrible guile UTF8 handling.\",",
        "context": "        \"./atomspace/opencog/ocaml/CamlWrap.cc:\t// XXX FIXME\",\n        \"./atomspace/opencog/guile/SchemeSmobAS.cc: * until a better permission system is invented. XXX FIXME.\",\n        \"./atomspace/opencog/guile/modules/ExecSCM.cc:// XXX FIXME: can we fix cython to not do this, already?\",\n        \"./atomspace/opencog/guile/SchemeSmobAtom.cc:// XXX FIXME. Work around the despicable, horrible guile UTF8 handling.\",\n        \"./atomspace/opencog/guile/SchemeSmobValue.cc: * XXX FIXME Clearly, a factory for values is called for.\",\n        \"./atomspace/opencog/guile/SchemeEval.cc:\t// XXX FIXME This lock is not needed, because in guile-2.2,\",\n        \"./atomspace/opencog/sheaf/attic/linear-parser.scm:  XXX FIXME WARNING DANGER: As written, this runs in exponential time\",\n",
        "description": ". Work around the despicable, horrible guile UTF8 handling.\","
      },
      {
        "file": "scripts/analysis/analyze_issue_examples.py",
        "line": 28,
        "type": "FIXME",
        "content": "\"./atomspace/opencog/guile/SchemeEval.cc:\t// XXX FIXME This lock is not needed, because in guile-2.2,\",",
        "context": "        \"./atomspace/opencog/guile/modules/ExecSCM.cc:// XXX FIXME: can we fix cython to not do this, already?\",\n        \"./atomspace/opencog/guile/SchemeSmobAtom.cc:// XXX FIXME. Work around the despicable, horrible guile UTF8 handling.\",\n        \"./atomspace/opencog/guile/SchemeSmobValue.cc: * XXX FIXME Clearly, a factory for values is called for.\",\n        \"./atomspace/opencog/guile/SchemeEval.cc:\t// XXX FIXME This lock is not needed, because in guile-2.2,\",\n        \"./atomspace/opencog/sheaf/attic/linear-parser.scm:  XXX FIXME WARNING DANGER: As written, this runs in exponential time\",\n        \"./atomspace/opencog/atoms/pattern/PatternUtils.cc:\t\t// XXX FIXME Are the below needed?\",\n        \"./atomspace/opencog/atoms/pattern/BindLink.cc:\t// Shoot. XXX FIXME. Most of the unit tests require that the atom\",\n",
        "description": "This lock is not needed, because in guile-2.2,\","
      },
      {
        "file": "scripts/analysis/analyze_issue_examples.py",
        "line": 30,
        "type": "FIXME",
        "content": "\"./atomspace/opencog/atoms/pattern/PatternUtils.cc:\t\t// XXX FIXME Are the below needed?\",",
        "context": "        \"./atomspace/opencog/guile/SchemeSmobValue.cc: * XXX FIXME Clearly, a factory for values is called for.\",\n        \"./atomspace/opencog/guile/SchemeEval.cc:\t// XXX FIXME This lock is not needed, because in guile-2.2,\",\n        \"./atomspace/opencog/sheaf/attic/linear-parser.scm:  XXX FIXME WARNING DANGER: As written, this runs in exponential time\",\n        \"./atomspace/opencog/atoms/pattern/PatternUtils.cc:\t\t// XXX FIXME Are the below needed?\",\n        \"./atomspace/opencog/atoms/pattern/BindLink.cc:\t// Shoot. XXX FIXME. Most of the unit tests require that the atom\",\n        \"./atomspace/opencog/atoms/pattern/PatternLink.cc:\t\t// XXX FIXME, more correct would be to loop over\",\n        \"./atomspace/opencog/atoms/truthvalue/FormulaTruthValue.cc:// XXX FIXME This update is not thread-safe.\",\n",
        "description": "Are the below needed?\","
      },
      {
        "file": "scripts/analysis/analyze_issue_examples.py",
        "line": 32,
        "type": "FIXME",
        "content": "\"./atomspace/opencog/atoms/pattern/PatternLink.cc:\t\t// XXX FIXME, more correct would be to loop over\",",
        "context": "        \"./atomspace/opencog/sheaf/attic/linear-parser.scm:  XXX FIXME WARNING DANGER: As written, this runs in exponential time\",\n        \"./atomspace/opencog/atoms/pattern/PatternUtils.cc:\t\t// XXX FIXME Are the below needed?\",\n        \"./atomspace/opencog/atoms/pattern/BindLink.cc:\t// Shoot. XXX FIXME. Most of the unit tests require that the atom\",\n        \"./atomspace/opencog/atoms/pattern/PatternLink.cc:\t\t// XXX FIXME, more correct would be to loop over\",\n        \"./atomspace/opencog/atoms/truthvalue/FormulaTruthValue.cc:// XXX FIXME This update is not thread-safe.\",\n        \"./atomspace/opencog/atoms/core/TypeChoice.cc:\t\t// For now, just avoid throwing an exception. XXX FIXME.\",\n        \"./atomspace/opencog/atoms/core/RandomChoice.cc:// XXX FIXME - fix this so it can also choose a single value\",\n",
        "description": ", more correct would be to loop over\","
      },
      {
        "file": "scripts/analysis/analyze_issue_examples.py",
        "line": 33,
        "type": "FIXME",
        "content": "\"./atomspace/opencog/atoms/truthvalue/FormulaTruthValue.cc:// XXX FIXME This update is not thread-safe.\",",
        "context": "        \"./atomspace/opencog/atoms/pattern/PatternUtils.cc:\t\t// XXX FIXME Are the below needed?\",\n        \"./atomspace/opencog/atoms/pattern/BindLink.cc:\t// Shoot. XXX FIXME. Most of the unit tests require that the atom\",\n        \"./atomspace/opencog/atoms/pattern/PatternLink.cc:\t\t// XXX FIXME, more correct would be to loop over\",\n        \"./atomspace/opencog/atoms/truthvalue/FormulaTruthValue.cc:// XXX FIXME This update is not thread-safe.\",\n        \"./atomspace/opencog/atoms/core/TypeChoice.cc:\t\t// For now, just avoid throwing an exception. XXX FIXME.\",\n        \"./atomspace/opencog/atoms/core/RandomChoice.cc:// XXX FIXME - fix this so it can also choose a single value\",\n        \"./atomspace/opencog/atoms/core/Variables.cc:\t// XXX FIXME URE calls us with broken handle!!\",\n",
        "description": "This update is not thread-safe.\","
      },
      {
        "file": "scripts/analysis/analyze_issue_examples.py",
        "line": 35,
        "type": "FIXME",
        "content": "\"./atomspace/opencog/atoms/core/RandomChoice.cc:// XXX FIXME - fix this so it can also choose a single value\",",
        "context": "        \"./atomspace/opencog/atoms/pattern/PatternLink.cc:\t\t// XXX FIXME, more correct would be to loop over\",\n        \"./atomspace/opencog/atoms/truthvalue/FormulaTruthValue.cc:// XXX FIXME This update is not thread-safe.\",\n        \"./atomspace/opencog/atoms/core/TypeChoice.cc:\t\t// For now, just avoid throwing an exception. XXX FIXME.\",\n        \"./atomspace/opencog/atoms/core/RandomChoice.cc:// XXX FIXME - fix this so it can also choose a single value\",\n        \"./atomspace/opencog/atoms/core/Variables.cc:\t// XXX FIXME URE calls us with broken handle!!\",\n        \"./atomspace/opencog/atoms/core/TypeUtils.cc:\t\t\t\t\\\"Not implemented! TODO XXX FIXME\\\");\",\n        \"./atomspace/opencog/atoms/value/FormulaStream.cc:// XXX FIXME The update here is not thread-safe...\",\n",
        "description": "- fix this so it can also choose a single value\","
      },
      {
        "file": "scripts/analysis/analyze_issue_examples.py",
        "line": 38,
        "type": "FIXME",
        "content": "\"./atomspace/opencog/atoms/value/FormulaStream.cc:// XXX FIXME The update here is not thread-safe...\",",
        "context": "        \"./atomspace/opencog/atoms/core/RandomChoice.cc:// XXX FIXME - fix this so it can also choose a single value\",\n        \"./atomspace/opencog/atoms/core/Variables.cc:\t// XXX FIXME URE calls us with broken handle!!\",\n        \"./atomspace/opencog/atoms/core/TypeUtils.cc:\t\t\t\t\\\"Not implemented! TODO XXX FIXME\\\");\",\n        \"./atomspace/opencog/atoms/value/FormulaStream.cc:// XXX FIXME The update here is not thread-safe...\",\n        \"./atomspace/opencog/atoms/execution/Instantiator.cc:/// cleanly separated from each other. (XXX FIXME, these need to be\",\n        \"./atomspace/opencog/atoms/join/JoinLink.cc:/// i.e. to use UnorderedHandleSet instead of HandleSet. XXX FIXME.\",\n        \"./atomspace/opencog/atoms/flow/FilterLink.cc:\t\t\t\t\t\t\\\"Globbing for Values not implemented! FIXME!\\\");\",\n",
        "description": "The update here is not thread-safe...\","
      },
      {
        "file": "scripts/analysis/analyze_issue_examples.py",
        "line": 42,
        "type": "FIXME",
        "content": "\"./atomspace/opencog/query/InitiateSearchMixin.cc:\t\t// XXX FIXME; we should be using ptm->isVariable() instead !?\",",
        "context": "        \"./atomspace/opencog/atoms/execution/Instantiator.cc:/// cleanly separated from each other. (XXX FIXME, these need to be\",\n        \"./atomspace/opencog/atoms/join/JoinLink.cc:/// i.e. to use UnorderedHandleSet instead of HandleSet. XXX FIXME.\",\n        \"./atomspace/opencog/atoms/flow/FilterLink.cc:\t\t\t\t\t\t\\\"Globbing for Values not implemented! FIXME!\\\");\",\n        \"./atomspace/opencog/query/InitiateSearchMixin.cc:\t\t// XXX FIXME; we should be using ptm->isVariable() instead !?\",\n        \"./atomspace/opencog/query/RewriteMixin.cc:\t// See issue #950 and pull req #962. XXX FIXME later.\",\n        \"./atomspace/opencog/query/PatternMatchEngine.cc:/// XXX FIXME: this is currently a weak stop-gap measure to handle\"\n    ]\n",
        "description": "; we should be using ptm->isVariable() instead !?\","
      },
      {
        "file": "scripts/analysis/analyze_issue_examples.py",
        "line": 44,
        "type": "FIXME",
        "content": "\"./atomspace/opencog/query/PatternMatchEngine.cc:/// XXX FIXME: this is currently a weak stop-gap measure to handle\"",
        "context": "        \"./atomspace/opencog/atoms/flow/FilterLink.cc:\t\t\t\t\t\t\\\"Globbing for Values not implemented! FIXME!\\\");\",\n        \"./atomspace/opencog/query/InitiateSearchMixin.cc:\t\t// XXX FIXME; we should be using ptm->isVariable() instead !?\",\n        \"./atomspace/opencog/query/RewriteMixin.cc:\t// See issue #950 and pull req #962. XXX FIXME later.\",\n        \"./atomspace/opencog/query/PatternMatchEngine.cc:/// XXX FIXME: this is currently a weak stop-gap measure to handle\"\n    ]\n    \n    # Categorize these specific examples\n",
        "description": "this is currently a weak stop-gap measure to handle\""
      },
      {
        "file": "scripts/entelechy/entelechy_marker_analyzer.py",
        "line": 31,
        "type": "TODO",
        "content": "marker_type: str  # TODO, FIXME, STUB, PLACEHOLDER, etc.",
        "context": "    \"\"\"Represents a single code marker (TODO, FIXME, etc.).\"\"\"\n    file_path: str\n    line_number: int\n    marker_type: str  # TODO, FIXME, STUB, PLACEHOLDER, etc.\n    content: str\n    context_before: List[str]\n    context_after: List[str]\n",
        "description": ", FIXME, STUB, PLACEHOLDER, etc."
      },
      {
        "file": "scripts/entelechy/entelechy_introspection.py",
        "line": 600,
        "type": "TODO",
        "content": "r'TODO.*list',  # TODO lists in docs",
        "context": "        \n        # Patterns to exclude (informational, not actionable)\n        exclude_patterns = [\n            r'TODO.*list',  # TODO lists in docs\n            r'GNU',         # GNU license references\n            r'Copyright',   # Copyright notices\n            r'from\\s+TODO', # References to TODO files/modules\n",
        "description": "lists in docs"
      },
      {
        "file": "moses/moses/moses/optimization/star-anneal.cc",
        "line": 44,
        "type": "TODO",
        "content": "// XXX TODO the annealing temperature control code should be ported over",
        "context": "// Star-shaped search  //\n/////////////////////////\n\n// XXX TODO the annealing temperature control code should be ported over\n// to the hill-climbing code, thus rendering the below obsolete.  The\n// hill-climbing code is much more sophisticated in every way: correct\n// definition of the temperature, termination conditions, exploration of\n",
        "description": "the annealing temperature control code should be ported over"
      },
      {
        "file": "moses/moses/moses/optimization/particle-swarm.h",
        "line": 347,
        "type": "TODO",
        "content": "// TODO: Wind dispersion, but test without first",
        "context": "    void update_cont_particle(instance& temp, const instance& personal,\n            const instance& global, velocity::iterator vel, const field_set& fs);\n\n    // TODO: Wind dispersion, but test without first\n    // Make it later is easy.\n\npublic:\n",
        "description": "Wind dispersion, but test without first"
      },
      {
        "file": "moses/moses/moses/optimization/hill-climbing.h",
        "line": 110,
        "type": "TODO",
        "content": "// XXX TODO make sure this value is appropriately updated.",
        "context": "\n    // Range of scores for which to keep instances.  This *should* be\n    // set to the value given by metapopulation::useful_score_range().\n    // XXX TODO make sure this value is appropriately updated.\n    //\n    // The range of scores is used to keep the size of the deme in check.\n    // The issue is that, for large feature sets, a large number of knobs\n",
        "description": "make sure this value is appropriately updated."
      },
      {
        "file": "moses/moses/moses/optimization/particle-swarm.cc",
        "line": 198,
        "type": "TODO",
        "content": "// TODO: work in a better way to identify convergence.",
        "context": "            break;\n        }\n\n        // TODO: work in a better way to identify convergence.\n        not_improving = (has_improved) ? 0 : not_improving + 1;\n        if (not_improving > 3) {\n            logger().debug(\"Terminate Local Search: Convergence.\");\n",
        "description": "work in a better way to identify convergence."
      },
      {
        "file": "moses/moses/moses/optimization/particle-swarm.cc",
        "line": 237,
        "type": "TODO",
        "content": "// TODO: Explanation",
        "context": "        \"complexity\";\n}\n\n// TODO: Explanation\n// There's no explanation for this, it's just a temporary solution.\n// Maybe use adaptative pso, something like LPSO (Lander).\nunsigned particle_swarm::calc_swarm_size(const field_set& fs) {\n",
        "description": "Explanation"
      },
      {
        "file": "moses/moses/moses/metapopulation/metapopulation.h",
        "line": 535,
        "type": "TODO",
        "content": "// TODO: we may want to output the visited status as well",
        "context": "    // metapopulation. This function is used for fine logging to\n    // deeply probe the metapopulation.\n    //\n    // TODO: we may want to output the visited status as well\n    std::ostream& ostream_metapop(std::ostream&, int n = INT_MAX) const;\n\nprivate:\n",
        "description": "we may want to output the visited status as well"
      },
      {
        "file": "moses/moses/moses/metapopulation/merging.cc",
        "line": 261,
        "type": "FIXME",
        "content": "// XXX FIXME: we should use a pointer set for scored_combo_tree_set",
        "context": "        logger().debug(\"Compute behavioral score of %d selected candidates\",\n                       candidates.size());\n\n        // XXX FIXME: we should use a pointer set for scored_combo_tree_set\n        // This would avoid some pointless copying here and a few other\n        // places.  This is easier said than done, because the stupid\n        // domination code is so snarky and icky.  Domination should die.\n",
        "description": "we should use a pointer set for scored_combo_tree_set"
      },
      {
        "file": "moses/moses/moses/metapopulation/merging.cc",
        "line": 404,
        "type": "TODO",
        "content": "// TODO: Make population cap size-sensitive to exemplar complexity.",
        "context": "    // formula was arrived at via some ad-hoc experimentation.  A default\n    // value of _params.cap_coef=50 seems to work well.\n    //\n    // TODO: Make population cap size-sensitive to exemplar complexity.\n    // Large exemplars should result in smaller population sizes to maintain\n    // efficiency. Consider implementing adaptive sizing based on exemplar metrics.\n    //\n",
        "description": "Make population cap size-sensitive to exemplar complexity."
      },
      {
        "file": "moses/moses/moses/metapopulation/merging.cc",
        "line": 552,
        "type": "FIXME",
        "content": "// XXX FIXME looks to me like it++ can often be collaed twice within this loop!",
        "context": "                    }\n                }\n\n// XXX FIXME looks to me like it++ can often be collaed twice within this loop!\n                prev_it = it++;\n            }\n\n",
        "description": "looks to me like it++ can often be collaed twice within this loop!"
      },
      {
        "file": "moses/moses/moses/metapopulation/metapopulation.cc",
        "line": 222,
        "type": "FIXME",
        "content": "// XXX FIXME should probably not recompute every time ...",
        "context": "    if (not _params.do_boosting)\n        return _best_cscore;\n\n    // XXX FIXME should probably not recompute every time ...\n    // need to figure who is calling this method, and what they are expecting.\n    return _cscorer.get_cscore(_ensemble.get_ensemble());\n}\n",
        "description": "should probably not recompute every time ..."
      },
      {
        "file": "moses/moses/moses/scoring/scoring_base.h",
        "line": 124,
        "type": "TODO",
        "content": "// XXX TODO should be a std::valarray not a vector.",
        "context": "\n    /// A vector of per-bscore weights, used to tote up the behavioral\n    /// score into a single number.\n    // XXX TODO should be a std::valarray not a vector.\n    virtual void update_weights(const std::vector<double>&);\n\n    /// Return the amount by which the bscore differs from a perfect\n",
        "description": "should be a std::valarray not a vector."
      },
      {
        "file": "moses/moses/moses/scoring/time_dispersion.cc",
        "line": 43,
        "type": "TODO",
        "content": "// TODO multipler other than 1 is not supported yet",
        "context": "      _granularity(granularity), _multiplier(multiplier),\n      _pressure(time_dispersion_pressure), _exponent(time_dispersion_exponent)\n{\n    // TODO multipler other than 1 is not supported yet\n    OC_ASSERT(_multiplier == 1, \"Multiplier other than 1 is not supported yet\");\n\n    // Set of timestamp classes\n",
        "description": "multipler other than 1 is not supported yet"
      },
      {
        "file": "moses/moses/moses/scoring/scoring_base.cc",
        "line": 152,
        "type": "FIXME",
        "content": "// XXX FIXME complexity_t should be a double not an int ...",
        "context": "        norm += w;\n    }\n\n    // XXX FIXME complexity_t should be a double not an int ...\n    return (complexity_t) floor (cpxy / norm + 0.5);\n}\n\n",
        "description": "complexity_t should be a double not an int ..."
      },
      {
        "file": "moses/moses/moses/scoring/discriminating_bscore.cc",
        "line": 512,
        "type": "TODO",
        "content": "// XXX TODO -- should not return the penalties as part of the bscore,",
        "context": "                  float hardness)\n    : discriminating_bscore(ct, min_recall, max_recall, hardness)\n{\n    // XXX TODO -- should not return the penalties as part of the bscore,\n    // since this messes up boosting.\n    _size = ct.size() + 2;\n}\n",
        "description": "-- should not return the penalties as part of the bscore,"
      },
      {
        "file": "moses/moses/moses/scoring/discriminating_bscore.cc",
        "line": 646,
        "type": "TODO",
        "content": "// XXX TODO FIXME is this really correct?",
        "context": "/// Return the break-even-point for this ctable row.\nscore_t bep_bscore::get_variable(score_t pos, score_t neg, unsigned cnt) const\n{\n    // XXX TODO FIXME is this really correct?\n    double best_possible_precision = pos / (cnt * _true_total);\n    double best_possible_recall = 1.0 / _true_total;\n    return (best_possible_precision + best_possible_recall) / 2;\n",
        "description": "FIXME is this really correct?"
      },
      {
        "file": "moses/moses/moses/scoring/discriminating_bscore.cc",
        "line": 655,
        "type": "TODO",
        "content": "// XXX TODO FIXME is this really correct?",
        "context": "/// Return the difference for this ctable row.\nscore_t bep_bscore::get_fixed(score_t pos, score_t neg, unsigned cnt) const\n{\n    // XXX TODO FIXME is this really correct?\n    double best_possible_precision = pos / (cnt);\n    double best_possible_recall = (0.0 < pos) ? 1.0 : 0.0;\n    return fabs(best_possible_precision - best_possible_recall);\n",
        "description": "FIXME is this really correct?"
      },
      {
        "file": "moses/moses/moses/scoring/discriminating_bscore.cc",
        "line": 707,
        "type": "TODO",
        "content": "// XXX TODO FIXME is this really correct?",
        "context": "// generation of best-possible score.\nscore_t f_one_bscore::get_fixed(score_t pos, score_t neg, unsigned cnt) const\n{\n    // XXX TODO FIXME is this really correct?\n    return 1.0;\n}\n\n",
        "description": "FIXME is this really correct?"
      },
      {
        "file": "moses/moses/moses/scoring/discriminating_bscore.cc",
        "line": 714,
        "type": "TODO",
        "content": "// XXX TODO FIXME is this really correct?",
        "context": "/// Return the f_one for this ctable row.\nscore_t f_one_bscore::get_variable(score_t pos, score_t neg, unsigned cnt) const\n{\n    // XXX TODO FIXME is this really correct?\n    double best_possible_precision = pos / cnt;\n    double best_possible_recall = 1.0;\n    double f_one = 2 * best_possible_precision * best_possible_recall\n",
        "description": "FIXME is this really correct?"
      },
      {
        "file": "moses/moses/moses/moses/local_moses.cc",
        "line": 180,
        "type": "TODO",
        "content": "// TODO use the option of the output",
        "context": "                   << \"\\t\" << ds.max;  // max distance\n\n                // diversity stats over all best n candidates of the metapopulation\n                // TODO use the option of the output\n                auto best_ds = mp.gather_diversity_stats(pa.max_cnd_output);\n                ss << \"\\t\" << best_ds.count // number of pairs of candidates\n                   << \"\\t\" << best_ds.mean  // average distance\n",
        "description": "use the option of the output"
      },
      {
        "file": "moses/moses/moses/moses/partial.cc",
        "line": 96,
        "type": "TODO",
        "content": "// TODO: Improve generation tracking by getting actual number",
        "context": "\n        _moses_params.max_evals -= _num_evals;\n\n        // TODO: Improve generation tracking by getting actual number\n        // of generations run from MOSES and subtracting it here.\n        // Currently no easy API exists to retrieve this information.\n        _moses_params.max_gens -= _num_gens;\n",
        "description": "Improve generation tracking by getting actual number"
      },
      {
        "file": "moses/moses/moses/moses/neighborhood_sampling.h",
        "line": 327,
        "type": "TODO",
        "content": "// XXX TODO, unroll the last tail call, just like the single-bit",
        "context": "        else\n        {\n            // Recursive call, moved for one position\n            // XXX TODO, unroll the last tail call, just like the single-bit\n            // knob case, below.\n            out = vary_n_knobs(fs, tmp_inst, dist, starting_index + 1, out, end);\n            // Left<->Right\n",
        "description": ", unroll the last tail call, just like the single-bit"
      },
      {
        "file": "moses/moses/moses/moses/moses_main.h",
        "line": 102,
        "type": "TODO",
        "content": "// XXX TODO this should be fixed, someday...",
        "context": "        // messages.  In fact, the mpi workers should not even have\n        // a printer at all, or use a null_printer.  Unfortunately,\n        // the current code structure makes this hard to implement.\n        // XXX TODO this should be fixed, someday...\n        if (is_mpi && metapop.size() == 0)\n            return;\n\n",
        "description": "this should be fixed, someday..."
      },
      {
        "file": "moses/moses/moses/moses/mpi_moses.cc",
        "line": 202,
        "type": "TODO",
        "content": "// XXX TODO -- trim the deme down, before sending, by using the worst acceptable score.",
        "context": "/// send_deme -- send the completed deme from the worker back to root\n///\n/// This sends a pretty big glob.\n// XXX TODO -- trim the deme down, before sending, by using the worst acceptable score.\nvoid moses_mpi_comm::send_deme(const metapopulation& mp, int n_evals)\n{\n    MPI::COMM_WORLD.Send(&n_evals, 1, MPI::INT, ROOT_NODE, MSG_NUM_EVALS);\n",
        "description": "-- trim the deme down, before sending, by using the worst acceptable score."
      },
      {
        "file": "moses/moses/moses/moses/mpi_moses.cc",
        "line": 318,
        "type": "TODO",
        "content": "// XXX TODO should probably fetch max_time from somewhere...",
        "context": "            continue; // Continue to next exemplar\n        }\n\n        // XXX TODO should probably fetch max_time from somewhere...\n        time_t max_time = INT_MAX;\n        dex.optimize_demes(max_evals, max_time);\n\n",
        "description": "should probably fetch max_time from somewhere..."
      },
      {
        "file": "moses/moses/moses/moses/mpi_moses.cc",
        "line": 616,
        "type": "TODO",
        "content": "// XXX TODO instead of overwritting the demeID it should be",
        "context": "        scored_combo_tree_set candidates;\n        stats.n_expansions ++;\n\n        // XXX TODO instead of overwritting the demeID it should be\n        // correctly defined by the worker and send back to the\n        // dispatcher. That way we can have the breadth_first\n        // componant of the demeID right.\n",
        "description": "instead of overwritting the demeID it should be"
      },
      {
        "file": "moses/moses/moses/moses/types.h",
        "line": 210,
        "type": "TODO",
        "content": "// TODO this should be a std::valarray not std::vector but I am too",
        "context": "/// in reference to a particular table of data.  Exactly which tree it\n/// is, and which table, is implicit.\n//\n// TODO this should be a std::valarray not std::vector but I am too\n// lazy to make the switch right now.\nstruct behavioral_score : public std::vector<score_t>\n{\n",
        "description": "this should be a std::valarray not std::vector but I am too"
      },
      {
        "file": "moses/moses/moses/deme/deme_expander.cc",
        "line": 441,
        "type": "TODO",
        "content": "// TODO: DO NOT CHANGE THE MAX SCORE IF USER SET IT: BUT THAT",
        "context": "                // dynamically selected, it might be less that the global target;\n                // that is, the deme might not be able to reach the best score.)\n                //\n                // TODO: DO NOT CHANGE THE MAX SCORE IF USER SET IT: BUT THAT\n                // OPTION ISN'T GLOBAL WHAT TO DO?\n                //\n                // But why would we want to over-ride the best-possible score?\n",
        "description": "DO NOT CHANGE THE MAX SCORE IF USER SET IT: BUT THAT"
      },
      {
        "file": "moses/moses/moses/deme/deme_expander.cc",
        "line": 457,
        "type": "TODO",
        "content": "// TODO: re-enable that once best_possible_bscore is fixed",
        "context": "                              \"terminate deme search. Except I think this \"\n                              \"is fixed now. It needs review and testing.\");\n\n                // TODO: re-enable that once best_possible_bscore is fixed\n                // I think its now fixed, but I'm not sure.  It needs to be\n                // reviewed and tested.\n#if THIS_IS_DISABLED_UNTIL_ABOVE_IS_FIXED\n",
        "description": "re-enable that once best_possible_bscore is fixed"
      },
      {
        "file": "moses/moses/moses/eda/replacement.h",
        "line": 62,
        "type": "TODO",
        "content": "// TODO: I think it might be a little more efficent to use the",
        "context": "// Replace the most similar individual, where similarity is determined by\n// the hamming distance.\n//\n// TODO: I think it might be a little more efficent to use the\n// hamming_distance as a sort comparison operator, and hand off the whole\n// thing to std:nth_element, and let that class figure out who is close or\n// not.  This avoids the use of doubly-nested loops, and multiple redundant\n",
        "description": "I think it might be a little more efficent to use the"
      },
      {
        "file": "moses/moses/moses/representation/representation.cc",
        "line": 51,
        "type": "TODO",
        "content": "// XXX TODO: One might think that varying the stepsize, i.e. shrinking",
        "context": "// Stepsize should be roughly the standard-deviation of the expected\n// distribution of the contin variables.\n//\n// XXX TODO: One might think that varying the stepsize, i.e. shrinking\n// it, as the optimizers tune into a specific value, would be a good\n// thing (so that the optimizer could tune to a more precise value).\n// Unfortunately, a simple experiment in tuning (see below, surrounded\n",
        "description": "One might think that varying the stepsize, i.e. shrinking"
      },
      {
        "file": "moses/moses/moses/representation/representation.cc",
        "line": 238,
        "type": "TODO",
        "content": "// XXX TODO need to add support for \"term algebra\" knobs",
        "context": "/// the instance supplied as the argument.\nvoid representation::transform(const instance& inst)\n{\n    // XXX TODO need to add support for \"term algebra\" knobs\n\n    contin_map_it ckb = contin.begin();\n    for (field_set::const_contin_iterator ci = _fields.begin_contin(inst);\n",
        "description": "need to add support for \"term algebra\" knobs"
      },
      {
        "file": "moses/moses/moses/representation/build_knobs.cc",
        "line": 318,
        "type": "TODO",
        "content": "/// TODO: measure and compare the resulting performance.",
        "context": "/// can be rather incredibly costly, especially when the exemplars start\n/// getting large.  So the real question is: is the performance cost of\n/// this routine worth the eventual savings when scoring instances?\n/// TODO: measure and compare the resulting performance.\n//\n// Notes to self: hmm. in 5-parity problem, about 2/3 of knobs are\n// disallowed! viz of 6738 probes, 4007 knobs are completely disallowed.\n",
        "description": "measure and compare the resulting performance."
      },
      {
        "file": "moses/moses/moses/representation/build_knobs.cc",
        "line": 491,
        "type": "TODO",
        "content": "// TODO: should bias the selection of these, so that",
        "context": "        }\n    }\n\n    // TODO: should bias the selection of these, so that\n    // larger subtrees are preferred .. !? why?\n\n    unsigned max_pairs = permitted_perms.size();\n",
        "description": "should bias the selection of these, so that"
      },
      {
        "file": "moses/moses/moses/representation/build_knobs.cc",
        "line": 583,
        "type": "TODO",
        "content": "// TODO: Benchmark and clarify optimal breakeven point across different problem sizes.",
        "context": "    // The number of 30K is a wild guesstimate, based on recent\n    // measurements of relatively simple exemplars; its maybe even\n    // too low.  For large exemplars, it might be too big !?\n    // TODO: Benchmark and clarify optimal breakeven point across different problem sizes.\n#define BREAKEVEN 30000\n    size_t np = perms.size();\n    int nthr = 1 + np / BREAKEVEN;\n",
        "description": "Benchmark and clarify optimal breakeven point across different problem sizes."
      },
      {
        "file": "moses/moses/moses/representation/build_knobs.cc",
        "line": 692,
        "type": "TODO",
        "content": "// XXX TODO: Is this really optimal?  The below adds an entire copy",
        "context": "        }\n    }\n\n    // XXX TODO: Is this really optimal?  The below adds an entire copy\n    // of the tree at it, which clearly increases the overall complexity.\n    // But is this really a wise thig to do? It seems gratuitous, and it's\n    // not obvious that knobs from this flipped tree will yeild benefits,\n",
        "description": "Is this really optimal?  The below adds an entire copy"
      },
      {
        "file": "moses/moses/moses/representation/build_knobs.cc",
        "line": 1183,
        "type": "TODO",
        "content": "//TODO: should bias the selection of these (and possibly choose larger subtrees)",
        "context": "        perms.push_back(tr);\n\n    //and n random pairs out of the total  2 * choose(n,2) = n * (n - 1) of these\n    //TODO: should bias the selection of these (and possibly choose larger subtrees)\n    lazy_random_selector randpair(n * (n - 1));\n\n    dorepeat(n) {\n",
        "description": "should bias the selection of these (and possibly choose larger subtrees)"
      },
      {
        "file": "moses/moses/moses/representation/build_knobs.cc",
        "line": 1344,
        "type": "FIXME",
        "content": "//FIXME: now just attaches to the first output",
        "context": "    cout << \"Created node: \" << new_node << endl;\n\n    //now attach the subtree to the hidden nodes\n    //FIXME: now just attaches to the first output\n    sib_it first_hidden = it.begin();\n\n    _exemplar.insert_subtree(first_hidden.begin(),new_node.begin());\n",
        "description": "now just attaches to the first output"
      },
      {
        "file": "moses/moses/moses/main/problem-params.h",
        "line": 46,
        "type": "FIXME",
        "content": "// XXX FIXME TODO The structure below should be split into multiple",
        "context": "\nnamespace opencog { namespace moses {\n\n// XXX FIXME TODO The structure below should be split into multiple\n// parts, with each sub-part responsible for picking out the argv's\n// that it cares about. Unfortunately, this requires getting rid of\n// boost::program_options (because boost::program_options does not\n",
        "description": "TODO The structure below should be split into multiple"
      },
      {
        "file": "moses/moses/moses/main/table-problems.cc",
        "line": 138,
        "type": "FIXME",
        "content": "// XXX FIXME -- the multiple tables should be merged into one.",
        "context": "    }\n    logger().info(\"Number of rows in tables = %d\", num_rows);\n\n    // XXX FIXME -- the multiple tables should be merged into one.\n    ctable = _ctables.front();\n    table = _tables.front();\n\n",
        "description": "-- the multiple tables should be merged into one."
      },
      {
        "file": "moses/moses/moses/main/table-problems.cc",
        "line": 150,
        "type": "FIXME",
        "content": "// XXX FIXME .. check that they all have the same signature.",
        "context": "    arity = table.get_arity();\n\n    // Check that all input data files have the same arity\n    // XXX FIXME .. check that they all have the same signature.\n    if (_tables.size() > 1) {\n        for (size_t i = 1; i < _tables.size(); ++i) {\n            combo::arity_t test_arity = _tables[i].get_arity();\n",
        "description": ".. check that they all have the same signature."
      },
      {
        "file": "moses/moses/moses/main/problem-params.cc",
        "line": 169,
        "type": "TODO",
        "content": "// XXX TODO: make this print correctly, instead of using brackets.",
        "context": "    using namespace std;\n\n    // Declare the supported options.\n    // XXX TODO: make this print correctly, instead of using brackets.\n    desc.add_options()\n\n        // General options\n",
        "description": "make this print correctly, instead of using brackets."
      },
      {
        "file": "moses/moses/comboreduct/type_checker/type_tree.cc",
        "line": 627,
        "type": "TODO",
        "content": "// XXX TODO the code below was modified to allow arg lists of",
        "context": "            // then check that a1 inherits from T1, and that a2, a3\n            // and a4 inherit from T2.  T3 is the output type.\n\n            // XXX TODO the code below was modified to allow arg lists of\n            // mixed type, e.g. so that the cond primitive could be\n            // supported (as the current definition of cond alternates\n            // between boolean-valued predicates, and the result type).\n",
        "description": "the code below was modified to allow arg lists of"
      },
      {
        "file": "moses/moses/comboreduct/type_checker/type_tree.h",
        "line": 235,
        "type": "TODO",
        "content": "// TODO : lambda",
        "context": "//\n// intersection of ill_formed and T is ill_formed\n//\n// TODO : lambda\n//\n// Of course the case if T1 inherit T2 then interection of T1 and T2\n// is T1 is also implemented. If the interection is ill_formed or\n",
        "description": "lambda"
      },
      {
        "file": "moses/moses/comboreduct/interpreter/interpreter.cc",
        "line": 336,
        "type": "TODO",
        "content": "// XXX TODO: contin_if should go away.",
        "context": "            return (i == id::logical_true ? 1.0 : 0.0);\n        }\n\n        // XXX TODO: contin_if should go away.\n        case id::contin_if :\n        case id::cond : {\n            sib_it sib = it.begin();\n",
        "description": "contin_if should go away."
      },
      {
        "file": "moses/moses/comboreduct/interpreter/eval.cc",
        "line": 563,
        "type": "TODO",
        "content": "// XXX TODO: contin_if should go away.",
        "context": "            return eval_throws_tree(new_bmap, lambda_expr);\n        }\n\n        // XXX TODO: contin_if should go away.\n        case id::contin_if :\n        case id::cond : {\n            sib_it sib = it.begin();\n",
        "description": "contin_if should go away."
      },
      {
        "file": "moses/moses/comboreduct/combo/descriptions.cc",
        "line": 45,
        "type": "TODO",
        "content": "// ToDo: would be nice to have a more Caml/Haskell style syntax here,",
        "context": "// with builtins as indicies, within the singleton class builtin_properties.\n// This array should not have any other usages.\n//\n// ToDo: would be nice to have a more Caml/Haskell style syntax here,\n// right?\nstatic const builtin_description bd[] =\n{\n",
        "description": "would be nice to have a more Caml/Haskell style syntax here,"
      },
      {
        "file": "moses/moses/comboreduct/table/table.h",
        "line": 1352,
        "type": "TODO",
        "content": "// XXX TODO, it would be easier if KLD took a sorted list",
        "context": "            }\n        }\n\n        // XXX TODO, it would be easier if KLD took a sorted list\n        // as the argument.\n        std::vector<contin_t> p, q;\n        for (auto pr : sorted_list) {\n",
        "description": ", it would be easier if KLD took a sorted list"
      },
      {
        "file": "moses/moses/comboreduct/table/table.h",
        "line": 1366,
        "type": "TODO",
        "content": "// XXX TODO remove this print, for better performance.",
        "context": "        // Also a problem, this is returning values greater than 1.0;\n        // I thought that IC was supposed to max out at 1.0 !?\n        contin_t ic = - KLD(p,q);\n        // XXX TODO remove this print, for better performance.\n        unsigned idx = *(fs.begin());\n        logger().debug() <<\"Contin MI for feat=\" << idx << \" ic=\" << ic;\n        return ic;\n",
        "description": "remove this print, for better performance."
      },
      {
        "file": "moses/moses/comboreduct/table/table.cc",
        "line": 420,
        "type": "TODO",
        "content": "// XXX TODO replace this by the util p_norm function.",
        "context": "    return rhs.get_label() == label;\n}\n\n// XXX TODO replace this by the util p_norm function.\ncontin_t OTable::abs_distance(const OTable& ot) const\n{\n    OC_ASSERT(ot.size() == size());\n",
        "description": "replace this by the util p_norm function."
      },
      {
        "file": "moses/moses/comboreduct/table/table.cc",
        "line": 445,
        "type": "TODO",
        "content": "// XXX TODO replace this by the util p_norm function.",
        "context": "    return res;\n}\n\n// XXX TODO replace this by the util p_norm function.\ncontin_t OTable::sum_squared_error(const OTable& ot) const\n{\n    OC_ASSERT(ot.size() == size());\n",
        "description": "replace this by the util p_norm function."
      },
      {
        "file": "moses/moses/comboreduct/table/table.cc",
        "line": 859,
        "type": "TODO",
        "content": "// XXX TODO replace this by the util p_norm function.",
        "context": "\n// -------------------------------------------------------\n\n// XXX TODO replace this by the util p_norm function.\ncomplete_truth_table::size_type\ncomplete_truth_table::hamming_distance(const complete_truth_table& other) const\n{\n",
        "description": "replace this by the util p_norm function."
      },
      {
        "file": "moses/moses/comboreduct/reduct/logical_rules.cc",
        "line": 102,
        "type": "TODO",
        "content": "// XXX TODO: I don't understand why this is not damaging contin_if  !??",
        "context": "    // Most nodes take simple lists; but not cond. Cond takes clauses,\n    // which are pairs. If we remove the condition, we must also remove\n    // the consequent.\n// XXX TODO: I don't understand why this is not damaging contin_if  !??\n// But .. umm, maybe build_knobs is not creating any kinds of contin_if's\n// that can be damaged... well, no matter, because thes if's will be\n// replaced by cond... \n",
        "description": "I don't understand why this is not damaging contin_if  !??"
      },
      {
        "file": "moses/moses/comboreduct/reduct/logical_rules.cc",
        "line": 669,
        "type": "stub",
        "content": "// stub out, for performance.",
        "context": "                              make_counting_iterator(current.end()));\n\n#if DEBUG\n        // stub out, for performance.\n        OC_ASSERT(std::is_sorted(dominant.begin(),dominant.end(), comp),\n                  \"dominant subtree_set should be sorted (reduce_and)\");\n#endif\n",
        "description": "// stub out, for performance."
      },
      {
        "file": "moses/moses/comboreduct/reduct/logical_rules.cc",
        "line": 699,
        "type": "stub",
        "content": "// stubbed out for performance",
        "context": "            tr.validate();\n\n#if DEBUG\n            // stubbed out for performance\n            OC_ASSERT(std::is_sorted(command.begin(),command.end(),comp),\n                      \"command subtree_set should be sorted (reduce_and)\");\n            OC_ASSERT(std::is_sorted(handle_set.begin(),handle_set.end(),comp),\n",
        "description": "// stubbed out for performance"
      },
      {
        "file": "moses/moses/comboreduct/reduct/mixed_rules.cc",
        "line": 1228,
        "type": "TODO",
        "content": "//check if 0<-(y+pi) -> false //TODO",
        "context": "                }\n            }\n            else if(*copy_tr.begin()==id::logical_false) {\n                //check if 0<-(y+pi) -> false //TODO\n                combo_tree copy2_tr = tr.subtree(sib_it(it), tr.next_sibling(sib_it(it)));\n                //copy old assumptions, begin\n                sib_it bna = copy2_tr.begin(); //before new assumption\n",
        "description": ""
      },
      {
        "file": "moses/moses/comboreduct/reduct/contin_rules.cc",
        "line": 964,
        "type": "TODO",
        "content": "// TODO:  sin(*(-1 x)) -> -sin(x)",
        "context": "// or more generally\n// sin(sum x_i + sum c_j) -> sin(sum x_i + ((sum c_j)+pi)%2pi -pi\n//\n// TODO:  sin(*(-1 x)) -> -sin(x)\n// The above is frequently seen in real-life ...\nvoid reduce_sin::operator()(combo_tree& tr, combo_tree::iterator it) const\n{\n",
        "description": "sin(*(-1 x)) -> -sin(x)"
      },
      {
        "file": "moses/moses/comboreduct/main/action-reductor.cc",
        "line": 94,
        "type": "TODO",
        "content": "// TODO -- replace this by cond",
        "context": "    cout << \"output type \" << ba2->get_output_type_tree() << endl;\n\n#if 0\n    // TODO -- replace this by cond\n    cout << \"6----------------\" << endl;\n\n    cout << \"arity \" << (int)get_arity(id::boolean_if) << endl;\n",
        "description": "-- replace this by cond"
      },
      {
        "file": "moses/moses/comboreduct/main/eval-table.cc",
        "line": 147,
        "type": "FIXME",
        "content": "// XXX FIXME",
        "context": "    }\n\n    // HERE WE ARE ASSUMING THAT THE INPUT FILE HAS A HEADER!!!\n// XXX FIXME\n    vector<string> header = get_header(pa.input_table_file);\n\n    // Add to ignore_values (header - all_unique_variables - target feature)\n",
        "description": ""
      },
      {
        "file": "atomspace-rocks/opencog/persist/monospace/MonoIO.cc",
        "line": 1004,
        "type": "FIXME",
        "content": "// XXX FIXME. We would like to call",
        "context": "\nvoid MonoStorage::storeAtomSpace(const AtomSpace* table)\n{\n\t// XXX FIXME. We would like to call\n\t// Options::PrepareForBulkLoad() here, but its too late, this\n\t// can only be set when opening the DB. Should we maybe close\n\t// and reopen the DB? This would be ... conducive of weird bugs.\n",
        "description": ". We would like to call"
      },
      {
        "file": "atomspace-rocks/opencog/persist/rocks/RocksIO.cc",
        "line": 1371,
        "type": "FIXME",
        "content": "// XXX FIXME. We would like to call",
        "context": "\t                        and nullptr != getAtomSpace())\n\t\tconvertForFrames(HandleCast(getAtomSpace()));\n\n\t// XXX FIXME. We would like to call\n\t// Options::PrepareForBulkLoad() here, but its too late, this\n\t// can only be set when opening the DB. Should we maybe close\n\t// and reopen the DB? This would be ... conducive of weird bugs.\n",
        "description": ". We would like to call"
      },
      {
        "file": "components/integration/opencog/opencog/nlp/fuzzy/Fuzzy.cc",
        "line": 82,
        "type": "TODO",
        "content": "// TODO: Extend to find similar links as well",
        "context": "    {\n        if (h->is_node())\n        {\n            // TODO: Extend to find similar links as well\n            if (lp->get_type() == SIMILARITY_LINK)\n                sl.emplace_back(lp->get_handle());\n\n",
        "description": "Extend to find similar links as well"
      },
      {
        "file": "components/integration/opencog/opencog/nlp/fuzzy/FuzzyMatchBasic.cc",
        "line": 107,
        "type": "TODO",
        "content": "// TODO: May use Truth Value instead",
        "context": "\tdouble similarity = common_nodes.size();\n\n\t// Roughly estimate how \"rare\" each node is by using 1 / incoming set size\n\t// TODO: May use Truth Value instead\n\t// for (const Handle& common_node : common_nodes)\n\t// \tsimilarity += 1.0 / common_node->getIncomingSetSize();\n\n",
        "description": "May use Truth Value instead"
      },
      {
        "file": "components/integration/opencog/opencog/openpsi/OpenPsiSCM.cc",
        "line": 92,
        "type": "TODO",
        "content": "// TODO: Should this be a singleton? What could be the issues that need",
        "context": "  const Handle& goal, const TruthValuePtr stv, const Handle& category)\n{\n  AtomSpacePtr asp = SchemeSmob::ss_get_env_as(\"psi-rule\");\n  // TODO: Should this be a singleton? What could be the issues that need\n  // to be handled? How to handle multiple atomspace, maybe a singleton per\n  // atomspace?\n  Handle rule = openpsi_cache(asp.get()).add_rule(context, action, goal, stv);\n",
        "description": "Should this be a singleton? What could be the issues that need"
      },
      {
        "file": "components/integration/opencog/opencog/openpsi/OpenPsiSCM.cc",
        "line": 96,
        "type": "TODO",
        "content": "// TODO: Add to multiple categories using scheme rest list.",
        "context": "  // to be handled? How to handle multiple atomspace, maybe a singleton per\n  // atomspace?\n  Handle rule = openpsi_cache(asp.get()).add_rule(context, action, goal, stv);\n  // TODO: Add to multiple categories using scheme rest list.\n  openpsi_cache(asp.get()).add_to_category(rule, category);\n  return rule;\n}\n",
        "description": "Add to multiple categories using scheme rest list."
      },
      {
        "file": "components/integration/opencog/opencog/openpsi/OpenPsiRules.cc",
        "line": 102,
        "type": "TODO",
        "content": "// TODO But why make the add_category public then?",
        "context": "{\n  _as->add_link(MEMBER_LINK, rule, category);\n  // Add the category just in case it hasn't been declared.\n  // TODO But why make the add_category public then?\n  add_category(category);\n  _category_index[category].insert(rule);\n\n",
        "description": "But why make the add_category public then?"
      },
      {
        "file": "components/integration/opencog/opencog/openpsi/OpenPsiSatisfier.cc",
        "line": 42,
        "type": "TODO",
        "content": "// TODO: Saperated component patterns aren't handled by this function",
        "context": "bool OpenPsiSatisfier::grounding(const HandleMap &var_soln,\n                                  const HandleMap &term_soln)\n{\n  // TODO: Saperated component patterns aren't handled by this function\n  // as PMCGroundings is used instead. Update to handle such cases.\n\n  // The psi-rule weight calculations could be done here.\n",
        "description": "Saperated component patterns aren't handled by this function"
      },
      {
        "file": "components/integration/opencog/opencog/openpsi/OpenPsiSatisfier.cc",
        "line": 56,
        "type": "TODO",
        "content": "// TODO: If we are here it means the suggested groundings doesn't have",
        "context": "      }\n    }\n\n    // TODO: If we are here it means the suggested groundings doesn't have\n    // VariableNodes, and can be cached. This doesn't account for terms\n    // that are under QuoteLink, or other similar type links. How should\n    // such cases be handled?\n",
        "description": "If we are here it means the suggested groundings doesn't have"
      },
      {
        "file": "components/integration/opencog/opencog/openpsi/OpenPsiSatisfier.cc",
        "line": 76,
        "type": "TODO",
        "content": "// TODO: This happens when InitiateSearchCB::no_search has groundings.",
        "context": "    _implicator -> _satisfiability_cache[_pattern_body] = var_soln;\n    return true;\n  } else {\n    // TODO: This happens when InitiateSearchCB::no_search has groundings.\n    // Cases for when this happens hasn't been tested yet. Explore the\n    // behavior and find a better solution. For now, log it and continue\n    // searching.\n",
        "description": "This happens when InitiateSearchCB::no_search has groundings."
      },
      {
        "file": "components/integration/opencog/opencog/openpsi/OpenPsiImplicator.cc",
        "line": 49,
        "type": "TODO",
        "content": "// TODO: Add cache per atomspace.",
        "context": "  Handle query_body = query->get_pattern().body;\n\n  // Always update cache to clear any previous result.\n  // TODO: Add cache per atomspace.\n  _satisfiability_cache.erase(query_body);\n  _pattern_seen.insert(query_body);\n\n",
        "description": "Add cache per atomspace."
      },
      {
        "file": "components/integration/opencog/opencog/openpsi/OpenPsiImplicator.h",
        "line": 44,
        "type": "TODO",
        "content": "// TODO Why would one need to reset during psi-loop?",
        "context": "  friend class OpenPsiSatisfier;\n\n  // Needed for resetting private cache.\n  // TODO Why would one need to reset during psi-loop?\n  friend class ::OpenPsiImplicatorUTest;\n\npublic:\n",
        "description": "Why would one need to reset during psi-loop?"
      },
      {
        "file": "components/integration/opencog/opencog/openpsi/OpenPsiRules.h",
        "line": 105,
        "type": "TODO",
        "content": "// TODO:add predicate to check for membership of category.",
        "context": "   * @param new_category The node reprsenting the new category.\n   * @return ConceptNode that represents the category.\n   */\n   // TODO:add predicate to check for membership of category.\n  Handle add_category(const Handle& new_category);\n\n  /**\n",
        "description": "add predicate to check for membership of category."
      },
      {
        "file": "components/integration/opencog/opencog/openpsi/OpenPsiRules.h",
        "line": 126,
        "type": "TODO",
        "content": "// TODO Should these entries be a member of Rules class?",
        "context": "   * where queryis a PatternLink that isn't added to the atomspace, and\n   * is used to check if the rule is satisfiable.\n   */\n  // TODO Should these entries be a member of Rules class?\n  typedef std::tuple<HandleSeq, Handle, Handle, PatternLinkPtr> PsiTuple;\n\n  /**\n",
        "description": "Should these entries be a member of Rules class?"
      },
      {
        "file": "components/integration/opencog/opencog/openpsi/OpenPsiRules.h",
        "line": 136,
        "type": "TODO",
        "content": "// TODO: Using names that are prefixed with \"OpenPsi: \" might be a bad idea,",
        "context": "   */\n  std::map<Handle, PsiTuple> _psi_rules;\n\n  // TODO: Using names that are prefixed with \"OpenPsi: \" might be a bad idea,\n  // because it might hinder interoperability with other components that\n  // expect an explicit ontological representation. For historic reasons we\n  // continue using such convention but should be replaced with graph that\n",
        "description": "Using names that are prefixed with \"OpenPsi: \" might be a bad idea,"
      },
      {
        "file": "components/language/learn/attic/run-ull-2019/SchemeEval.cc",
        "line": 1029,
        "type": "TODO",
        "content": "// TODO: it would be nice to pass exceptions on through, but",
        "context": "\t}\n\texpr = scm_cons(sfunc, expr);\n\n\t// TODO: it would be nice to pass exceptions on through, but\n\t// this currently breaks unit tests.\n\t// if (_in_eval)\n\t//    return scm_eval(expr, scm_interaction_environment());\n",
        "description": "it would be nice to pass exceptions on through, but"
      },
      {
        "file": "components/language/learn/attic/run-ull-2019/SchemeEval.cc",
        "line": 1182,
        "type": "FIXME",
        "content": "// XXX FIXME only a subset is needed.",
        "context": "\nvoid SchemeEval::init_scheme(void)\n{\n\t// XXX FIXME only a subset is needed.\n\tSchemeEval sch;\n}\n\n",
        "description": "only a subset is needed."
      },
      {
        "file": "components/language/lg-atomese/opencog/nlp/lg-dict/LGDictNode.cc",
        "line": 135,
        "type": "empty_function",
        "content": "void opencog_nlp_lgparse_init(void) {}",
        "context": "\t// Module initialization function for Guile FFI\n\t// Empty because initialization is handled by C++ constructors\n\t// and the LGDictNode factory registration\n\tvoid opencog_nlp_lgparse_init(void) {}\n};\n",
        "description": "\tvoid opencog_nlp_lgparse_init(void) {}\n"
      },
      {
        "file": "components/language/lg-atomese/opencog/nlp/lg-dict/LGDictExpContainer.cc",
        "line": 239,
        "type": "FIXME",
        "content": "// XXX FIXME this does not smell right; optionals should get",
        "context": "\n    if (m_type == CONNECTOR_type)\n    {\n        // XXX FIXME this does not smell right; optionals should get\n        // blown up into pairs of disjuncts, one with and one without.\n        if (m_string == \"OPTIONAL\") return { optnl };\n\n",
        "description": "this does not smell right; optionals should get"
      },
      {
        "file": "components/language/lg-atomese/opencog/nlp/lg-dict/LGDictExpContainer.cc",
        "line": 268,
        "type": "FIXME",
        "content": "// XXX FIXME ... using an std::map would be more efficient.",
        "context": "    // remove repeated atoms from OR\n    if (m_type == OR_type)\n    {\n        // XXX FIXME ... using an std::map would be more efficient.\n        std::sort(outgoing.begin(), outgoing.end());\n        outgoing.erase(std::unique(outgoing.begin(),\n                                   outgoing.end()),\n",
        "description": "... using an std::map would be more efficient."
      },
      {
        "file": "components/language/lg-atomese/opencog/nlp/lg-dict/LGDictReader.cc",
        "line": 46,
        "type": "FIXME",
        "content": "// FIXME XXX -- Optionals are handled incorrectly here;",
        "context": "\n    std::vector<LGDictExpContainer> subcontainers;\n\n    // FIXME XXX -- Optionals are handled incorrectly here;\n    // they are denoted by a null Exp pointer in an OR_list!\n    // Ignoring all the nulls is just ... wrong.\n#if (LINK_MAJOR_VERSION == 5) &&  (LINK_MINOR_VERSION < 7)\n",
        "description": "XXX -- Optionals are handled incorrectly here;"
      },
      {
        "file": "components/language/lg-atomese/opencog/nlp/lg-dict/LGDictReader.cc",
        "line": 103,
        "type": "FIXME",
        "content": "// XXX FIXME -- if dn_head is null, then we should check regexes.",
        "context": "\n    HandleSeq outgoing;\n\n// XXX FIXME -- if dn_head is null, then we should check regexes.\n// Currently, LG does not do this automatically, but it almost surely\n// should. i.e. the LG public API needs to also handle regexes\n// automatically.\n",
        "description": "-- if dn_head is null, then we should check regexes."
      },
      {
        "file": "components/language/lg-atomese/opencog/nlp/lg-parse/LGParseLink.cc",
        "line": 229,
        "type": "FIXME",
        "content": "// XXX FIXME. This should be part of the LgDictNode but since",
        "context": "\t// Set up the dictionary config, if any.\n\t// This must happen before ldn->get_dictionary() because the\n\t// setup is stateful. This seems buggy, but is adequate for now.\n\t// XXX FIXME. This should be part of the LgDictNode but since\n\t// LgDictNode is a node, not a link, its hard to pass args.\n\t// We would need to wrap it with a StateLink, or maybe use the\n\t// new-fangled \"sensory API\". Sheesh.\n",
        "description": ". This should be part of the LgDictNode but since"
      },
      {
        "file": "components/language/lg-atomese/opencog/nlp/lg-parse/LGParseLink.cc",
        "line": 338,
        "type": "FIXME",
        "content": "// XXX FIXME -- We should fish parse options out of the atomspace.",
        "context": "\t\t\tparse_options_set_linkage_limit(opts, max_linkages);\n\t}\n\n\t// XXX FIXME -- We should fish parse options out of the atomspace.\n\t// Something like this, maybe:\n\t//     EvaluationLink\n\t//         PredicateNode \"LG ParseTime\"\n",
        "description": "-- We should fish parse options out of the atomspace."
      },
      {
        "file": "components/core/atomspace-rocks/opencog/persist/monospace/MonoIO.cc",
        "line": 920,
        "type": "TODO",
        "content": "// XXX TODO - maybe load links depth-order...",
        "context": "{\n\tCHECK_OPEN;\n\t// First, load all the nodes ... then the links.\n\t// XXX TODO - maybe load links depth-order...\n\tloadAtoms(table, \"n@\");\n\tloadAtoms(table, \"l@\");\n}\n",
        "description": "- maybe load links depth-order..."
      },
      {
        "file": "components/core/atomspace-rocks/opencog/persist/monospace/MonoIO.cc",
        "line": 944,
        "type": "FIXME",
        "content": "// XXX FIXME. We would like to call",
        "context": "\nvoid MonoStorage::storeAtomSpace(const AtomSpace* table)\n{\n\t// XXX FIXME. We would like to call\n\t// Options::PrepareForBulkLoad() here, but its too late, this\n\t// can only be set when opening the DB. Should we maybe close\n\t// and reopen the DB? This would be ... conducive of weird bugs.\n",
        "description": ". We would like to call"
      },
      {
        "file": "components/core/atomspace-rocks/opencog/persist/rocks/RocksDAG.cc",
        "line": 240,
        "type": "TODO",
        "content": "// XXX TODO: we should probably cache the results, instead of",
        "context": "void RocksStorage::makeOrder(Handle hasp,\n                             std::map<uint64_t, Handle>& order)\n{\n// XXX TODO: we should probably cache the results, instead of\n// recomputing every time!?\n\t// As long as there's a stack of Frames, just loop.\n\twhile (true)\n",
        "description": "we should probably cache the results, instead of"
      },
      {
        "file": "components/core/atomspace-rocks/opencog/persist/rocks/RocksIO.cc",
        "line": 1283,
        "type": "FIXME",
        "content": "// XXX FIXME. We would like to call",
        "context": "\t                        and nullptr != getAtomSpace())\n\t\tconvertForFrames(HandleCast(getAtomSpace()));\n\n\t// XXX FIXME. We would like to call\n\t// Options::PrepareForBulkLoad() here, but its too late, this\n\t// can only be set when opening the DB. Should we maybe close\n\t// and reopen the DB? This would be ... conducive of weird bugs.\n",
        "description": ". We would like to call"
      },
      {
        "file": "components/core/atomspace-rocks/opencog/persist/rocks/RocksPersistSCM.cc",
        "line": 82,
        "type": "FIXME",
        "content": "// XXX FIXME -- are open and close actually needed for anything?",
        "context": "    _storage = nullptr;\n}\n\n// XXX FIXME -- are open and close actually needed for anything?\nvoid RocksPersistSCM::do_open(const std::string& uri)\n{\n    if (_storage)\n",
        "description": "-- are open and close actually needed for anything?"
      },
      {
        "file": "components/core/atomspace-restful/lib/zmq/zhelpers.hpp",
        "line": 31,
        "type": "TODO",
        "content": "// todo: package updated zmq.hpp",
        "context": "\n#include <zmq.hpp>\n//#include <lib/zmq/zmq.hpp>\n// todo: package updated zmq.hpp\n\n#include <iostream>\n#include <iomanip>\n",
        "description": "package updated zmq.hpp"
      },
      {
        "file": "components/core/atomspace-restful/opencog/python/web/api/utilities.py",
        "line": 17,
        "type": "FIXME",
        "content": "# FIXME: Should this moved to the atomspace repo and be part",
        "context": "# https://github.com/opencog/opencog/pull/2012 and,\n# https://github.com/opencog/atomspace/pull/611\n# NOTE: This is similar to scheme `cog-node`.\n# FIXME: Should this moved to the atomspace repo and be part\n# of opencog.atomspace module?\ndef get_atoms_by_name(z_type, name, atomspace):\n    return filter(lambda x: x.name == name, atomspace.get_atoms_by_type(z_type))\n",
        "description": "Should this moved to the atomspace repo and be part"
      },
      {
        "file": "components/learning/moses/moses/moses/optimization/star-anneal.cc",
        "line": 42,
        "type": "TODO",
        "content": "// XXX TODO the annealing temperature control code should be ported over",
        "context": "// Star-shaped search  //\n/////////////////////////\n\n// XXX TODO the annealing temperature control code should be ported over\n// to the hill-climbing code, thus rendering the below obsolete.  The\n// hill-climbing code is much more sophisticated in every way: correct\n// definition of the temperature, termination conditions, exploration of\n",
        "description": "the annealing temperature control code should be ported over"
      },
      {
        "file": "components/learning/moses/moses/moses/optimization/particle-swarm.h",
        "line": 154,
        "type": "TODO",
        "content": "// TODO: pso description",
        "context": "// Particle Swarm //\n////////////////////\n\n// TODO: pso description\nstruct particle_swarm : optimizer_base\n{\n    particle_swarm(const optim_parameters& op = optim_parameters(),\n",
        "description": "pso description"
      },
      {
        "file": "components/learning/moses/moses/moses/optimization/particle-swarm.h",
        "line": 315,
        "type": "TODO",
        "content": "// TODO: Wind dispersion, but test without first",
        "context": "    void update_cont_particle(instance& temp, const instance& personal,\n            const instance& global, velocity::iterator vel, const field_set& fs);\n\n    // TODO: Wind dispersion, but test without first\n    // Make it later is easy.\n\npublic:\n",
        "description": "Wind dispersion, but test without first"
      },
      {
        "file": "components/learning/moses/moses/moses/optimization/hill-climbing.h",
        "line": 110,
        "type": "TODO",
        "content": "// XXX TODO make sure this value is appropriately updated.",
        "context": "\n    // Range of scores for which to keep instances.  This *should* be\n    // set to the value given by metapopulation::useful_score_range().\n    // XXX TODO make sure this value is appropriately updated.\n    //\n    // The range of scores is used to keep the size of the deme in check.\n    // The issue is that, for large feature sets, a large number of knobs\n",
        "description": "make sure this value is appropriately updated."
      },
      {
        "file": "components/learning/moses/moses/moses/optimization/particle-swarm.cc",
        "line": 197,
        "type": "TODO",
        "content": "// TODO: work in a better way to identify convergence.",
        "context": "            break;\n        }\n\n        // TODO: work in a better way to identify convergence.\n        not_improving = (has_improved) ? 0 : not_improving + 1;\n        if (not_improving > 3) {\n            logger().debug(\"Terminate Local Search: Convergence.\");\n",
        "description": "work in a better way to identify convergence."
      },
      {
        "file": "components/learning/moses/moses/moses/optimization/particle-swarm.cc",
        "line": 236,
        "type": "TODO",
        "content": "// TODO: Explanation",
        "context": "        \"complexity\";\n}\n\n// TODO: Explanation\n// There's no explanation for this, it's just a temporary solution.\n// Maybe use adaptative pso, something like LPSO (Lander).\nunsigned particle_swarm::calc_swarm_size(const field_set& fs) {\n",
        "description": "Explanation"
      },
      {
        "file": "components/learning/moses/moses/moses/metapopulation/metapopulation.h",
        "line": 535,
        "type": "TODO",
        "content": "// TODO: we may want to output the visited status as well",
        "context": "    // metapopulation. This function is used for fine logging to\n    // deeply probe the metapopulation.\n    //\n    // TODO: we may want to output the visited status as well\n    std::ostream& ostream_metapop(std::ostream&, int n = INT_MAX) const;\n\nprivate:\n",
        "description": "we may want to output the visited status as well"
      },
      {
        "file": "components/learning/moses/moses/moses/metapopulation/merging.cc",
        "line": 261,
        "type": "FIXME",
        "content": "// XXX FIXME: we should use a pointer set for scored_combo_tree_set",
        "context": "        logger().debug(\"Compute behavioral score of %d selected candidates\",\n                       candidates.size());\n\n        // XXX FIXME: we should use a pointer set for scored_combo_tree_set\n        // This would avoid some pointless copying here and a few other\n        // places.  This is easier said than done, because the stupid\n        // domination code is so snarky and icky.  Domination should die.\n",
        "description": "we should use a pointer set for scored_combo_tree_set"
      },
      {
        "file": "components/learning/moses/moses/moses/metapopulation/merging.cc",
        "line": 404,
        "type": "TODO",
        "content": "// TODO: Make population cap size-sensitive to exemplar complexity.",
        "context": "    // formula was arrived at via some ad-hoc experimentation.  A default\n    // value of _params.cap_coef=50 seems to work well.\n    //\n    // TODO: Make population cap size-sensitive to exemplar complexity.\n    // Large exemplars should result in smaller population sizes to maintain\n    // efficiency. Consider implementing adaptive sizing based on exemplar metrics.\n    //\n",
        "description": "Make population cap size-sensitive to exemplar complexity."
      },
      {
        "file": "components/learning/moses/moses/moses/metapopulation/merging.cc",
        "line": 552,
        "type": "FIXME",
        "content": "// XXX FIXME looks to me like it++ can often be collaed twice within this loop!",
        "context": "                    }\n                }\n\n// XXX FIXME looks to me like it++ can often be collaed twice within this loop!\n                prev_it = it++;\n            }\n\n",
        "description": "looks to me like it++ can often be collaed twice within this loop!"
      },
      {
        "file": "components/learning/moses/moses/moses/metapopulation/metapopulation.cc",
        "line": 222,
        "type": "FIXME",
        "content": "// XXX FIXME should probably not recompute every time ...",
        "context": "    if (not _params.do_boosting)\n        return _best_cscore;\n\n    // XXX FIXME should probably not recompute every time ...\n    // need to figure who is calling this method, and what they are expecting.\n    return _cscorer.get_cscore(_ensemble.get_ensemble());\n}\n",
        "description": "should probably not recompute every time ..."
      },
      {
        "file": "components/learning/moses/moses/moses/scoring/scoring_base.h",
        "line": 124,
        "type": "TODO",
        "content": "// XXX TODO should be a std::valarray not a vector.",
        "context": "\n    /// A vector of per-bscore weights, used to tote up the behavioral\n    /// score into a single number.\n    // XXX TODO should be a std::valarray not a vector.\n    virtual void update_weights(const std::vector<double>&);\n\n    /// Return the amount by which the bscore differs from a perfect\n",
        "description": "should be a std::valarray not a vector."
      },
      {
        "file": "components/learning/moses/moses/moses/scoring/time_dispersion.cc",
        "line": 43,
        "type": "TODO",
        "content": "// TODO multipler other than 1 is not supported yet",
        "context": "      _granularity(granularity), _multiplier(multiplier),\n      _pressure(time_dispersion_pressure), _exponent(time_dispersion_exponent)\n{\n    // TODO multipler other than 1 is not supported yet\n    OC_ASSERT(_multiplier == 1, \"Multiplier other than 1 is not supported yet\");\n\n    // Set of timestamp classes\n",
        "description": "multipler other than 1 is not supported yet"
      },
      {
        "file": "components/learning/moses/moses/moses/scoring/scoring_base.cc",
        "line": 108,
        "type": "FIXME",
        "content": "// XXX FIXME complexity_t should be a double not an int ...",
        "context": "        norm += w;\n    }\n\n    // XXX FIXME complexity_t should be a double not an int ...\n    return (complexity_t) floor (cpxy / norm + 0.5);\n}\n\n",
        "description": "complexity_t should be a double not an int ..."
      },
      {
        "file": "components/learning/moses/moses/moses/scoring/discriminating_bscore.cc",
        "line": 486,
        "type": "TODO",
        "content": "// XXX TODO -- should not return the penalties as part of the bscore,",
        "context": "                  float hardness)\n    : discriminating_bscore(ct, min_recall, max_recall, hardness)\n{\n    // XXX TODO -- should not return the penalties as part of the bscore,\n    // since this messes up boosting.\n    _size = ct.size() + 2;\n}\n",
        "description": "-- should not return the penalties as part of the bscore,"
      },
      {
        "file": "components/learning/moses/moses/moses/moses/local_moses.cc",
        "line": 180,
        "type": "TODO",
        "content": "// TODO use the option of the output",
        "context": "                   << \"\\t\" << ds.max;  // max distance\n\n                // diversity stats over all best n candidates of the metapopulation\n                // TODO use the option of the output\n                auto best_ds = mp.gather_diversity_stats(pa.max_cnd_output);\n                ss << \"\\t\" << best_ds.count // number of pairs of candidates\n                   << \"\\t\" << best_ds.mean  // average distance\n",
        "description": "use the option of the output"
      },
      {
        "file": "components/learning/moses/moses/moses/moses/partial.cc",
        "line": 96,
        "type": "TODO",
        "content": "// TODO: Improve generation tracking by getting actual number",
        "context": "\n        _moses_params.max_evals -= _num_evals;\n\n        // TODO: Improve generation tracking by getting actual number\n        // of generations run from MOSES and subtracting it here.\n        // Currently no easy API exists to retrieve this information.\n        _moses_params.max_gens -= _num_gens;\n",
        "description": "Improve generation tracking by getting actual number"
      },
      {
        "file": "components/learning/moses/moses/moses/moses/mpi_moses.cc",
        "line": 201,
        "type": "TODO",
        "content": "// XXX TODO -- trim the deme down, before sending, by using the worst acceptable score.",
        "context": "/// send_deme -- send the completed deme from the worker back to root\n///\n/// This sends a pretty big glob.\n// XXX TODO -- trim the deme down, before sending, by using the worst acceptable score.\nvoid moses_mpi_comm::send_deme(const metapopulation& mp, int n_evals)\n{\n    MPI::COMM_WORLD.Send(&n_evals, 1, MPI::INT, ROOT_NODE, MSG_NUM_EVALS);\n",
        "description": "-- trim the deme down, before sending, by using the worst acceptable score."
      },
      {
        "file": "components/learning/moses/moses/moses/moses/types.h",
        "line": 210,
        "type": "TODO",
        "content": "// TODO this should be a std::valarray not std::vector but I am too",
        "context": "/// in reference to a particular table of data.  Exactly which tree it\n/// is, and which table, is implicit.\n//\n// TODO this should be a std::valarray not std::vector but I am too\n// lazy to make the switch right now.\nstruct behavioral_score : public std::vector<score_t>\n{\n",
        "description": "this should be a std::valarray not std::vector but I am too"
      },
      {
        "file": "components/learning/moses/moses/moses/deme/deme_expander.cc",
        "line": 441,
        "type": "TODO",
        "content": "// TODO: DO NOT CHANGE THE MAX SCORE IF USER SET IT: BUT THAT",
        "context": "                // dynamically selected, it might be less that the global target;\n                // that is, the deme might not be able to reach the best score.)\n                //\n                // TODO: DO NOT CHANGE THE MAX SCORE IF USER SET IT: BUT THAT\n                // OPTION ISN'T GLOBAL WHAT TO DO?\n                //\n                // But why would we want to over-ride the best-possible score?\n",
        "description": "DO NOT CHANGE THE MAX SCORE IF USER SET IT: BUT THAT"
      },
      {
        "file": "components/learning/moses/moses/moses/deme/deme_expander.cc",
        "line": 457,
        "type": "TODO",
        "content": "// TODO: re-enable that once best_possible_bscore is fixed",
        "context": "                              \"terminate deme search. Except I think this \"\n                              \"is fixed now. It needs review and testing.\");\n\n                // TODO: re-enable that once best_possible_bscore is fixed\n                // I think its now fixed, but I'm not sure.  It needs to be\n                // reviewed and tested.\n#if THIS_IS_DISABLED_UNTIL_ABOVE_IS_FIXED\n",
        "description": "re-enable that once best_possible_bscore is fixed"
      },
      {
        "file": "components/learning/moses/moses/moses/eda/replacement.h",
        "line": 62,
        "type": "TODO",
        "content": "// TODO: I think it might be a little more efficent to use the",
        "context": "// Replace the most similar individual, where similarity is determined by\n// the hamming distance.\n//\n// TODO: I think it might be a little more efficent to use the\n// hamming_distance as a sort comparison operator, and hand off the whole\n// thing to std:nth_element, and let that class figure out who is close or\n// not.  This avoids the use of doubly-nested loops, and multiple redundant\n",
        "description": "I think it might be a little more efficent to use the"
      },
      {
        "file": "components/learning/moses/moses/moses/representation/representation.cc",
        "line": 51,
        "type": "TODO",
        "content": "// XXX TODO: One might think that varying the stepsize, i.e. shrinking",
        "context": "// Stepsize should be roughly the standard-deviation of the expected\n// distribution of the contin variables.\n//\n// XXX TODO: One might think that varying the stepsize, i.e. shrinking\n// it, as the optimizers tune into a specific value, would be a good\n// thing (so that the optimizer could tune to a more precise value).\n// Unfortunately, a simple experiment in tuning (see below, surrounded\n",
        "description": "One might think that varying the stepsize, i.e. shrinking"
      },
      {
        "file": "components/learning/moses/moses/moses/representation/representation.cc",
        "line": 236,
        "type": "TODO",
        "content": "// XXX TODO need to add support for \"term algebra\" knobs",
        "context": "/// the instance supplied as the argument.\nvoid representation::transform(const instance& inst)\n{\n    // XXX TODO need to add support for \"term algebra\" knobs\n\n    contin_map_it ckb = contin.begin();\n    for (field_set::const_contin_iterator ci = _fields.begin_contin(inst);\n",
        "description": "need to add support for \"term algebra\" knobs"
      },
      {
        "file": "components/learning/moses/moses/moses/representation/build_knobs.cc",
        "line": 316,
        "type": "TODO",
        "content": "/// TODO: measure and compare the resulting performance.",
        "context": "/// can be rather incredibly costly, especially when the exemplars start\n/// getting large.  So the real question is: is the performance cost of\n/// this routine worth the eventual savings when scoring instances?\n/// TODO: measure and compare the resulting performance.\n//\n// Notes to self: hmm. in 5-parity problem, about 2/3 of knobs are\n// disallowed! viz of 6738 probes, 4007 knobs are completely disallowed.\n",
        "description": "measure and compare the resulting performance."
      },
      {
        "file": "components/learning/moses/moses/moses/representation/build_knobs.cc",
        "line": 489,
        "type": "TODO",
        "content": "// TODO: should bias the selection of these, so that",
        "context": "        }\n    }\n\n    // TODO: should bias the selection of these, so that\n    // larger subtrees are preferred .. !? why?\n\n    unsigned max_pairs = permitted_perms.size();\n",
        "description": "should bias the selection of these, so that"
      },
      {
        "file": "components/learning/moses/moses/moses/representation/build_knobs.cc",
        "line": 581,
        "type": "TODO",
        "content": "// TODO: Benchmark and clarify optimal breakeven point across different problem sizes.",
        "context": "    // The number of 30K is a wild guesstimate, based on recent\n    // measurements of relatively simple exemplars; its maybe even\n    // too low.  For large exemplars, it might be too big !?\n    // TODO: Benchmark and clarify optimal breakeven point across different problem sizes.\n#define BREAKEVEN 30000\n    size_t np = perms.size();\n    int nthr = 1 + np / BREAKEVEN;\n",
        "description": "Benchmark and clarify optimal breakeven point across different problem sizes."
      },
      {
        "file": "components/learning/moses/moses/moses/representation/build_knobs.cc",
        "line": 690,
        "type": "TODO",
        "content": "// XXX TODO: Is this really optimal?  The below adds an entire copy",
        "context": "        }\n    }\n\n    // XXX TODO: Is this really optimal?  The below adds an entire copy\n    // of the tree at it, which clearly increases the overall complexity.\n    // But is this really a wise thig to do? It seems gratuitous, and it's\n    // not obvious that knobs from this flipped tree will yeild benefits,\n",
        "description": "Is this really optimal?  The below adds an entire copy"
      },
      {
        "file": "components/learning/moses/moses/moses/representation/build_knobs.cc",
        "line": 1181,
        "type": "TODO",
        "content": "//TODO: should bias the selection of these (and possibly choose larger subtrees)",
        "context": "        perms.push_back(tr);\n\n    //and n random pairs out of the total  2 * choose(n,2) = n * (n - 1) of these\n    //TODO: should bias the selection of these (and possibly choose larger subtrees)\n    lazy_random_selector randpair(n * (n - 1));\n\n    dorepeat(n) {\n",
        "description": "should bias the selection of these (and possibly choose larger subtrees)"
      },
      {
        "file": "components/learning/moses/moses/moses/representation/build_knobs.cc",
        "line": 1342,
        "type": "FIXME",
        "content": "//FIXME: now just attaches to the first output",
        "context": "    cout << \"Created node: \" << new_node << endl;\n\n    //now attach the subtree to the hidden nodes\n    //FIXME: now just attaches to the first output\n    sib_it first_hidden = it.begin();\n\n    _exemplar.insert_subtree(first_hidden.begin(),new_node.begin());\n",
        "description": "now just attaches to the first output"
      },
      {
        "file": "components/learning/moses/moses/moses/main/problem-params.h",
        "line": 46,
        "type": "FIXME",
        "content": "// XXX FIXME TODO The structure below should be split into multiple",
        "context": "\nnamespace opencog { namespace moses {\n\n// XXX FIXME TODO The structure below should be split into multiple\n// parts, with each sub-part responsible for picking out the argv's\n// that it cares about. Unfortunately, this requires getting rid of\n// boost::program_options (because boost::program_options does not\n",
        "description": "TODO The structure below should be split into multiple"
      },
      {
        "file": "components/learning/moses/moses/moses/main/table-problems.cc",
        "line": 138,
        "type": "FIXME",
        "content": "// XXX FIXME -- the multiple tables should be merged into one.",
        "context": "    }\n    logger().info(\"Number of rows in tables = %d\", num_rows);\n\n    // XXX FIXME -- the multiple tables should be merged into one.\n    ctable = _ctables.front();\n    table = _tables.front();\n\n",
        "description": "-- the multiple tables should be merged into one."
      },
      {
        "file": "components/learning/moses/moses/moses/main/table-problems.cc",
        "line": 150,
        "type": "FIXME",
        "content": "// XXX FIXME .. check that they all have the same signature.",
        "context": "    arity = table.get_arity();\n\n    // Check that all input data files have the same arity\n    // XXX FIXME .. check that they all have the same signature.\n    if (_tables.size() > 1) {\n        for (size_t i = 1; i < _tables.size(); ++i) {\n            combo::arity_t test_arity = _tables[i].get_arity();\n",
        "description": ".. check that they all have the same signature."
      },
      {
        "file": "components/learning/moses/moses/moses/main/problem-params.cc",
        "line": 166,
        "type": "TODO",
        "content": "// XXX TODO: make this print correctly, instead of using brackets.",
        "context": "    using namespace std;\n\n    // Declare the supported options.\n    // XXX TODO: make this print correctly, instead of using brackets.\n    desc.add_options()\n\n        // General options\n",
        "description": "make this print correctly, instead of using brackets."
      },
      {
        "file": "components/learning/moses/moses/comboreduct/type_checker/type_tree.cc",
        "line": 599,
        "type": "TODO",
        "content": "// XXX TODO the code below was modified to allow arg lists of",
        "context": "            // then check that a1 inherits from T1, and that a2, a3\n            // and a4 inherit from T2.  T3 is the output type.\n\n            // XXX TODO the code below was modified to allow arg lists of\n            // mixed type, e.g. so that the cond primitive could be\n            // supported (as the current definition of cond alternates\n            // between boolean-valued predicates, and the result type).\n",
        "description": "the code below was modified to allow arg lists of"
      },
      {
        "file": "components/learning/moses/moses/comboreduct/type_checker/type_tree.h",
        "line": 235,
        "type": "TODO",
        "content": "// TODO : lambda",
        "context": "//\n// intersection of ill_formed and T is ill_formed\n//\n// TODO : lambda\n//\n// Of course the case if T1 inherit T2 then interection of T1 and T2\n// is T1 is also implemented. If the interection is ill_formed or\n",
        "description": "lambda"
      },
      {
        "file": "components/learning/moses/moses/comboreduct/interpreter/interpreter.cc",
        "line": 336,
        "type": "TODO",
        "content": "// XXX TODO: contin_if should go away.",
        "context": "            return (i == id::logical_true ? 1.0 : 0.0);\n        }\n\n        // XXX TODO: contin_if should go away.\n        case id::contin_if :\n        case id::cond : {\n            sib_it sib = it.begin();\n",
        "description": "contin_if should go away."
      },
      {
        "file": "components/learning/moses/moses/comboreduct/interpreter/eval.cc",
        "line": 530,
        "type": "TODO",
        "content": "// XXX TODO: contin_if should go away.",
        "context": "            return eval_throws_tree(bmap, exp_tr);\n        }\n\n        // XXX TODO: contin_if should go away.\n        case id::contin_if :\n        case id::cond : {\n            sib_it sib = it.begin();\n",
        "description": "contin_if should go away."
      },
      {
        "file": "components/learning/moses/moses/comboreduct/combo/descriptions.cc",
        "line": 45,
        "type": "TODO",
        "content": "// ToDo: would be nice to have a more Caml/Haskell style syntax here,",
        "context": "// with builtins as indicies, within the singleton class builtin_properties.\n// This array should not have any other usages.\n//\n// ToDo: would be nice to have a more Caml/Haskell style syntax here,\n// right?\nstatic const builtin_description bd[] =\n{\n",
        "description": "would be nice to have a more Caml/Haskell style syntax here,"
      },
      {
        "file": "components/learning/moses/moses/comboreduct/table/table.h",
        "line": 1352,
        "type": "TODO",
        "content": "// XXX TODO, it would be easier if KLD took a sorted list",
        "context": "            }\n        }\n\n        // XXX TODO, it would be easier if KLD took a sorted list\n        // as the argument.\n        std::vector<contin_t> p, q;\n        for (auto pr : sorted_list) {\n",
        "description": ", it would be easier if KLD took a sorted list"
      },
      {
        "file": "components/learning/moses/moses/comboreduct/table/table.h",
        "line": 1366,
        "type": "TODO",
        "content": "// XXX TODO remove this print, for better performance.",
        "context": "        // Also a problem, this is returning values greater than 1.0;\n        // I thought that IC was supposed to max out at 1.0 !?\n        contin_t ic = - KLD(p,q);\n        // XXX TODO remove this print, for better performance.\n        unsigned idx = *(fs.begin());\n        logger().debug() <<\"Contin MI for feat=\" << idx << \" ic=\" << ic;\n        return ic;\n",
        "description": "remove this print, for better performance."
      },
      {
        "file": "components/learning/moses/moses/comboreduct/table/table.cc",
        "line": 409,
        "type": "TODO",
        "content": "// XXX TODO replace this by the util p_norm function.",
        "context": "    return rhs.get_label() == label;\n}\n\n// XXX TODO replace this by the util p_norm function.\ncontin_t OTable::abs_distance(const OTable& ot) const\n{\n    OC_ASSERT(ot.size() == size());\n",
        "description": "replace this by the util p_norm function."
      },
      {
        "file": "components/learning/moses/moses/comboreduct/table/table.cc",
        "line": 434,
        "type": "TODO",
        "content": "// XXX TODO replace this by the util p_norm function.",
        "context": "    return res;\n}\n\n// XXX TODO replace this by the util p_norm function.\ncontin_t OTable::sum_squared_error(const OTable& ot) const\n{\n    OC_ASSERT(ot.size() == size());\n",
        "description": "replace this by the util p_norm function."
      },
      {
        "file": "components/learning/moses/moses/comboreduct/table/table.cc",
        "line": 848,
        "type": "TODO",
        "content": "// XXX TODO replace this by the util p_norm function.",
        "context": "\n// -------------------------------------------------------\n\n// XXX TODO replace this by the util p_norm function.\ncomplete_truth_table::size_type\ncomplete_truth_table::hamming_distance(const complete_truth_table& other) const\n{\n",
        "description": "replace this by the util p_norm function."
      },
      {
        "file": "components/learning/moses/moses/comboreduct/reduct/logical_rules.cc",
        "line": 100,
        "type": "TODO",
        "content": "// XXX TODO: I don't understand why this is not damaging contin_if  !??",
        "context": "    // Most nodes take simple lists; but not cond. Cond takes clauses,\n    // which are pairs. If we remove the condition, we must also remove\n    // the consequent.\n// XXX TODO: I don't understand why this is not damaging contin_if  !??\n// But .. umm, maybe build_knobs is not creating any kinds of contin_if's\n// that can be damaged... well, no matter, because thes if's will be\n// replaced by cond... \n",
        "description": "I don't understand why this is not damaging contin_if  !??"
      },
      {
        "file": "components/learning/moses/moses/comboreduct/reduct/logical_rules.cc",
        "line": 667,
        "type": "stub",
        "content": "// stub out, for performance.",
        "context": "                              make_counting_iterator(current.end()));\n\n#if DEBUG\n        // stub out, for performance.\n        OC_ASSERT(std::is_sorted(dominant.begin(),dominant.end(), comp),\n                  \"dominant subtree_set should be sorted (reduce_and)\");\n#endif\n",
        "description": "// stub out, for performance."
      },
      {
        "file": "components/learning/moses/moses/comboreduct/reduct/logical_rules.cc",
        "line": 697,
        "type": "stub",
        "content": "// stubbed out for performance",
        "context": "            tr.validate();\n\n#if DEBUG\n            // stubbed out for performance\n            OC_ASSERT(std::is_sorted(command.begin(),command.end(),comp),\n                      \"command subtree_set should be sorted (reduce_and)\");\n            OC_ASSERT(std::is_sorted(handle_set.begin(),handle_set.end(),comp),\n",
        "description": "// stubbed out for performance"
      },
      {
        "file": "components/learning/moses/moses/comboreduct/reduct/mixed_rules.cc",
        "line": 1228,
        "type": "TODO",
        "content": "//check if 0<-(y+pi) -> false //TODO",
        "context": "                }\n            }\n            else if(*copy_tr.begin()==id::logical_false) {\n                //check if 0<-(y+pi) -> false //TODO\n                combo_tree copy2_tr = tr.subtree(sib_it(it), tr.next_sibling(sib_it(it)));\n                //copy old assumptions, begin\n                sib_it bna = copy2_tr.begin(); //before new assumption\n",
        "description": ""
      },
      {
        "file": "components/learning/moses/moses/comboreduct/reduct/contin_rules.cc",
        "line": 964,
        "type": "TODO",
        "content": "// TODO:  sin(*(-1 x)) -> -sin(x)",
        "context": "// or more generally\n// sin(sum x_i + sum c_j) -> sin(sum x_i + ((sum c_j)+pi)%2pi -pi\n//\n// TODO:  sin(*(-1 x)) -> -sin(x)\n// The above is frequently seen in real-life ...\nvoid reduce_sin::operator()(combo_tree& tr, combo_tree::iterator it) const\n{\n",
        "description": "sin(*(-1 x)) -> -sin(x)"
      },
      {
        "file": "components/learning/moses/moses/comboreduct/main/action-reductor.cc",
        "line": 94,
        "type": "TODO",
        "content": "// TODO -- replace this by cond",
        "context": "    cout << \"output type \" << ba2->get_output_type_tree() << endl;\n\n#if 0\n    // TODO -- replace this by cond\n    cout << \"6----------------\" << endl;\n\n    cout << \"arity \" << (int)get_arity(id::boolean_if) << endl;\n",
        "description": "-- replace this by cond"
      },
      {
        "file": "components/learning/moses/moses/comboreduct/main/eval-table.cc",
        "line": 147,
        "type": "FIXME",
        "content": "// XXX FIXME",
        "context": "    }\n\n    // HERE WE ARE ASSUMING THAT THE INPUT FILE HAS A HEADER!!!\n// XXX FIXME\n    vector<string> header = get_header(pa.input_table_file);\n\n    // Add to ignore_values (header - all_unique_variables - target feature)\n",
        "description": ""
      },
      {
        "file": "opencog/opencog/main/LGParser.cc",
        "line": 128,
        "type": "stub",
        "content": "lg_dictionary_ = new int(1); // Stub: minimal allocation for compatibility",
        "context": "#else\n        // Fallback stub implementation when Link Grammar library is not available\n        // These stubs maintain API compatibility while indicating library absence\n        lg_dictionary_ = new int(1); // Stub: minimal allocation for compatibility\n        lg_options_ = new int(2);    // Stub: minimal allocation for compatibility\n        \n        logger().warn(\"Link Grammar library not available. Using fallback parser with limited functionality.\");\n",
        "description": "// Stub: minimal allocation for compatibility"
      },
      {
        "file": "opencog/opencog/main/LGParser.cc",
        "line": 129,
        "type": "stub",
        "content": "lg_options_ = new int(2);    // Stub: minimal allocation for compatibility",
        "context": "        // Fallback stub implementation when Link Grammar library is not available\n        // These stubs maintain API compatibility while indicating library absence\n        lg_dictionary_ = new int(1); // Stub: minimal allocation for compatibility\n        lg_options_ = new int(2);    // Stub: minimal allocation for compatibility\n        \n        logger().warn(\"Link Grammar library not available. Using fallback parser with limited functionality.\");\n        \n",
        "description": "// Stub: minimal allocation for compatibility"
      },
      {
        "file": "opencog/opencog/main/LGParser.h",
        "line": 136,
        "type": "stub",
        "content": "// Stub types when Link Grammar is not available",
        "context": "    Dictionary lg_dictionary_;\n    Parse_Options lg_options_;\n#else\n    // Stub types when Link Grammar is not available\n    // Using void* to maintain API compatibility while indicating unavailability\n    void* lg_dictionary_;  // Stub: would be Dictionary if HAVE_LINK_GRAMMAR\n    void* lg_options_;     // Stub: would be Parse_Options if HAVE_LINK_GRAMMAR\n",
        "description": "// Stub types when Link Grammar is not available"
      },
      {
        "file": "opencog/opencog/main/LGParser.h",
        "line": 138,
        "type": "stub",
        "content": "void* lg_dictionary_;  // Stub: would be Dictionary if HAVE_LINK_GRAMMAR",
        "context": "#else\n    // Stub types when Link Grammar is not available\n    // Using void* to maintain API compatibility while indicating unavailability\n    void* lg_dictionary_;  // Stub: would be Dictionary if HAVE_LINK_GRAMMAR\n    void* lg_options_;     // Stub: would be Parse_Options if HAVE_LINK_GRAMMAR\n#endif\n    \n",
        "description": "// Stub: would be Dictionary if HAVE_LINK_GRAMMAR"
      },
      {
        "file": "opencog/opencog/main/LGParser.h",
        "line": 139,
        "type": "stub",
        "content": "void* lg_options_;     // Stub: would be Parse_Options if HAVE_LINK_GRAMMAR",
        "context": "    // Stub types when Link Grammar is not available\n    // Using void* to maintain API compatibility while indicating unavailability\n    void* lg_dictionary_;  // Stub: would be Dictionary if HAVE_LINK_GRAMMAR\n    void* lg_options_;     // Stub: would be Parse_Options if HAVE_LINK_GRAMMAR\n#endif\n    \n    /**\n",
        "description": "// Stub: would be Parse_Options if HAVE_LINK_GRAMMAR"
      },
      {
        "file": "atomspace/opencog/guile/SchemeSmobAtom.cc",
        "line": 84,
        "type": "FIXME",
        "content": "// XXX FIXME. Work around the despicable, horrible guile UTF8 handling.",
        "context": "\n/* ============================================================== */\n\n// XXX FIXME. Work around the despicable, horrible guile UTF8 handling.\n// I am flabbergasted. The guile people are smart, but they could not have\n// possibly picked a crappier string handling design. Fuck me. See\n// https://stackoverflow.com/questions/79329532/c-c-encode-binary-into-utf8\n",
        "description": ". Work around the despicable, horrible guile UTF8 handling."
      },
      {
        "file": "atomspace/opencog/guile/modules/ExecSCM.cc",
        "line": 73,
        "type": "FIXME",
        "content": "// XXX FIXME: can we fix cython to not do this, already?",
        "context": "// NOTE HACK ALERT This needs to be static, in order for python to\n// work correctly.  The problem is that python keeps creating and\n// destroying this class, but it expects things to stick around.\n// XXX FIXME: can we fix cython to not do this, already?\n// Oh well. I guess that's OK, since the definition is meant to be\n// for the lifetime of the process, anyway.\nstd::vector<FunctionWrap*>* ExecSCM::_binders = nullptr;\n",
        "description": "can we fix cython to not do this, already?"
      },
      {
        "file": "atomspace/opencog/query/InitiateSearchMixin.cc",
        "line": 126,
        "type": "FIXME",
        "content": "// XXX FIXME; we should be using ptm->isVariable() instead !?",
        "context": "\tType t = h->get_type();\n\tif (_nameserver.isNode(t))\n\t{\n\t\t// XXX FIXME; we should be using ptm->isVariable() instead !?\n\t\tif (VARIABLE_NODE != t and GLOB_NODE != t and SIGN_NODE != t)\n\t\t{\n\t\t\twidth = h->getIncomingSetSize();\n",
        "description": "; we should be using ptm->isVariable() instead !?"
      },
      {
        "file": "atomspace/opencog/query/InitiateSearchMixin.cc",
        "line": 382,
        "type": "TODO",
        "content": "// TODO -- weed out duplicates!",
        "context": "\t}\n\telse\n\t{\n\t\t// TODO -- weed out duplicates!\n\t}\n\treturn true;\n}\n",
        "description": "-- weed out duplicates!"
      },
      {
        "file": "atomspace/opencog/query/NextSearchMixin.cc",
        "line": 167,
        "type": "TODO",
        "content": "// XXX TODO ... Rather than counting the number of variables, we",
        "context": "// can be done in a direct fashion; it resembles the concept of\n// \"unit propagation\" in the DPLL algorithm.\n//\n// XXX TODO ... Rather than counting the number of variables, we\n// should instead look for one with the smallest incoming set.\n// That is because the very next thing that we do will be to\n// iterate over the incoming set of \"pursue\" ... so it could be\n",
        "description": "... Rather than counting the number of variables, we"
      },
      {
        "file": "atomspace/opencog/query/SatisfyMixin.cc",
        "line": 583,
        "type": "FIXME",
        "content": "// XXX FIXME terrible hack.",
        "context": "\t\t// pure absent is found.\n\t\tif (is_pure_absent)\n\t\t{\n\t\t\t// XXX FIXME terrible hack.\n\t\t\tTermMatchMixin* intu =\n\t\t\t\tdynamic_cast<TermMatchMixin*>(this);\n\t\t\tif (intu->optionals_present()) return false;\n",
        "description": "terrible hack."
      },
      {
        "file": "atomspace/opencog/query/TermMatchMixin.cc",
        "line": 551,
        "type": "TODO",
        "content": "// XXX TODO as discussed on the mailing list, we should perhaps first",
        "context": "\t//       Arg1Atom\n\t//       Arg2Atom\n\t//\n\t// XXX TODO as discussed on the mailing list, we should perhaps first\n\t// see if the following can be found in the atomspace:\n\t//\n\t//   EvaluationLink\n",
        "description": "as discussed on the mailing list, we should perhaps first"
      },
      {
        "file": "atomspace/opencog/query/TermMatchMixin.cc",
        "line": 710,
        "type": "FIXME",
        "content": "// XXX FIXME: worse: this cannot possibly be right when",
        "context": "\t\t// possibilities?  And if they failed to do so, can we even do\n\t\t// anything about that here? Seems like we can't do anything...\n\t\t//\n\t\t// XXX FIXME: worse: this cannot possibly be right when\n\t\t// the ChoiceLink contains presentLinks.\n\t\tfor (const Handle& h : oset)\n\t\t{\n",
        "description": "worse: this cannot possibly be right when"
      },
      {
        "file": "atomspace/opencog/query/Recognizer.cc",
        "line": 126,
        "type": "TODO",
        "content": "// TODO: Change to something better if possible...",
        "context": "\t// mis-matched types are a dead-end.\n\tif (lpat->get_type() != lsoln->get_type()) return false;\n\n\t// TODO: Change to something better if possible...\n\t// What is happening here is to manually call the\n\t// fuzzy_match callback immediately if and only if\n\t// lsoln has one or more GlobNodes AND lpat and lsoln\n",
        "description": "Change to something better if possible..."
      },
      {
        "file": "atomspace/opencog/query/PatternMatchEngine.cc",
        "line": 284,
        "type": "FIXME",
        "content": "/// XXX FIXME: this is currently a weak stop-gap measure to handle",
        "context": "/// Compare the contents of a Present term in the pattern to the\n/// proposed grounding. The term `ptm` points at the Present term.\n///\n/// XXX FIXME: this is currently a weak stop-gap measure to handle\n/// the special case of Present terms embedded in Choice terms.\n/// Present terms that are NOT in a Choice are handled by the\n/// do_next_clause() system, which assumes that Present terms happen\n",
        "description": "this is currently a weak stop-gap measure to handle"
      },
      {
        "file": "atomspace/opencog/query/PatternMatchEngine.cc",
        "line": 1532,
        "type": "FIXME",
        "content": "// XXX FIXME - this is not very elegant. We should probably",
        "context": "\t// If this is some other rando variable that is not part of\n\t// search pattern, i.e. if is is a scoped variable, then\n\t// accept a match to any other alpha-equivalent variable.\n\t// XXX FIXME - this is not very elegant. We should probably\n\t// have a distinct `scoped_link_compare()` function to handle\n\t// this. Right now, the scope_match() callback uses a rather\n\t// screwy and indirect trick to check alpha conversion.\n",
        "description": "- this is not very elegant. We should probably"
      },
      {
        "file": "atomspace/opencog/query/PatternMatchEngine.cc",
        "line": 1982,
        "type": "FIXME",
        "content": "// XXX FIXME: Issue #3016 - Unification with unordered AndLinks",
        "context": "/// form; see `explore_sparse_branches()`.\n///\n//\n// XXX FIXME: Issue #3016 - Unification with unordered AndLinks\n// The current implementation of unordered link permutation exploration\n// in IdenticalLinks stops after finding the first valid permutation \n// instead of continuing to find all possible permutations. This is \n",
        "description": "Issue #3016 - Unification with unordered AndLinks"
      },
      {
        "file": "atomspace/opencog/query/PatternMatchEngine.cc",
        "line": 2049,
        "type": "FIXME",
        "content": "/// XXX FIXME: Right now, this code handles graphs that have only one",
        "context": "/// functional group, and the glob will end up holding the moiety that\n/// is not a part of the functional group.\n///\n/// XXX FIXME: Right now, this code handles graphs that have only one\n/// single sparse search.   Nested sparse searches are not supported;\n/// to implement those, its \"easy\": implement the same flow control as\n/// the unordered_explore steppers. I'm lzay, today, so I am not doing\n",
        "description": "Right now, this code handles graphs that have only one"
      },
      {
        "file": "atomspace/opencog/query/PatternMatchEngine.cc",
        "line": 2065,
        "type": "TODO",
        "content": "// XXX TODO FIXME. The ptm needs to be decomposed into connected",
        "context": "{\n\tlogmsg(\"Explore sparse: Start exploration\");\n\n\t// XXX TODO FIXME. The ptm needs to be decomposed into connected\n\t// components. Then only the connected components need to be walked\n\t// over.  That would be much more efficient.\n\tdo\n",
        "description": "FIXME. The ptm needs to be decomposed into connected"
      },
      {
        "file": "atomspace/opencog/query/PatternMatchEngine.cc",
        "line": 2223,
        "type": "FIXME",
        "content": "/// XXX FIXME -- do the above.",
        "context": "/// -- build a connectivity map, just like the one for clauses\n/// -- build a clause_variables struct, but just for this term\n/// -- search for the thinnest joint, just like `get_next_clause`\n/// XXX FIXME -- do the above.\n///\nbool PatternMatchEngine::next_untried_present(const PatternTermPtr& parent,\n                                              const PatternTermPtr& clause,\n",
        "description": "-- do the above."
      },
      {
        "file": "atomspace/opencog/query/PatternMatchEngine.cc",
        "line": 2445,
        "type": "TODO",
        "content": "// XXX TODO make sure that all variables in the clause have",
        "context": "\t\t                 << \"Parent has evaluatable but code path was expected to be unreachable. \"\n\t\t                 << \"Clause: \" << clause->getQuote()->to_string();\n\t\t// Continue with the evaluation anyway, but log the issue\n\t\t// XXX TODO make sure that all variables in the clause have\n\t\t// been grounded!  If they're not, something is badly wrong!\n\t\tlogmsg(\"Term inside evaluatable, move up to it's top:\",\n\t\t\t       clause->getQuote());\n",
        "description": "make sure that all variables in the clause have"
      },
      {
        "file": "atomspace/opencog/cython/PythonEval.cc",
        "line": 1427,
        "type": "FIXME",
        "content": "// XXX FIXME this does a lot of wasteful string copying.",
        "context": "\nvoid PythonEval::eval_expr(const std::string& partial_expr)\n{\n    // XXX FIXME this does a lot of wasteful string copying.\n    std::string expr = partial_expr;\n    size_t nl = expr.find_first_of(\"\\n\\r\");\n    while (std::string::npos != nl)\n",
        "description": "this does a lot of wasteful string copying."
      },
      {
        "file": "atomspace/opencog/atomspace/AtomSpace.cc",
        "line": 139,
        "type": "TODO",
        "content": "// TODO: this should probably be moved to a method on class Atom.",
        "context": "        }\n\n        // Check the values...\n        // TODO: this should probably be moved to a method on class Atom.\n        if (check_values)\n        {\n            HandleSet keys_first = atom_first->getKeys();\n",
        "description": "this should probably be moved to a method on class Atom."
      },
      {
        "file": "atomspace/opencog/atomspace/AtomSpace.cc",
        "line": 273,
        "type": "FIXME",
        "content": "// Fixme maybe later someday, if/when this is needed.",
        "context": "\t// having one AtomSpace be placed as a member into many others,\n\t// except that we don't have any viable mechanisms for such multiple\n\t// membership, and so I don't know how to treat this right now.\n\t// Fixme maybe later someday, if/when this is needed.\n\tif (not (nullptr == _atom_space or as == nullptr))\n\t\tthrow RuntimeException(TRACE_INFO,\n\t\t\t\"At this time, an AtomSpace can only be placed in one other\\n\"\n",
        "description": "maybe later someday, if/when this is needed."
      },
      {
        "file": "atomspace/opencog/atomspace/AtomSpace.h",
        "line": 524,
        "type": "FIXME",
        "content": "// XXX FIXME Users should call StorageNode::add_nocheck() instead.",
        "context": "\n    /* ----------------------------------------------------------- */\n    // Not for public use! Only StorageNodes get to call this!\n    // XXX FIXME Users should call StorageNode::add_nocheck() instead.\n    Handle storage_add_nocheck(const Handle& h) { return add(h); }\n};\n\n",
        "description": "Users should call StorageNode::add_nocheck() instead."
      },
      {
        "file": "atomspace/opencog/atoms/flow/FormulaPredicateLink.cc",
        "line": 41,
        "type": "FIXME",
        "content": "/// XXX FIXME - in the future, some user is going to want to include",
        "context": "/// not typed, and there are *two* bodies, each body returning one\n/// component of the final truth value...\n///\n/// XXX FIXME - in the future, some user is going to want to include\n/// variable declarations, and/or an explicit Lambda in the body, for\n/// some reason that I cannot imagine.  The code below will then fail.\n/// For now, ignore this possibility.\n",
        "description": "- in the future, some user is going to want to include"
      },
      {
        "file": "atomspace/opencog/atoms/flow/FilterLink.cc",
        "line": 615,
        "type": "TODO",
        "content": "// XXX TODO FIXME -- if vex is a stream, e.g. a QueueValue,",
        "context": "\t{\n\t\tvex = _outgoing[1]->execute(as, silent);\n\n\t\t// XXX TODO FIXME -- if vex is a stream, e.g. a QueueValue,\n\t\t// then we should construct another Queue as the return value,\n\t\t// and perform filtering on-demand.\n\t\tif (vex->is_type(LINK_VALUE))\n",
        "description": "FIXME -- if vex is a stream, e.g. a QueueValue,"
      },
      {
        "file": "atomspace/opencog/atoms/flow/CollectionOfLink.cc",
        "line": 70,
        "type": "TODO",
        "content": "// TODO: Handle executable _outgoing[0] by executing it first.",
        "context": "\n\t_have_typespec = true;\n\n\t// TODO: Handle executable _outgoing[0] by executing it first.\n\t// TODO: Support complex type signatures beyond simple TYPE_NODE.\n\t// Current implementation assumes simple type specification.\n\tif (not _outgoing[0]->is_type(TYPE_NODE))\n",
        "description": "Handle executable _outgoing[0] by executing it first."
      },
      {
        "file": "atomspace/opencog/atoms/flow/CollectionOfLink.cc",
        "line": 71,
        "type": "TODO",
        "content": "// TODO: Support complex type signatures beyond simple TYPE_NODE.",
        "context": "\t_have_typespec = true;\n\n\t// TODO: Handle executable _outgoing[0] by executing it first.\n\t// TODO: Support complex type signatures beyond simple TYPE_NODE.\n\t// Current implementation assumes simple type specification.\n\tif (not _outgoing[0]->is_type(TYPE_NODE))\n\t\tthrow InvalidParamException(TRACE_INFO,\n",
        "description": "Support complex type signatures beyond simple TYPE_NODE."
      },
      {
        "file": "atomspace/opencog/atoms/flow/ValueOfLink.cc",
        "line": 84,
        "type": "TODO",
        "content": "// XXX TODO FIXME ... if either of these are executable, then",
        "context": "\t// space; we can add the Atom there, and things will\n\t// trickle out properly in the end.\n\t//\n\t// XXX TODO FIXME ... if either of these are executable, then\n\t// they need to be executed, first, right? Yes, they do! We\n\t// can currently get away with not doing this for two reasons:\n\t// In all existing code, the first Atom is always an anchor,\n",
        "description": "FIXME ... if either of these are executable, then"
      },
      {
        "file": "atomspace/opencog/atoms/join/JoinLink.cc",
        "line": 550,
        "type": "TODO",
        "content": "/// TODO: it might be faster to use hash tables instead of rb-trees",
        "context": "/// think of any way of combining steps (2) and (3) that would avoid\n/// step (4) ... or even would reduce the work for stpe (4). Oh well.\n///\n/// TODO: it might be faster to use hash tables instead of rb-trees\n/// i.e. to use UnorderedHandleSet instead of HandleSet. XXX FIXME.\nHandleSet JoinLink::supremum(AtomSpace* as, bool silent,\n                             Traverse& trav) const\n",
        "description": "it might be faster to use hash tables instead of rb-trees"
      },
      {
        "file": "atomspace/opencog/atoms/join/JoinLink.cc",
        "line": 722,
        "type": "FIXME",
        "content": "// XXX FIXME this is really dumb, using a queue and then",
        "context": "\n\tHandleSet hs = container(as, jcb, silent);\n\n\t// XXX FIXME this is really dumb, using a queue and then\n\t// copying things into it. Whatever. Fix this.\n\tQueueValuePtr qvp(createQueueValue());\n\tfor (const Handle& h : hs)\n",
        "description": "this is really dumb, using a queue and then"
      },
      {
        "file": "atomspace/opencog/atoms/parallel/ExecuteThreadedLink.cc",
        "line": 59,
        "type": "TODO",
        "content": "/// XXX TODO: We could have a non-blocking version of this atom. We",
        "context": "/// Atoms in the set. If the NumberNode is present, then the number of\n/// threads is the smaller of the NumberNode and the seize of the Set.\n///\n/// XXX TODO: We could have a non-blocking version of this atom. We\n/// could just return the QueueValue immediately; the user could check\n/// to see if the queue is closed, to find out if the threads have\n/// finished.\n",
        "description": "We could have a non-blocking version of this atom. We"
      },
      {
        "file": "atomspace/opencog/atoms/pattern/PatternUtils.cc",
        "line": 55,
        "type": "FIXME",
        "content": "// XXX FIXME Are the below needed?",
        "context": "\t\t      or nameserver().isA(clause->getOutgoingAtom(0)->get_type(),\n\t\t                          EVALUATABLE_LINK)))\n\n\t\t// XXX FIXME Are the below needed?\n\t\tor contains_atomtype(clause, DEFINED_PREDICATE_NODE)\n\t\tor contains_atomtype(clause, DEFINED_SCHEMA_NODE)\n\t\tor is_black_box(clause);\n",
        "description": "Are the below needed?"
      },
      {
        "file": "atomspace/opencog/atoms/pattern/PatternLink.cc",
        "line": 146,
        "type": "FIXME",
        "content": "// XXX FIXME, more correct would be to loop over",
        "context": "\t{\n\t\t// The variables for that component are just the variables\n\t\t// that can be found in that component.\n\t\t// XXX FIXME, more correct would be to loop over\n\t\t// _pat.clause_variables and add those. Probably makes\n\t\t// no difference in most cases.\n\t\tFindAtoms fv(_variables.varset);\n",
        "description": ", more correct would be to loop over"
      },
      {
        "file": "atomspace/opencog/atoms/pattern/PatternLink.cc",
        "line": 165,
        "type": "FIXME",
        "content": "// XXX FIXME, this handles `absents`, `always` and `grouping`",
        "context": "\t\tunbundle_clauses(h);\n\n\t\t// Each component consists of the assorted parts.\n\t\t// XXX FIXME, this handles `absents`, `always` and `grouping`\n\t\t// incorrectly.\n\t\tHandleSeq clseq;\n\t\tfor (const PatternTermPtr& ptm: _pat.pmandatory)\n",
        "description": ", this handles `absents`, `always` and `grouping`"
      },
      {
        "file": "atomspace/opencog/atoms/pattern/PatternLink.cc",
        "line": 1039,
        "type": "FIXME",
        "content": "/// XXX FIXME: the code here assumes that the situation is indeed",
        "context": "/// If the ImplicationLink is suitably simple, it can be added\n/// as an ordinary clause, and searched for as if it was \"present\".\n///\n/// XXX FIXME: the code here assumes that the situation is indeed\n/// simple: more complex cases are not handled correctly.  Doing this\n/// correctly would require iterating again, and examining the\n/// contents of the left and right side of the IdenticalLink... ugh.\n",
        "description": "the code here assumes that the situation is indeed"
      },
      {
        "file": "atomspace/opencog/atoms/pattern/PatternLink.cc",
        "line": 1255,
        "type": "FIXME",
        "content": "// XXX FIXME -- this is wrong. What we really want is to",
        "context": "\t\t\t_pat.have_evaluatables = true;\n\t\t\tptm->addEvaluatable();\n\n\t\t\t// XXX FIXME -- this is wrong. What we really want is to\n\t\t\t// identify those clauses that bridge across multiple\n\t\t\t// components... not everything here does so. The\n\t\t\t// get_bridged_components() should be modified to\n",
        "description": "-- this is wrong. What we really want is to"
      },
      {
        "file": "atomspace/opencog/atoms/pattern/PatternTerm.h",
        "line": 80,
        "type": "TODO",
        "content": "// TODO: it would probably be more efficient to swap which of these",
        "context": "\tHandle _handle;\n\tHandle _quote;\n\n\t// TODO: it would probably be more efficient to swap which of these\n\t// two is weak, since I think _outgoing is requested far more often\n\t// than _parent, and having it run faster would be a performance win.\n\tPatternTermPtr _parent;\n",
        "description": "it would probably be more efficient to swap which of these"
      },
      {
        "file": "atomspace/opencog/atoms/execution/Instantiator.cc",
        "line": 289,
        "type": "TODO",
        "content": "// TODO: what about globs?",
        "context": "\t\t\treturn expr;\n\n\t\t// If it is a quoted or shadowed variable don't substitute.\n\t\t// TODO: what about globs?\n\t\tif (VARIABLE_NODE == t and not context_cp.is_free_variable(expr))\n\t\t\treturn expr;\n\n",
        "description": "what about globs?"
      },
      {
        "file": "atomspace/opencog/atoms/execution/Instantiator.cc",
        "line": 599,
        "type": "FIXME",
        "content": "// XXX FIXME Can we defer the addition to the atomspace to an even",
        "context": "\t// We do this here, instead of in walk_tree(), because adding\n\t// atoms to the atomspace is an expensive process.  We can save\n\t// some time by doing it just once, right here, in one big batch.\n\t// XXX FIXME Can we defer the addition to the atomspace to an even\n\t// later time??\n\tif (_as) return _as->add_atom(grounded);\n\treturn grounded;\n",
        "description": "Can we defer the addition to the atomspace to an even"
      },
      {
        "file": "atomspace/opencog/atoms/execution/Instantiator.cc",
        "line": 643,
        "type": "FIXME",
        "content": "// XXX FIXME, we need to get rid of this call entirely, and just",
        "context": "\tif (expr->is_type(NODE) and expr->is_executable())\n\t\treturn expr->execute(_as, silent);\n\n\t// XXX FIXME, we need to get rid of this call entirely, and just\n\t// return expr->execute(_as, silent) instead, like above.\n\t// However, assorted parts are still broken and don't work.\n\tValuePtr vp(instantiate(expr, GroundingMap(), silent));\n",
        "description": ", we need to get rid of this call entirely, and just"
      },
      {
        "file": "atomspace/opencog/atoms/base/Valuation.cc",
        "line": 50,
        "type": "TODO",
        "content": "// XXX TODO -- C++ smart pointers are not atomic; we really",
        "context": "\nvoid Valuation::setValue(const ValuePtr& v)\n{\n\t// XXX TODO -- C++ smart pointers are not atomic; we really\n\t// need to use a lock here, to avoid thread-races.\n\t_value = v;\n}\n",
        "description": "-- C++ smart pointers are not atomic; we really"
      },
      {
        "file": "atomspace/opencog/atoms/core/RewriteLink.h",
        "line": 224,
        "type": "TODO",
        "content": "// TODO: we probably want to",
        "context": "\t */\n\tHandle consume_quotations() const;\n\tstatic Handle consume_quotations(const Variables& variables, const Handle& h,\n\t                                 // TODO: we probably want to\n\t                                 // move quotation,\n\t                                 // needless_quotation,\n\t                                 // clause_root and more in\n",
        "description": "we probably want to"
      },
      {
        "file": "atomspace/opencog/atoms/core/Variables.cc",
        "line": 441,
        "type": "TODO",
        "content": "// XXX TODO type-checking could be lazy; if the function is not",
        "context": "\t\t\t\"Incorrect number of arguments specified, expecting %lu got %lu\",\n\t\t\tvarseq.size(), args.size());\n\n\t// XXX TODO type-checking could be lazy; if the function is not\n\t// actually using one of the args, it's type should not be checked.\n\t// Viz., one of the arguments might be undefined, and that's OK,\n\t// if that argument is never actually used.  Fixing this requires a\n",
        "description": "type-checking could be lazy; if the function is not"
      },
      {
        "file": "atomspace/opencog/atoms/core/RandomChoice.cc",
        "line": 110,
        "type": "FIXME",
        "content": "// XXX FIXME - fix this so it can also choose a single value",
        "context": "///           AtomZ\n///\n\n// XXX FIXME - fix this so it can also choose a single value\n// out of a vector of values.\nValuePtr RandomChoiceLink::execute(AtomSpace* as, bool silent)\n{\n",
        "description": "- fix this so it can also choose a single value"
      },
      {
        "file": "atomspace/opencog/atoms/core/RandomChoice.cc",
        "line": 143,
        "type": "TODO",
        "content": "// XXX TODO if execute() above returns FloatValue, use that!",
        "context": "\t\t\tif (hw->is_executable())\n\t\t\t\thw = HandleCast(hw->execute(as, silent));\n\n\t\t\t// XXX TODO if execute() above returns FloatValue, use that!\n\t\t\tNumberNodePtr nn(NumberNodeCast(hw));\n\t\t\tif (nullptr == nn) // goto uniform;\n\t\t\t\tthrow SyntaxException(TRACE_INFO,\n",
        "description": "if execute() above returns FloatValue, use that!"
      },
      {
        "file": "atomspace/opencog/atoms/core/RandomChoice.cc",
        "line": 179,
        "type": "FIXME",
        "content": "// XXX FIXME, also allow a FloatValue!!",
        "context": "\t\tstd::vector<double> weights;\n\t\tfor (Handle h : ofirst->getOutgoingSet())\n\t\t{\n\t\t\t// XXX FIXME, also allow a FloatValue!!\n\t\t\tif (h->is_executable())\n\t\t\t\th = HandleCast(h->execute(as, silent));\n\n",
        "description": ", also allow a FloatValue!!"
      },
      {
        "file": "atomspace/opencog/atoms/core/TypeNode.h",
        "line": 90,
        "type": "TODO",
        "content": "// XXX TODO ... Some types are defined. In this case,",
        "context": "\tstatic void validate(const std::string& str)\n\t{\n\t\tType t = nameserver().getType(str);\n\t\t// XXX TODO ... Some types are defined. In this case,\n\t\t// verify that the string occurs as a name inside\n\t\t// some DefineLink... if it does, then it's valid.\n\t\t// If it does not, then it's invalid.\n",
        "description": "... Some types are defined. In this case,"
      },
      {
        "file": "atomspace/opencog/atoms/core/RewriteLink.cc",
        "line": 297,
        "type": "TODO",
        "content": "// TODO: the following has no unit test!!! Yet it introduces a",
        "context": "\t// Base case\n\tif (h->is_node())\n\t{\n\t\t// TODO: the following has no unit test!!! Yet it introduces a\n\t\t// bug covered by RewriteLinkUTest::test_consume_quotations_4(),\n\t\t// thus this code is disabled till a unit test it created for it\n\t\t// and we understand what it fixes and how it fixes.\n",
        "description": "the following has no unit test!!! Yet it introduces a"
      },
      {
        "file": "atomspace/opencog/atoms/core/RewriteLink.cc",
        "line": 341,
        "type": "TODO",
        "content": "// TODO: generalize with when Unquote and Quote are apart",
        "context": "\t\t\t// A succession of (Unquote (Quote ..)) is an involution\n\t\t\t// and thus can be remove.\n\t\t\t//\n\t\t\t// TODO: generalize with when Unquote and Quote are apart\n\t\t\tif (child->get_type() == QUOTE_LINK)\n\t\t\t{\n\t\t\t\tquotation.update(child->get_type());\n",
        "description": "generalize with when Unquote and Quote are apart"
      },
      {
        "file": "atomspace/opencog/atoms/core/Checkers.cc",
        "line": 95,
        "type": "FIXME",
        "content": "// XXX FIXME ... Perhaps IntersectionLink, UnionLink will",
        "context": "\t\t// want to forbid it in the future by maybe introducing a\n\t\t// specialized operator to explicitly map the higher order into\n\t\t// the lower order but as of today it is required.\n\t\t// XXX FIXME ... Perhaps IntersectionLink, UnionLink will\n\t\t// resolve this?\n\t\tif (h->is_type(SIMILARITY_LINK) or\n\t\t    h->is_type(MEMBER_LINK))\n",
        "description": "... Perhaps IntersectionLink, UnionLink will"
      },
      {
        "file": "atomspace/opencog/atoms/reduct/DecimateLink.cc",
        "line": 65,
        "type": "FIXME",
        "content": "// XXX FIXME ... both the NumberNode and the FloatValue variations",
        "context": "\t\treturn do_execute(vmask, vi);\n\t}\n\n\t// XXX FIXME ... both the NumberNode and the FloatValue variations\n\t// below make a copy of the mask.  Instead of making a copy, create\n\t// something more efficient/faster. It is, after all, a simple\n\t// test...\n",
        "description": "... both the NumberNode and the FloatValue variations"
      },
      {
        "file": "atomspace/opencog/atoms/reduct/AccumulateLink.cc",
        "line": 71,
        "type": "TODO",
        "content": "// XXX TODO -- we could also handle vectors of strings, by",
        "context": "\t\treturn createFloatValue(acc);\n\t}\n\n\t// XXX TODO -- we could also handle vectors of strings, by\n\t// concatenating them into one long string.  However, for this\n\t// to be generally useful, we'd want to insert whitespace in\n\t// between. But how? One way would be to pass another argument\n",
        "description": "-- we could also handle vectors of strings, by"
      },
      {
        "file": "atomspace/tests/cython/atomspace/test_atomspace.py",
        "line": 362,
        "type": "FIXME",
        "content": "# XXX FIXME is testing the name of the bottom type",
        "context": "    def test_get_type_name(self):\n        self.assertEqual(get_type_name(types.Node), \"Node\")\n        self.assertEqual(get_type_name(2231), \"\")\n        # XXX FIXME is testing the name of the bottom type\n        # a sane thing to do?\n        self.assertEqual(get_type_name(types.NO_TYPE), \"*** Bottom Type! ***\")\n",
        "description": "is testing the name of the bottom type"
      },
      {
        "file": "atomspace-storage/opencog/persist/flow/StoreValueOfLink.cc",
        "line": 61,
        "type": "TODO",
        "content": "// XXX TODO FIXME ... if either of these are executable, then",
        "context": "{\n\tStorageNodePtr stnp = StorageNodeCast(_outgoing[2]);\n\n\t// XXX TODO FIXME ... if either of these are executable, then\n\t// they need to be executed, first, right? Because that's the\n\t// usual intent. Else they'd be wrapped in a DontExecLink, right?\n\t// I'm confused.\n",
        "description": "FIXME ... if either of these are executable, then"
      },
      {
        "file": "atomspace-storage/opencog/persist/flow/FetchValueOfLink.cc",
        "line": 63,
        "type": "TODO",
        "content": "// XXX TODO FIXME ... if either of _outgoing[0] or _outgoing[1]",
        "context": "{\n\tStorageNodePtr stnp = StorageNodeCast(_outgoing[2]);\n\n\t// XXX TODO FIXME ... if either of _outgoing[0] or _outgoing[1]\n\t// are executable, then they need to be executed, first, right?\n\t// Yes, they do. But, for just right now, we don't, to stay\n\t// compatible with ValueOfLink. See comments in that code.\n",
        "description": "FIXME ... if either of _outgoing[0] or _outgoing[1]"
      },
      {
        "file": "atomspace-storage/opencog/persist/csv/table_read.h",
        "line": 38,
        "type": "TODO",
        "content": "// TODO: Should this be a StringValue?",
        "context": "\nnamespace opencog {\n\n// TODO: Should this be a StringValue?\ntypedef std::vector<std::string> string_seq;\n\n/**\n",
        "description": "Should this be a StringValue?"
      },
      {
        "file": "atomspace-storage/opencog/persist/proxy/ProxyNode.cc",
        "line": 141,
        "type": "FIXME",
        "content": "// XXX FIXME. Using this ProxyParametersLink thing is a kind of",
        "context": "// Get our configuration from the DefineLink we live in.\n// Hmm, perhaps this should be a StateLink?\n//\n// XXX FIXME. Using this ProxyParametersLink thing is a kind of\n// cheesy hack, to pass parameters to the ProxyNode. It vaguely\n// resembles the structure of an ExecutionLink, but instead of\n// writing (Execution (Predicate \"foo\") (List (args...)))\n",
        "description": ". Using this ProxyParametersLink thing is a kind of"
      },
      {
        "file": "atomspace-storage/opencog/persist/proxy/WriteBufferProxy.cc",
        "line": 195,
        "type": "FIXME",
        "content": "// XXX FIXME. Buffering these naively, like this, voilates the",
        "context": "void WriteBufferProxy::updateValue(const Handle& atom, const Handle& key,\n                                   const ValuePtr& delta)\n{\n\t// XXX FIXME. Buffering these naively, like this, voilates the\n\t// intent of how this method should work. However, for the\n\t// RocksStorageNode, doing this is harmless. And the\n\t// CogStorageNode is just a pass-through. So there are no\n",
        "description": ". Buffering these naively, like this, voilates the"
      },
      {
        "file": "hypermind/hypermind.hpp",
        "line": 166,
        "type": "TODO",
        "content": "// TODO: Serialize message to wire format (e.g., Protocol Buffers, MessagePack)",
        "context": "    \n    bool send_network_message(Message* msg, const std::string& dest_address) {\n        // STUB: Network serialization not yet implemented\n        // TODO: Serialize message to wire format (e.g., Protocol Buffers, MessagePack)\n        // TODO: Send over actual network transport (ZeroMQ, gRPC, raw TCP/IP)\n        // TODO: Handle network errors and retries\n        // Reference: specs/integrations.zpp SendNetworkMessage operation\n",
        "description": "Serialize message to wire format (e.g., Protocol Buffers, MessagePack)"
      },
      {
        "file": "hypermind/hypermind.hpp",
        "line": 167,
        "type": "TODO",
        "content": "// TODO: Send over actual network transport (ZeroMQ, gRPC, raw TCP/IP)",
        "context": "    bool send_network_message(Message* msg, const std::string& dest_address) {\n        // STUB: Network serialization not yet implemented\n        // TODO: Serialize message to wire format (e.g., Protocol Buffers, MessagePack)\n        // TODO: Send over actual network transport (ZeroMQ, gRPC, raw TCP/IP)\n        // TODO: Handle network errors and retries\n        // Reference: specs/integrations.zpp SendNetworkMessage operation\n        \n",
        "description": "Send over actual network transport (ZeroMQ, gRPC, raw TCP/IP)"
      },
      {
        "file": "hypermind/hypermind.hpp",
        "line": 185,
        "type": "TODO",
        "content": "// TODO: Receive data from network transport",
        "context": "    \n    bool receive_network_message(NetworkEnvelope& envelope) {\n        // STUB: Network deserialization not yet implemented\n        // TODO: Receive data from network transport\n        // TODO: Deserialize from wire format to NetworkEnvelope\n        // TODO: Handle network errors and timeouts\n        // Reference: specs/integrations.zpp ReceiveNetworkMessage operation\n",
        "description": "Receive data from network transport"
      },
      {
        "file": "hypermind/hypermind.hpp",
        "line": 186,
        "type": "TODO",
        "content": "// TODO: Deserialize from wire format to NetworkEnvelope",
        "context": "    bool receive_network_message(NetworkEnvelope& envelope) {\n        // STUB: Network deserialization not yet implemented\n        // TODO: Receive data from network transport\n        // TODO: Deserialize from wire format to NetworkEnvelope\n        // TODO: Handle network errors and timeouts\n        // Reference: specs/integrations.zpp ReceiveNetworkMessage operation\n        \n",
        "description": "Deserialize from wire format to NetworkEnvelope"
      },
      {
        "file": "hypermind/hypermind.hpp",
        "line": 297,
        "type": "stub",
        "content": "return nullptr; // STUB",
        "context": "        /* Get session state from session state map */\n        // TODO: Implement hash lookup in _session_map\n        (void)session_id; // Suppress unused warning\n        return nullptr; // STUB\n    };\n    \n    void handle_activate(ID session_id, ValueArray* sum_array) {\n",
        "description": "// STUB\n"
      },
      {
        "file": "hypermind/hypermind.hpp",
        "line": 358,
        "type": "TODO",
        "content": "// TODO: Manage GPU memory allocation and transfers",
        "context": "    void submit_gpu_operation(GPUOperation* op) {\n        // STUB: GPU integration not yet implemented\n        // TODO: Implement CUDA/OpenCL submission\n        // TODO: Manage GPU memory allocation and transfers\n        // TODO: Submit computation kernel to GPU stream\n        // Reference: specs/integrations.zpp SubmitGPUComputation operation\n        if (!op) {\n",
        "description": "Manage GPU memory allocation and transfers"
      },
      {
        "file": "hypermind/hypermind.hpp",
        "line": 359,
        "type": "TODO",
        "content": "// TODO: Submit computation kernel to GPU stream",
        "context": "        // STUB: GPU integration not yet implemented\n        // TODO: Implement CUDA/OpenCL submission\n        // TODO: Manage GPU memory allocation and transfers\n        // TODO: Submit computation kernel to GPU stream\n        // Reference: specs/integrations.zpp SubmitGPUComputation operation\n        if (!op) {\n            handle_error(IntegrationError{ERROR, \"Null GPU operation pointer\", \"GPU\", -1});\n",
        "description": "Submit computation kernel to GPU stream"
      },
      {
        "file": "hypermind/hypermind.hpp",
        "line": 371,
        "type": "TODO",
        "content": "// TODO: Use connection pooling for efficiency",
        "context": "    void submit_database_query(DatabaseQuery* query) {\n        // STUB: Database integration not yet implemented\n        // TODO: Implement PostgreSQL async query submission\n        // TODO: Use connection pooling for efficiency\n        // TODO: Submit to PostgreSQL pipe for async processing\n        // Reference: specs/integrations.zpp SubmitDatabaseOperation operation\n        if (!query) {\n",
        "description": "Use connection pooling for efficiency"
      },
      {
        "file": "hypermind/hypermind.hpp",
        "line": 372,
        "type": "TODO",
        "content": "// TODO: Submit to PostgreSQL pipe for async processing",
        "context": "        // STUB: Database integration not yet implemented\n        // TODO: Implement PostgreSQL async query submission\n        // TODO: Use connection pooling for efficiency\n        // TODO: Submit to PostgreSQL pipe for async processing\n        // Reference: specs/integrations.zpp SubmitDatabaseOperation operation\n        if (!query) {\n            handle_error(IntegrationError{ERROR, \"Null database query pointer\", \"Database\", -1});\n",
        "description": "Submit to PostgreSQL pipe for async processing"
      },
      {
        "file": "ure/opencog/ure/Rule.h",
        "line": 373,
        "type": "TODO",
        "content": "// TODO: subdivide in smaller and shared mutexes",
        "context": "\t// True if the rule has already been applied.\n\tbool _exhausted;\n\n\t// TODO: subdivide in smaller and shared mutexes\n\tmutable std::mutex _mutex;\n\n\t// Return a copy of the rule with the variables alpha-converted\n",
        "description": "subdivide in smaller and shared mutexes"
      },
      {
        "file": "ure/opencog/ure/forwardchainer/FCStat.h",
        "line": 76,
        "type": "TODO",
        "content": "// TODO: subdivide in smaller and shared mutexes",
        "context": "\tstd::vector<InferenceRecord> _inf_rec;\n\tAtomSpace* _trace_as;\n\n\t// TODO: subdivide in smaller and shared mutexes\n\tmutable std::mutex _whole_mutex;\n};\n\n",
        "description": "subdivide in smaller and shared mutexes"
      },
      {
        "file": "ure/opencog/ure/forwardchainer/ForwardChainer.h",
        "line": 237,
        "type": "TODO",
        "content": "// TODO: subdivide in smaller and shared mutexes",
        "context": "\n\tbool _search_focus_set;\n\n\t// TODO: subdivide in smaller and shared mutexes\n\tmutable std::mutex _whole_mutex;\n\tmutable std::mutex _part_mutex;\n\n",
        "description": "subdivide in smaller and shared mutexes"
      },
      {
        "file": "ure/opencog/ure/forwardchainer/ForwardChainer.h",
        "line": 241,
        "type": "TODO",
        "content": "// TODO: use shared mutexes",
        "context": "\tmutable std::mutex _whole_mutex;\n\tmutable std::mutex _part_mutex;\n\n\t// TODO: use shared mutexes\n\tmutable std::mutex _rules_mutex;\n\n\t// Keep track of the number of threads to make sure\n",
        "description": "use shared mutexes"
      },
      {
        "file": "ure/opencog/ure/forwardchainer/ForwardChainer.cc",
        "line": 101,
        "type": "TODO",
        "content": "// TODO: For now the FC follows the old standard. We may move to",
        "context": "\n\t// Set rules.\n\t_rules = _config.get_rules();\n\t// TODO: For now the FC follows the old standard. We may move to\n\t// the new standard when all rules have been ported to the new one.\n\tfor (RulePtr rule : _rules)\n\t\trule->premises_as_clauses = true;\n",
        "description": "For now the FC follows the old standard. We may move to"
      },
      {
        "file": "ure/opencog/ure/forwardchainer/ForwardChainer.cc",
        "line": 165,
        "type": "TODO",
        "content": "// TODO: if creating/destroying threads is too expensive, use a thread",
        "context": "\twhile (not termination()) do_step(_iteration++);\n}\n\n// TODO: if creating/destroying threads is too expensive, use a thread\n// pool (see boost::asio::thread_pool).\nvoid ForwardChainer::do_steps_multithread()\n{\n",
        "description": "if creating/destroying threads is too expensive, use a thread"
      },
      {
        "file": "ure/opencog/ure/forwardchainer/ForwardChainer.cc",
        "line": 289,
        "type": "TODO",
        "content": "// TODO: This can be simplified but is let here until do_step is",
        "context": "\t\t// before being passed to the new source constructor, as this\n\t\t// one will take it into account.\n\t\t//\n\t\t// TODO: This can be simplified but is let here until do_step is\n\t\t// replaced by do_step_srpi.\n\t\tdouble weight = std::min(1.0, slc_sr.source->weight);\n\t\tdouble prob = success_plty / weight;\n",
        "description": "This can be simplified but is let here until do_step is"
      },
      {
        "file": "ure/opencog/ure/forwardchainer/ForwardChainer.cc",
        "line": 372,
        "type": "TODO",
        "content": "// TODO: refine mutex",
        "context": "\nSourcePtr ForwardChainer::select_source(const std::string& msgprfx)\n{\n\t// TODO: refine mutex\n\tstd::unique_lock<std::mutex> lock(_part_mutex);\n\n\tstd::vector<double> weights = _sources.get_weights();\n",
        "description": "refine mutex"
      },
      {
        "file": "ure/opencog/ure/forwardchainer/ForwardChainer.cc",
        "line": 409,
        "type": "TODO",
        "content": "// TODO: This has the effect of deallocating the rules, which",
        "context": "\t\tif (_config.get_retry_exhausted_sources()) {\n\t\t\ture_logger().debug() << msgprfx\n\t\t\t                     << \"Reset all exhausted flags to retry them\";\n\t\t\t// TODO: This has the effect of deallocating the rules, which\n\t\t\t// might cause a memory corruption if another thread is\n\t\t\t// attempting to apply that rule at the same time.\n\t\t\t_sources.reset_exhausted();\n",
        "description": "This has the effect of deallocating the rules, which"
      },
      {
        "file": "ure/opencog/ure/forwardchainer/ForwardChainer.cc",
        "line": 523,
        "type": "TODO",
        "content": "std::lock_guard<std::mutex> lock(_rules_mutex); // TODO: refine",
        "context": "\nRuleSet ForwardChainer::get_valid_rules(const Source& source)\n{\n\tstd::lock_guard<std::mutex> lock(_rules_mutex); // TODO: refine\n\n\t// Generate all valid rules\n\tRuleSet valid_rules;\n",
        "description": "refine"
      },
      {
        "file": "ure/opencog/ure/forwardchainer/SourceSet.h",
        "line": 54,
        "type": "TODO",
        "content": "// TODO: this class has thing in common with AndBIT, maybe their",
        "context": " *\n * 4. a flag call indicating if the source expansions have been exhausted.\n */\n// TODO: this class has thing in common with AndBIT, maybe their\n// common things could be placed in a parent class.\nclass Source : public boost::totally_ordered<Source>\n{\n",
        "description": "this class has thing in common with AndBIT, maybe their"
      },
      {
        "file": "ure/opencog/ure/forwardchainer/SourceSet.h",
        "line": 151,
        "type": "TODO",
        "content": "// TODO: subdivide in smaller and shared mutexes",
        "context": "\tRuleSet rules;\n\nprivate:\n\t// TODO: subdivide in smaller and shared mutexes\n\tmutable std::mutex _mutex;\n};\n\n",
        "description": "subdivide in smaller and shared mutexes"
      },
      {
        "file": "ure/opencog/ure/forwardchainer/SourceSet.h",
        "line": 165,
        "type": "TODO",
        "content": "// TODO: this class has things in common with BIT, maybe their common",
        "context": "/**\n * Population of sources to forwardly expand. Primary owner.\n */\n// TODO: this class has things in common with BIT, maybe their common\n// things could be placed in a parent class.\nclass SourceSet\n{\n",
        "description": "this class has things in common with BIT, maybe their common"
      },
      {
        "file": "ure/opencog/ure/forwardchainer/SourceSet.h",
        "line": 223,
        "type": "TODO",
        "content": "// TODO: subdivide in smaller and shared mutexes",
        "context": "private:\n\tconst UREConfig& _config;\n\n\t// TODO: subdivide in smaller and shared mutexes\n\tmutable std::mutex _mutex;\n};\n\n",
        "description": "subdivide in smaller and shared mutexes"
      },
      {
        "file": "ure/opencog/ure/backwardchainer/TraceRecorder.h",
        "line": 94,
        "type": "TODO",
        "content": "// TODO: the TV on the evaluation link should be more carefully",
        "context": "\t// is reported to the EvaluationLink, otherwise it is not\n\t// recorded.\n\t//\n\t// TODO: the TV on the evaluation link should be more carefully\n\t// thought. For instance maybe it was already proved to begin\n\t// with.\n\tvoid proof(const Handle& andbit_fcs, const Handle& target_result);\n",
        "description": "the TV on the evaluation link should be more carefully"
      },
      {
        "file": "ure/opencog/ure/backwardchainer/BIT.cc",
        "line": 374,
        "type": "TODO",
        "content": "// TODO: is this merging necessary?",
        "context": "\tHandle nrewrite = expand_fcs_rewrite(nfcs_rewrite, rule.first);\n\n\t// Generate new vardecl\n\t// TODO: is this merging necessary?\n\tHandle merged_vardecl = merge_vardecl(nfcs_vardecl, rule_vardecl);\n\tHandle nvardecl = filter_vardecl(merged_vardecl, {npattern, nrewrite});\n\n",
        "description": "is this merging necessary?"
      },
      {
        "file": "ure/opencog/ure/backwardchainer/BIT.h",
        "line": 72,
        "type": "TODO",
        "content": "// TODO: Maybe this should be moved to BackwardChainer",
        "context": "\n\t// Estimate the probability of usefulness of expanding this\n\t// BIT-Node.\n\t// TODO: Maybe this should be moved to BackwardChainer\n\tdouble operator()() const;\n\n\tstd::string to_string(const std::string& indent=\"\") const;\n",
        "description": "Maybe this should be moved to BackwardChainer"
      },
      {
        "file": "ure/opencog/ure/backwardchainer/BackwardChainer.cc",
        "line": 288,
        "type": "TODO",
        "content": "// TODO: Maybe we could take advantage of the new read-only",
        "context": "\t// of concerns instead of the atoms themselves, and only modify\n\t// the atoms if there are existing results to copy back to _as.\n\t//\n\t// TODO: Maybe we could take advantage of the new read-only\n\t// capabilities of the AtomSpace.\n\tHandle hresult = HandleCast(fcs->execute(tmp_as.get()));\n\tHandleSeq results;\n",
        "description": "Maybe we could take advantage of the new read-only"
      },
      {
        "file": "ure/opencog/ure/backwardchainer/BackwardChainer.h",
        "line": 232,
        "type": "TODO",
        "content": "// TODO: perhaps move that under BIT",
        "context": "\t// Structure holding the Back Inference Tree\n\tBIT _bit;\n\n\t// TODO: perhaps move that under BIT\n\tAndBITFitness _andbit_fitness;\n\n\t// In charge of recording the inference traces\n",
        "description": "perhaps move that under BIT"
      },
      {
        "file": "ure/opencog/ure/backwardchainer/ControlPolicy.h",
        "line": 45,
        "type": "TODO",
        "content": "// TODO: maybe wrap that in a class, and use it in foward chainer",
        "context": "// selected rule fulfills the objective, which must be passed\n// to the BIT to calculate the and-BIT complexity.\n//\n// TODO: maybe wrap that in a class, and use it in foward chainer\ntypedef std::pair<RuleTypedSubstitutionPair, double> RuleSelection;\n\nclass ControlPolicy\n",
        "description": "maybe wrap that in a class, and use it in foward chainer"
      },
      {
        "file": "ure/opencog/ure/backwardchainer/Fitness.h",
        "line": 75,
        "type": "TODO",
        "content": "// TODO: we may want to move the arguments in its own class if it",
        "context": "\t\tTrace\n\t};\n\n\t// TODO: we may want to move the arguments in its own class if it\n\t// grows bigger.\n\tAndBITFitness(FitnessType ft=Uniform,\n\t              const std::set<ContentHash>& tr=std::set<ContentHash>());\n",
        "description": "we may want to move the arguments in its own class if it"
      },
      {
        "file": "ure/opencog/ure/backwardchainer/Fitness.h",
        "line": 92,
        "type": "TODO",
        "content": "// TODO: replace by class dedicated to hold the parameters",
        "context": "\tdouble operator()(const AndBIT& andbit) const;\n\nprivate:\n\t// TODO: replace by class dedicated to hold the parameters\n\tstd::set<ContentHash> _trace;\n};\n\n",
        "description": "replace by class dedicated to hold the parameters"
      },
      {
        "file": "cogzero/agentzero-core/src/ActionScheduler.cpp",
        "line": 861,
        "type": "TODO",
        "content": "// TODO: Replace with actual SpaceTimeIntegrator::Trajectory when linking is available",
        "context": "    }\n    \n    // Plan and validate trajectory using SpaceTimeIntegrator\n    // TODO: Replace with actual SpaceTimeIntegrator::Trajectory when linking is available\n    TrajectoryPlaceholder trajectory;\n    auto trajectory_end_time = start_time + std::chrono::minutes(5); // Assume 5-minute trajectory duration\n    \n",
        "description": "Replace with actual SpaceTimeIntegrator::Trajectory when linking is available"
      },
      {
        "file": "cogzero/agentzero-core/src/ActionScheduler.cpp",
        "line": 936,
        "type": "TODO",
        "content": "// TODO: Replace with actual SpaceTimeIntegrator call when linking is available",
        "context": "    }\n    \n    // Find optimal time window using SpaceTimeIntegrator\n    // TODO: Replace with actual SpaceTimeIntegrator call when linking is available\n    struct {\n        bool feasible = true;\n        std::chrono::steady_clock::time_point optimal_start_time;\n",
        "description": "Replace with actual SpaceTimeIntegrator call when linking is available"
      },
      {
        "file": "cogzero/agentzero-core/src/ActionScheduler.cpp",
        "line": 1005,
        "type": "TODO",
        "content": "// TODO: Replace with actual configuration when linking is available",
        "context": "        \n        if (_spacetime_integrator) {\n            // Configure the integrator\n            // TODO: Replace with actual configuration when linking is available\n            /*\n            SpaceTimeIntegrator::Configuration config;\n            config.spatial_resolution = spatial_resolution;\n",
        "description": "Replace with actual configuration when linking is available"
      },
      {
        "file": "cogzero/agentzero-python-bridge/opencog/agentzero/utils.py",
        "line": 10,
        "type": "stub",
        "content": "# Stubs when atomspace not available",
        "context": "try:\n    from opencog.atomspace import ConceptNode, PredicateNode, ListLink, ExecutionLink\nexcept ImportError:\n    # Stubs when atomspace not available\n    class ConceptNode:\n        def __init__(self, name):\n            self.name = name\n",
        "description": "# Stubs when atomspace not available"
      },
      {
        "file": "cogzero/agentzero-python-bridge/tests/test_agentzero.py",
        "line": 64,
        "type": "stub",
        "content": "# Stub may return empty string",
        "context": "        \"\"\"Test configuration management.\"\"\"\n        self.core.set_config(\"test_key\", \"test_value\")\n        value = self.core.get_config(\"test_key\")\n        # Stub may return empty string\n        self.assertIsInstance(value, str)\n\n\n",
        "description": "# Stub may return empty string"
      },
      {
        "file": "cogzero/agentzero-python-bridge/tests/test_agentzero.py",
        "line": 93,
        "type": "stub",
        "content": "# Stub may not update state",
        "context": "        self.assertTrue(self.loop.is_reflection_enabled())\n        \n        self.loop.enable_reflection(False)\n        # Stub may not update state\n    \n    def test_statistics(self):\n        \"\"\"Test statistics retrieval.\"\"\"\n",
        "description": "# Stub may not update state"
      },
      {
        "file": "atenspace/aten/src/ATen/ScalarOps.h",
        "line": 10,
        "type": "FIXME",
        "content": "// FIXME: this should be (and was) Scalar::toTensor, but there is currently no way",
        "context": "// This is in the c10 namespace because we use ADL to find the functions in it.\nnamespace c10 {\n\n// FIXME: this should be (and was) Scalar::toTensor, but there is currently no way\n// to implement this without going through Derived Types (which are not part of core).\ninline at::Tensor scalar_to_tensor(Scalar s, const Device device = at::kCPU) {\n  // This is the fast track we have for CPU scalar tensors.\n",
        "description": "this should be (and was) Scalar::toTensor, but there is currently no way"
      },
      {
        "file": "atenspace/aten/src/ATen/Context.cpp",
        "line": 28,
        "type": "TODO",
        "content": "// TODO: This could be bad juju if someone calls globalContext() in the",
        "context": "    : thc_state(nullptr, [](THCState* p) { /* no-op */ }),\n      thh_state(nullptr, [](THHState* p) { /* no-op */ }) {}\n\n// TODO: This could be bad juju if someone calls globalContext() in the\n// destructor of an object with static lifetime.\nContext& globalContext() {\n  static Context globalContext_;\n",
        "description": "This could be bad juju if someone calls globalContext() in the"
      },
      {
        "file": "atenspace/aten/src/ATen/Utils.h",
        "line": 68,
        "type": "TODO",
        "content": "// TODO: This unwrapping code is ONLY used for TH bindings; once TH goes",
        "context": "  return expr;\n}\n\n// TODO: This unwrapping code is ONLY used for TH bindings; once TH goes\n// away, we can delete this function\nstatic inline TensorImpl* checked_dense_tensor_unwrap(const Tensor& expr, const char * name, int pos, const char * api, bool allowNull, DeviceType device_type, ScalarType scalar_type) {\n  if(allowNull && !expr.defined()) {\n",
        "description": "This unwrapping code is ONLY used for TH bindings; once TH goes"
      },
      {
        "file": "atenspace/aten/src/ATen/Utils.h",
        "line": 111,
        "type": "TODO",
        "content": "// TODO: is this necessary?  We used to treat nullptr-vs-not in IntList differently",
        "context": "template <size_t N>\nstd::array<int64_t, N> check_intlist(ArrayRef<int64_t> list, const char * name, int pos) {\n  if (list.empty()) {\n    // TODO: is this necessary?  We used to treat nullptr-vs-not in IntList differently\n    // with strides as a way of faking optional.\n    list = {};\n  }\n",
        "description": "is this necessary?  We used to treat nullptr-vs-not in IntList differently"
      },
      {
        "file": "atenspace/aten/src/ATen/SparseTensorUtils.h",
        "line": 42,
        "type": "TODO",
        "content": "// TODO: put this into the public API",
        "context": "      values.to(self._values().options(), non_blocking, /*copy=*/true));\n}\n\n// TODO: put this into the public API\ninline bool is_same_tensor(const Tensor& lhs, const Tensor& rhs) {\n  return lhs.unsafeGetTensorImpl() == rhs.unsafeGetTensorImpl();\n}\n",
        "description": "put this into the public API"
      },
      {
        "file": "atenspace/aten/src/ATen/SparseTensorUtils.h",
        "line": 53,
        "type": "TODO",
        "content": "// TODO: Expose this for real in ATen, some day?",
        "context": "\n// Give us a new values tensor, with the same dimensionality\n// as 'values' but with a new number of non-zero elements.\n// TODO: Expose this for real in ATen, some day?\n// NB: Doesn't preserve data.\ninline Tensor new_values_with_size_of(const Tensor& values, int64_t nnz) {\n  std::vector<int64_t> size = values.sizes().vec();\n",
        "description": "Expose this for real in ATen, some day?"
      },
      {
        "file": "atenspace/aten/src/ATen/Version.cpp",
        "line": 93,
        "type": "TODO",
        "content": "ss << \"PyTorch built with:\\n\"; // TODO add the version of PyTorch",
        "context": "\nstd::string show_config() {\n  std::ostringstream ss;\n  ss << \"PyTorch built with:\\n\"; // TODO add the version of PyTorch\n\n  // Reference:\n  // https://blog.kowalczyk.info/article/j/guide-to-predefined-macros-in-c-compilers-gcc-clang-msvc-etc..html\n",
        "description": "add the version of PyTorch"
      },
      {
        "file": "atenspace/aten/src/ATen/Version.cpp",
        "line": 129,
        "type": "TODO",
        "content": "// TODO: Actually record which one we actually picked",
        "context": "#endif\n\n#ifdef USE_LAPACK\n  // TODO: Actually record which one we actually picked\n  ss << \"  - LAPACK is enabled (usually provided by MKL)\\n\";\n#endif\n\n",
        "description": "Actually record which one we actually picked"
      },
      {
        "file": "atenspace/aten/src/ATen/Version.cpp",
        "line": 134,
        "type": "TODO",
        "content": "// TODO: No version; c.f. https://github.com/Maratyszcza/NNPACK/issues/165",
        "context": "#endif\n\n#if AT_NNPACK_ENABLED()\n  // TODO: No version; c.f. https://github.com/Maratyszcza/NNPACK/issues/165\n  ss << \"  - NNPACK is enabled\\n\";\n#endif\n\n",
        "description": "No version; c.f. https://github.com/Maratyszcza/NNPACK/issues/165"
      },
      {
        "file": "atenspace/aten/src/ATen/Version.cpp",
        "line": 150,
        "type": "TODO",
        "content": "// TODO: do HIP",
        "context": "  }\n  ss << \"\\n\";\n\n  // TODO: do HIP\n  // TODO: do XLA\n\n  return ss.str();\n",
        "description": "do HIP"
      },
      {
        "file": "atenspace/aten/src/ATen/Version.cpp",
        "line": 151,
        "type": "TODO",
        "content": "// TODO: do XLA",
        "context": "  ss << \"\\n\";\n\n  // TODO: do HIP\n  // TODO: do XLA\n\n  return ss.str();\n}\n",
        "description": "do XLA"
      },
      {
        "file": "atenspace/aten/src/ATen/common_with_cwrap.py",
        "line": 45,
        "type": "TODO",
        "content": "# TODO(zach): why does cwrap not propagate 'name'? I need it",
        "context": "    # Propagate defaults from declaration to options\n    for option in declaration['options']:\n        for k, v in declaration.items():\n            # TODO(zach): why does cwrap not propagate 'name'? I need it\n            # propagaged for ATen\n            if k != 'options':\n                option.setdefault(k, v)\n",
        "description": "(zach): why does cwrap not propagate 'name'? I need it"
      },
      {
        "file": "atenspace/aten/src/ATen/common_with_cwrap.py",
        "line": 50,
        "type": "TODO",
        "content": "# TODO(zach): added option to remove keyword handling for C++ which cannot",
        "context": "            if k != 'options':\n                option.setdefault(k, v)\n\n# TODO(zach): added option to remove keyword handling for C++ which cannot\n# support it.\n\n\n",
        "description": "(zach): added option to remove keyword handling for C++ which cannot"
      },
      {
        "file": "atenspace/aten/src/ATen/NamedTensorUtils.cpp",
        "line": 47,
        "type": "TODO",
        "content": "// TODO(zou3519): Can improve message by checking if names are alignable and suggesting workarounds",
        "context": "    DimnameList names,\n    DimnameList other_names,\n    const char* action) {\n  // TODO(zou3519): Can improve message by checking if names are alignable and suggesting workarounds\n  TORCH_CHECK(false,\n      \"Error when attempting to \", action, \" dims \", names, \" and dims \",\n      other_names, \": dim \", name, \" and dim \", other_name, \" are at the same position \"\n",
        "description": "(zou3519): Can improve message by checking if names are alignable and suggesting workarounds"
      },
      {
        "file": "atenspace/aten/src/ATen/NamedTensorUtils.cpp",
        "line": 63,
        "type": "TODO",
        "content": "// TODO(zou3519): Can improve message by checking if names are alignable and suggesting workarounds",
        "context": "    return;\n  }\n  auto it = std::find(other_names.begin(), other_names.end(), name);\n  // TODO(zou3519): Can improve message by checking if names are alignable and suggesting workarounds\n  TORCH_CHECK(it == other_names.end(),\n      \"Misaligned dims when attempting to \", action, \" dims \", names, \" and dims \",\n      other_names, \": dim \", name, \" appears in a different position from the right \"\n",
        "description": "(zou3519): Can improve message by checking if names are alignable and suggesting workarounds"
      },
      {
        "file": "atenspace/aten/src/ATen/TensorUtils.h",
        "line": 39,
        "type": "TODO",
        "content": "// TODO: Consider generalizing this into a call stack.",
        "context": "\n// A string describing which function did checks on its input\n// arguments.\n// TODO: Consider generalizing this into a call stack.\nusing CheckedFrom = const char*;\n\n// The undefined convention: singular operators assume their arguments\n",
        "description": "Consider generalizing this into a call stack."
      },
      {
        "file": "atenspace/aten/src/ATen/TensorUtils.h",
        "line": 112,
        "type": "FIXME",
        "content": "// FixMe: does TensorArg slow things down?",
        "context": "CAFFE2_API void checkDefined(CheckedFrom c, const TensorArg& t);\nCAFFE2_API void checkAllDefined(CheckedFrom c, at::ArrayRef<TensorArg> t);\n\n// FixMe: does TensorArg slow things down?\nCAFFE2_API void checkBackend(\n    CheckedFrom c,\n    at::ArrayRef<Tensor> t,\n",
        "description": "does TensorArg slow things down?"
      },
      {
        "file": "atenspace/aten/src/ATen/ParallelNative.cpp",
        "line": 246,
        "type": "TODO",
        "content": "// TODO: caffe2::ThreadPool doesn't support submitting tasks separately and",
        "context": "    func();\n  }\n#else\n  // TODO: caffe2::ThreadPool doesn't support submitting tasks separately and\n  // running in parallel. Should fix it when this API becomes popular.\n  func();\n#endif // C10_MOBILE\n",
        "description": "caffe2::ThreadPool doesn't support submitting tasks separately and"
      },
      {
        "file": "atenspace/aten/src/ATen/ParallelNative.cpp",
        "line": 269,
        "type": "TODO",
        "content": "// TODO: caffe2::ThreadPool doesn't support submitting tasks separately and",
        "context": "  }\n  return future;\n#else\n  // TODO: caffe2::ThreadPool doesn't support submitting tasks separately and\n  // running in parallel. Should fix it when this API becomes popular.\n  auto future = std::make_shared<c10::ivalue::Future>(NoneType::get());\n  func();\n",
        "description": "caffe2::ThreadPool doesn't support submitting tasks separately and"
      },
      {
        "file": "atenspace/aten/src/ATen/function_wrapper.py",
        "line": 1714,
        "type": "TODO",
        "content": "# TODO: check for move semantics...",
        "context": "                    else:\n                        types = [to_return_type(arg, option)['type']\n                                 for arg in arguments]\n                        # TODO: check for move semantics...\n                        names = [arg['name'] for arg in arguments]\n                        case_body.append(CodeTemplate(\"return std::tuple<${types}>(${names});\").substitute(\n                            types=types, names=names))\n",
        "description": "check for move semantics..."
      },
      {
        "file": "atenspace/aten/src/ATen/gen.py",
        "line": 149,
        "type": "TODO",
        "content": "densities = ['Dense', 'Sparse', 'Mkldnn']  # TODO: layout instead of densities?",
        "context": "    return backend\n\nbackends = ['CPU', 'CUDA']\ndensities = ['Dense', 'Sparse', 'Mkldnn']  # TODO: layout instead of densities?\n\nquantized_backends = ['QuantizedCPU']\n\n",
        "description": "layout instead of densities?"
      },
      {
        "file": "atenspace/aten/src/ATen/cudnn/Descriptors.h",
        "line": 15,
        "type": "TODO",
        "content": "// TODO: Add constructors for all of the descriptors",
        "context": "\nnamespace at { namespace native {\n\n// TODO: Add constructors for all of the descriptors\n\ninline int dataSize(cudnnDataType_t dataType)\n{\n",
        "description": "Add constructors for all of the descriptors"
      },
      {
        "file": "atenspace/aten/src/ATen/cudnn/Descriptors.h",
        "line": 70,
        "type": "TODO",
        "content": "// TODO: Figure out why const-correctness doesn't work here",
        "context": "class TORCH_CUDA_API Descriptor\n{\npublic:\n  // TODO: Figure out why const-correctness doesn't work here\n\n  // Use desc() to access the underlying descriptor pointer in\n  // a read-only fashion.  Most client code should use this.\n",
        "description": "Figure out why const-correctness doesn't work here"
      },
      {
        "file": "atenspace/aten/src/ATen/cudnn/Utils.h",
        "line": 12,
        "type": "TODO",
        "content": "// TODO: Should getCurrentStream be a method on Context?",
        "context": "namespace at { namespace native {\n\ninline void setCuDNNStreamToCurrent() {\n  // TODO: Should getCurrentStream be a method on Context?\n  AT_CUDNN_CHECK(cudnnSetStream(getCudnnHandle(), at::cuda::getCurrentCUDAStream()));\n}\n\n",
        "description": "Should getCurrentStream be a method on Context?"
      },
      {
        "file": "atenspace/aten/src/ATen/cuda/detail/CUDAHooks.h",
        "line": 6,
        "type": "TODO",
        "content": "// TODO: No need to have this whole header, we can just put it all in",
        "context": "#include <ATen/Generator.h>\n#include <c10/util/Optional.h>\n\n// TODO: No need to have this whole header, we can just put it all in\n// the cpp file\n\nnamespace at { namespace cuda { namespace detail {\n",
        "description": "No need to have this whole header, we can just put it all in"
      },
      {
        "file": "atenspace/aten/src/ATen/cuda/detail/CUDAHooks.cpp",
        "line": 246,
        "type": "TODO",
        "content": "// TODO: Make HIPIFY understand CUDART_VERSION macro",
        "context": "  printCudaStyleVersion(runtimeVersion);\n  oss << \"\\n\";\n\n  // TODO: Make HIPIFY understand CUDART_VERSION macro\n#ifndef __HIP_PLATFORM_HCC__\n  if (runtimeVersion != CUDART_VERSION) {\n    oss << \"  - Built with CUDA Runtime \";\n",
        "description": "Make HIPIFY understand CUDART_VERSION macro"
      },
      {
        "file": "atenspace/aten/src/ATen/cuda/detail/CUDAHooks.cpp",
        "line": 284,
        "type": "TODO",
        "content": "// TODO: Check if miopen has the functions above and unify",
        "context": "  }\n#endif\n#else\n  // TODO: Check if miopen has the functions above and unify\n  oss << \"  - MIOpen \" << MIOPEN_VERSION_MAJOR << \".\" << MIOPEN_VERSION_MINOR << \".\" << MIOPEN_VERSION_PATCH << \"\\n\";\n#endif\n\n",
        "description": "Check if miopen has the functions above and unify"
      },
      {
        "file": "atenspace/aten/src/ATen/core/jit_type.h",
        "line": 1329,
        "type": "TODO",
        "content": "// TODO: static_assert that a templated function exists, and throw a friendy",
        "context": "} // namespace detail\ntemplate <class T>\ninline TypePtr getTypePtr() {\n  // TODO: static_assert that a templated function exists, and throw a friendy\n  // error message if not\n  return detail::getTypePtr_<T>::call();\n}\n",
        "description": "static_assert that a templated function exists, and throw a friendy"
      },
      {
        "file": "atenspace/aten/src/ATen/core/jit_type.h",
        "line": 1571,
        "type": "TODO",
        "content": "// TODO: once modules support arbitrary ivalue attributes, we don't need this",
        "context": "\n  // Mapping of attribute names -> their type.\n  // NOTE: this does not contain methods, which are stored in the module\n  // TODO: once modules support arbitrary ivalue attributes, we don't need this\n  // anymore.\n  // TODO: This is better represented as an OrderedDict, but alas it is not yet\n  // available from c10\n",
        "description": "once modules support arbitrary ivalue attributes, we don't need this"
      },
      {
        "file": "atenspace/aten/src/ATen/core/jit_type.h",
        "line": 1573,
        "type": "TODO",
        "content": "// TODO: This is better represented as an OrderedDict, but alas it is not yet",
        "context": "  // NOTE: this does not contain methods, which are stored in the module\n  // TODO: once modules support arbitrary ivalue attributes, we don't need this\n  // anymore.\n  // TODO: This is better represented as an OrderedDict, but alas it is not yet\n  // available from c10\n  std::vector<std::string> attributeNames_;\n  std::vector<TypePtr> attributeTypes_;\n",
        "description": "This is better represented as an OrderedDict, but alas it is not yet"
      },
      {
        "file": "atenspace/aten/src/ATen/core/blob.h",
        "line": 69,
        "type": "TODO",
        "content": "// TODO(jerryzh): add a Get(DeviceType) function?",
        "context": "   * @brief Gets the const reference of the stored object. The code checks if\n   * the stored object is of the desired type.\n   */\n  // TODO(jerryzh): add a Get(DeviceType) function?\n  template <class T>\n  const T& Get() const {\n    AT_ASSERTM(\n",
        "description": "(jerryzh): add a Get(DeviceType) function?"
      },
      {
        "file": "atenspace/aten/src/ATen/core/blob.h",
        "line": 78,
        "type": "TODO",
        "content": "// TODO: after we add Get<Tensor>(DeviceType)",
        "context": "        meta_.name(),\n        \" while caller expects \",\n        TypeMeta::TypeName<T>());\n    // TODO: after we add Get<Tensor>(DeviceType)\n    // and changed all the callsites, we can add\n    // a static assert here to enforce T != Tensor\n    return *static_cast<const T*>(pointer_);\n",
        "description": "after we add Get<Tensor>(DeviceType)"
      },
      {
        "file": "atenspace/aten/src/ATen/core/blob.h",
        "line": 108,
        "type": "TODO",
        "content": "// TODO Re-enable logging",
        "context": "    if (IsType<T>()) {\n      return static_cast<T*>(pointer_);\n    } else {\n      // TODO Re-enable logging\n      // VLOG(1) << \"Create new mutable object \" << TypeMeta::TypeName<T>();\n      return Reset<T>(new T());\n    }\n",
        "description": "Re-enable logging"
      },
      {
        "file": "atenspace/aten/src/ATen/core/blob.h",
        "line": 158,
        "type": "TODO",
        "content": "// TODO Remove ShareExternal() and have Blob always own its content",
        "context": "        TypeMeta::Make<typename std::remove_const<T>::type>()));\n  }\n\n  // TODO Remove ShareExternal() and have Blob always own its content\n  void* ShareExternal(void* allocated, const TypeMeta& meta) {\n    free_();\n    meta_ = meta;\n",
        "description": "Remove ShareExternal() and have Blob always own its content"
      },
      {
        "file": "atenspace/aten/src/ATen/core/LegacyTypeDispatch.cpp",
        "line": 5,
        "type": "TODO",
        "content": "// TODO: This could be bad juju if someone calls globalContext() in the",
        "context": "\nnamespace at {\n\n// TODO: This could be bad juju if someone calls globalContext() in the\n// destructor of an object with static lifetime.\nLegacyTypeDispatch & globalLegacyTypeDispatch() {\n  static LegacyTypeDispatch singleton;\n",
        "description": "This could be bad juju if someone calls globalContext() in the"
      },
      {
        "file": "atenspace/aten/src/ATen/core/ivalue.cpp",
        "line": 173,
        "type": "TODO",
        "content": "// TODO we should attempt to call __str__ if the object defines it.",
        "context": "    case IValue::Tag::GenericDict:\n      return printDict(out, v.toGenericDict());\n    case IValue::Tag::Object:\n      // TODO we should attempt to call __str__ if the object defines it.\n      auto obj = v.toObject();\n      // print this out the way python would do it\n      return out << \"<\" << obj->name() << \" object at \" << obj.get() << \">\";\n",
        "description": "we should attempt to call __str__ if the object defines it."
      },
      {
        "file": "atenspace/aten/src/ATen/core/List.h",
        "line": 423,
        "type": "TODO",
        "content": "// TODO Test use_count",
        "context": "   * Returns the number of Lists currently pointing to this same list.\n   * If this is the only instance pointing to this list, returns 1.\n   */\n  // TODO Test use_count\n  size_t use_count() const;\n\n  TypePtr elementType() const;\n",
        "description": "Test use_count"
      },
      {
        "file": "atenspace/aten/src/ATen/core/interned_strings.h",
        "line": 314,
        "type": "TODO",
        "content": "// TODO: eliminate me",
        "context": "  static Symbol user(const std::string & s);\n  static Symbol caffe2(const std::string & s);\n  static Symbol dimname(const std::string & s);\n  // TODO: eliminate me\n  static Symbol scope(const std::string & s);\n\n  bool is_attr() const;\n",
        "description": "eliminate me"
      },
      {
        "file": "atenspace/aten/src/ATen/core/LegacyTypeDispatch.h",
        "line": 9,
        "type": "TODO",
        "content": "// TODO: Clean up what remains here",
        "context": "//\n// This has been deprecated in favor of ATenDispatch, and in the future,\n// c10 dispatcher.\n// TODO: Clean up what remains here\n\n#include <c10/core/Backend.h>\n#include <c10/core/ScalarType.h>\n",
        "description": "Clean up what remains here"
      },
      {
        "file": "atenspace/aten/src/ATen/core/LegacyTypeDispatch.h",
        "line": 24,
        "type": "TODO",
        "content": "// TODO: Avoid use of legacyExtractTypeId here.  The key",
        "context": "class CAFFE2_API LegacyTypeDispatch {\n public:\n  void initForTensorTypeSet(TensorTypeSet ts) {\n    // TODO: Avoid use of legacyExtractTypeId here.  The key\n    // problem is that you may get a TensorTypeSet with\n    // VariableTensorId set; should you initialize the \"underlying\"\n    // type in that case?  Hard to say.\n",
        "description": "Avoid use of legacyExtractTypeId here.  The key"
      },
      {
        "file": "atenspace/aten/src/ATen/core/LegacyTypeDispatch.h",
        "line": 74,
        "type": "TODO",
        "content": "// TODO: Since `torch::NoGradGuard` serves almost the same purpose in libtorch,",
        "context": "// when enabled will cause `legacyTensorType()` and `getType()` to always return\n// non-Variable type, even if the tensor being called on is a variable.\n//\n// TODO: Since `torch::NoGradGuard` serves almost the same purpose in libtorch,\n// we should merge these two thread-local guards.  However, NoGradGuard does\n// something subtly different: it turns off gradient recording, but DOES NOT\n// skip VariableType implementation (as we still might need to profile or\n",
        "description": "Since `torch::NoGradGuard` serves almost the same purpose in libtorch,"
      },
      {
        "file": "atenspace/aten/src/ATen/core/LegacyTypeDispatch.h",
        "line": 83,
        "type": "TODO",
        "content": "// TODO: Eliminate this parameter entirely",
        "context": "\nstruct CAFFE2_API AutoNonVariableTypeMode {\n  // NB: The enabled parameter must ALWAYS be black, as Henry Ford used to say.\n  // TODO: Eliminate this parameter entirely\n  AutoNonVariableTypeMode(bool enabled = true) :\n    guard_(TensorTypeId::VariableTensorId) {\n\n",
        "description": "Eliminate this parameter entirely"
      },
      {
        "file": "atenspace/aten/src/ATen/core/DeprecatedTypePropertiesRegistry.cpp",
        "line": 26,
        "type": "TODO",
        "content": "// TODO: This could be bad juju if someone calls globalContext() in the",
        "context": "  return *registry[static_cast<int>(p)][static_cast<int>(s)];\n}\n\n// TODO: This could be bad juju if someone calls globalContext() in the\n// destructor of an object with static lifetime.\nDeprecatedTypePropertiesRegistry & globalDeprecatedTypePropertiesRegistry() {\n  static DeprecatedTypePropertiesRegistry singleton;\n",
        "description": "This could be bad juju if someone calls globalContext() in the"
      },
      {
        "file": "atenspace/aten/src/ATen/core/stack.h",
        "line": 5,
        "type": "TODO",
        "content": "// TODO move this to c10 namespace",
        "context": "\n#include <ATen/core/ivalue.h>\n\n// TODO move this to c10 namespace\n\nnamespace torch {\nnamespace jit {\n",
        "description": "move this to c10 namespace"
      },
      {
        "file": "atenspace/aten/src/ATen/core/interned_strings.cpp",
        "line": 99,
        "type": "TODO",
        "content": "// TODO: Make this actually return something that's \"user friendly\".",
        "context": "}\n\nconst char * Symbol::toDisplayString() const {\n  // TODO: Make this actually return something that's \"user friendly\".\n  // The trouble is that, for this to be usable in printf-style assert\n  // statements, this has to return a const char* (whose lifetime is\n  // global), so we can't actually assemble a string on the fly.\n",
        "description": "Make this actually return something that's \"user friendly\"."
      },
      {
        "file": "atenspace/aten/src/ATen/core/ivalue.h",
        "line": 157,
        "type": "TODO",
        "content": "// TODO (after Tensor merge) If we pass in a Blob holding a Tensor, extract",
        "context": "\n  IValue(intrusive_ptr<caffe2::Blob> blob)\n  : tag(Tag::Blob), is_intrusive_ptr(true) {\n    // TODO (after Tensor merge) If we pass in a Blob holding a Tensor, extract\n    // and store it as a Tensor instead.\n    payload.as_intrusive_ptr = blob.release();\n  }\n",
        "description": "(after Tensor merge) If we pass in a Blob holding a Tensor, extract"
      },
      {
        "file": "atenspace/aten/src/ATen/core/List_inl.h",
        "line": 210,
        "type": "TODO",
        "content": "// TODO Use list_element_from?",
        "context": "template<class T>\ntemplate<class... Args>\ntypename List<T>::iterator List<T>::emplace(iterator pos, Args&&... value) const {\n  // TODO Use list_element_from?\n  return iterator { impl_->list.emplace(pos.iterator_, std::forward<Args>(value)...) };\n}\n\n",
        "description": "Use list_element_from?"
      },
      {
        "file": "atenspace/aten/src/ATen/core/List_inl.h",
        "line": 236,
        "type": "TODO",
        "content": "// TODO Use list_element_from?",
        "context": "template<class T>\ntemplate<class... Args>\nvoid List<T>::emplace_back(Args&&... args) const {\n  // TODO Use list_element_from?\n  impl_->list.emplace_back(std::forward<Args>(args)...);\n}\n\n",
        "description": "Use list_element_from?"
      },
      {
        "file": "atenspace/aten/src/ATen/core/dispatch/Dispatcher.h",
        "line": 177,
        "type": "TODO",
        "content": "// TODO This should be a nested lambda instead of a separate function call, but that triggers an internal",
        "context": "\n  // note: this doesn't need the mutex because write operations on the list keep iterators intact.\n  return op.operatorIterator_->op.readDispatchTable([&] (const DispatchTable& dispatchTable) -> Return {\n    // TODO This should be a nested lambda instead of a separate function call, but that triggers an internal\n    // compiler error on GCC5. Change this once we don't need gcc 5 anymore.\n    return doCallUnboxed<Return, Args...>(dispatchTable, backendFallbackKernels_, std::forward<Args>(args)...);\n  });\n",
        "description": "This should be a nested lambda instead of a separate function call, but that triggers an internal"
      },
      {
        "file": "atenspace/aten/src/ATen/core/dispatch/Dispatcher.h",
        "line": 199,
        "type": "TODO",
        "content": "// TODO This should be a nested lambda instead of a separate function call, but that triggers an internal",
        "context": "\n  // note: this doesn't need the mutex because write operations on the list keep iterators intact.\n  return op.operatorIterator_->op.readDispatchTable([&] (const DispatchTable& dispatchTable) -> Return {\n    // TODO This should be a nested lambda instead of a separate function call, but that triggers an internal\n    // compiler error on GCC5. Change this once we don't need gcc 5 anymore.\n    return doCallUnboxedOnly<Return, Args...>(dispatchTable, backendFallbackKernels_, std::forward<Args>(args)...);\n  });\n",
        "description": "This should be a nested lambda instead of a separate function call, but that triggers an internal"
      },
      {
        "file": "atenspace/aten/src/ATen/core/dispatch/DispatchTable.h",
        "line": 52,
        "type": "TODO",
        "content": "// TODO Stop generating those kernels and re-enable this assertion here.",
        "context": "    // autograd kernels for operators without tensor arguments even though\n    // they are never called. These, however, register kernels for\n    // VariableTensorId.\n    // TODO Stop generating those kernels and re-enable this assertion here.\n    auto emplaced = kernels_.emplace(dispatchKey, kernel);\n    if (!emplaced.second) {\n      // Element already existed. Overwrite it.\n",
        "description": "Stop generating those kernels and re-enable this assertion here."
      },
      {
        "file": "atenspace/aten/src/ATen/core/dispatch/DispatchKeyExtractor.h",
        "line": 21,
        "type": "TODO",
        "content": "// TODO: I'm not sure if this should live in this header or not; the operant",
        "context": "//\n// NB: I didn't make this take a Tensor to avoid header include shenanigans.\n//\n// TODO: I'm not sure if this should live in this header or not; the operant\n// question is whether or not we have access to all the relevant TLS at this\n// point.\nstatic inline TensorTypeId dispatchTypeId(TensorTypeSet ts) {\n",
        "description": "I'm not sure if this should live in this header or not; the operant"
      },
      {
        "file": "atenspace/aten/src/ATen/core/dispatch/DispatchKeyExtractor.h",
        "line": 72,
        "type": "TODO",
        "content": "// TODO Unboxed dispatch supports TensorOptions (i.e. ScalarType/Device/Layout) arguments",
        "context": "  }\n\n  c10::optional<TensorTypeId> getDispatchKeyBoxed(const Stack* stack) const {\n    // TODO Unboxed dispatch supports TensorOptions (i.e. ScalarType/Device/Layout) arguments\n    //      but boxed doesn't yet. These should be aligned and do the same thing.\n\n    TensorTypeSet ts;\n",
        "description": "Unboxed dispatch supports TensorOptions (i.e. ScalarType/Device/Layout) arguments"
      },
      {
        "file": "atenspace/aten/src/ATen/core/dispatch/DispatchKeyExtractor.h",
        "line": 91,
        "type": "TODO",
        "content": "// TODO: Don't use legacy extractor; blocked on c10 understanding variable",
        "context": "      return c10::nullopt;\n    }\n\n    // TODO: Don't use legacy extractor; blocked on c10 understanding variable\n    return c10::legacyExtractTypeId(ts);\n  }\n\n",
        "description": "Don't use legacy extractor; blocked on c10 understanding variable"
      },
      {
        "file": "atenspace/aten/src/ATen/core/dispatch/DispatchKeyExtractor.h",
        "line": 117,
        "type": "TODO",
        "content": "// TODO: a potential optimization is to store a bitfield of arg locations,",
        "context": "  // again and again for each dispatcher lookup.\n  // num_args_ is allowed to be zero; that just means you must do the\n  // fallthrough\n  // TODO: a potential optimization is to store a bitfield of arg locations,\n  size_t num_args_;\n};\n\n",
        "description": "a potential optimization is to store a bitfield of arg locations,"
      },
      {
        "file": "atenspace/aten/src/ATen/core/op_registration/op_registration_test.cpp",
        "line": 114,
        "type": "TODO",
        "content": "// TODO Rewrite (since this is now allowed) and reenable",
        "context": "  EXPECT_TRUE(called);\n}\n\n// TODO Rewrite (since this is now allowed) and reenable\n// TEST(OperatorRegistrationTest, givenOpWithCatchallKernel_whenRegisteringDispatchedKernel_thenFails) {\n//   bool called = false;\n//   auto registrar = c10::RegisterOperators().op(\"_test::dummy(Tensor dummy) -> ()\", c10::RegisterOperators::options().catchAllKernel<MockKernel>(&called));\n",
        "description": "Rewrite (since this is now allowed) and reenable"
      },
      {
        "file": "atenspace/aten/src/ATen/core/op_registration/op_registration_test.cpp",
        "line": 147,
        "type": "TODO",
        "content": "// TODO Rewrite (since this is now allowed) and reenable",
        "context": "  EXPECT_TRUE(called);\n}\n\n// TODO Rewrite (since this is now allowed) and reenable\n// TEST(OperatorRegistrationTest, givenOpWithDispatchedKernel_whenRegisteringCatchallKernel_thenFails) {\n//   bool called = false;\n//   auto registrar = c10::RegisterOperators().op(\"_test::dummy(Tensor dummy) -> ()\", c10::RegisterOperators::options().kernel<MockKernel>(c10::TensorTypeId::CPUTensorId, &called));\n",
        "description": "Rewrite (since this is now allowed) and reenable"
      },
      {
        "file": "atenspace/aten/src/ATen/core/op_registration/op_registration_test.cpp",
        "line": 642,
        "type": "TODO",
        "content": "// TODO Reenable these",
        "context": "  called_autograd = true;\n}\n\n// TODO Reenable these\n// TEST(OperatorRegistrationTest, whenRegisteringAutogradKernel_thenCanCallAutogradKernel) {\n//   auto registrar = c10::RegisterOperators().op(\"_test::dummy(Tensor dummy) -> ()\", c10::RegisterOperators::options()\n//     .impl_unboxedOnlyKernel<decltype(autograd_kernel), &autograd_kernel>(TensorTypeId::VariableTensorId));\n",
        "description": "Reenable these"
      },
      {
        "file": "atenspace/aten/src/ATen/core/op_registration/op_registration_test.cpp",
        "line": 772,
        "type": "TODO",
        "content": "// TODO Test Scalar",
        "context": "};\n\nTEST(OperatorRegistrationTest, testAvailableArgTypes) {\n  // TODO Test Scalar\n\n  // primitive types\n  testArgTypes<double>::test(\n",
        "description": "Test Scalar"
      },
      {
        "file": "atenspace/aten/src/ATen/core/op_registration/op_registration.h",
        "line": 227,
        "type": "TODO",
        "content": "// TODO Do schema inference without relying on WrapKernelFunction",
        "context": "      return std::move(*this).kernel(\n        std::move(dispatch_key),\n        KernelFunction::makeFromUnboxedFunction<FuncType, kernel_func>(),\n        // TODO Do schema inference without relying on WrapKernelFunction\n        detail::FunctionSchemaInferer<typename detail::WrapKernelFunction<FuncType, kernel_func>::type>()()\n      );\n    }\n",
        "description": "Do schema inference without relying on WrapKernelFunction"
      },
      {
        "file": "atenspace/aten/src/ATen/core/op_registration/op_registration.h",
        "line": 255,
        "type": "TODO",
        "content": "// TODO Do schema inference without relying on WrapKernelFunction",
        "context": "      return std::move(*this).kernel(\n        c10::nullopt,\n        KernelFunction::makeFromUnboxedFunction<FuncType, kernel_func>(),\n        // TODO Do schema inference without relying on WrapKernelFunction\n        detail::FunctionSchemaInferer<typename detail::WrapKernelFunction<FuncType, kernel_func>::type>()()\n      );\n    }\n",
        "description": "Do schema inference without relying on WrapKernelFunction"
      },
      {
        "file": "atenspace/aten/src/ATen/core/op_registration/op_registration.h",
        "line": 269,
        "type": "TODO",
        "content": "// TODO Do schema inference without relying on WrapKernelFunction",
        "context": "      return std::move(*this).kernel(\n        std::move(dispatch_key),\n        KernelFunction::makeFromUnboxedRuntimeFunction(kernel_func),\n        // TODO Do schema inference without relying on WrapKernelFunction\n        detail::FunctionSchemaInferer<detail::WrapRuntimeKernelFunctor<guts::decay_t<FuncType>>>()()\n      );\n    }\n",
        "description": "Do schema inference without relying on WrapKernelFunction"
      },
      {
        "file": "atenspace/aten/src/ATen/core/op_registration/op_registration.h",
        "line": 283,
        "type": "TODO",
        "content": "// TODO Do schema inference without relying on WrapKernelFunction",
        "context": "      return std::move(*this).kernel(\n        c10::nullopt,\n        KernelFunction::makeFromUnboxedRuntimeFunction(kernel_func),\n        // TODO Do schema inference without relying on WrapKernelFunction\n        detail::FunctionSchemaInferer<detail::WrapRuntimeKernelFunctor<guts::decay_t<FuncType>>>()()\n      );\n    }\n",
        "description": "Do schema inference without relying on WrapKernelFunction"
      },
      {
        "file": "atenspace/aten/src/ATen/core/op_registration/op_registration.h",
        "line": 288,
        "type": "TODO",
        "content": "// TODO Remove impl_unboxedOnlyKernel once all of aten can generate boxed kernels",
        "context": "      );\n    }\n\n    // TODO Remove impl_unboxedOnlyKernel once all of aten can generate boxed kernels\n    template<class FuncType, FuncType* kernel_func>\n    // enable_if: only enable it if FuncType is actually a function\n    guts::enable_if_t<guts::is_function_type<FuncType>::value, Options&&> impl_unboxedOnlyKernel(TensorTypeId dispatch_key) && {\n",
        "description": "Remove impl_unboxedOnlyKernel once all of aten can generate boxed kernels"
      },
      {
        "file": "atenspace/aten/src/ATen/core/op_registration/op_registration.h",
        "line": 302,
        "type": "TODO",
        "content": "// TODO Remove impl_unboxedOnlyCatchAllKernel once all of aten can generate boxed kernels",
        "context": "      );\n    }\n\n    // TODO Remove impl_unboxedOnlyCatchAllKernel once all of aten can generate boxed kernels\n    template<class FuncType, FuncType* kernel_func>\n    // enable_if: only enable it if FuncType is actually a function\n    guts::enable_if_t<guts::is_function_type<FuncType>::value, Options&&> impl_unboxedOnlyCatchAllKernel() && {\n",
        "description": "Remove impl_unboxedOnlyCatchAllKernel once all of aten can generate boxed kernels"
      },
      {
        "file": "atenspace/aten/src/ATen/core/op_registration/op_registration.h",
        "line": 351,
        "type": "TODO",
        "content": "// TODO Do schema inference without relying on WrapRuntimeKernelFunctor",
        "context": "      return std::move(*this).kernel(\n        std::move(dispatch_key),\n        KernelFunction::makeFromUnboxedLambda(std::forward<Lambda>(functor)),\n        // TODO Do schema inference without relying on WrapRuntimeKernelFunctor\n        detail::FunctionSchemaInferer<detail::WrapRuntimeKernelFunctor<guts::decay_t<Lambda>>>()()\n      );\n    }\n",
        "description": "Do schema inference without relying on WrapRuntimeKernelFunctor"
      },
      {
        "file": "atenspace/aten/src/ATen/core/op_registration/op_registration.h",
        "line": 391,
        "type": "TODO",
        "content": "// TODO Do schema inference without relying on WrapRuntimeKernelFunctor",
        "context": "      return std::move(*this).kernel(\n        c10::nullopt,\n        KernelFunction::makeFromUnboxedLambda(std::forward<Lambda>(lambda)),\n        // TODO Do schema inference without relying on WrapRuntimeKernelFunctor\n        detail::FunctionSchemaInferer<detail::WrapRuntimeKernelFunctor<guts::decay_t<Lambda>>>()()\n      );\n    }\n",
        "description": "Do schema inference without relying on WrapRuntimeKernelFunctor"
      },
      {
        "file": "atenspace/aten/src/ATen/core/op_registration/op_registration.h",
        "line": 518,
        "type": "TODO",
        "content": "// TODO Do schema inference without relying on WrapRuntimeKernelFunctor",
        "context": "     return std::move(*this).op(std::move(options).schema(schemaOrName).kernel(\n       c10::nullopt,\n       KernelFunction::makeFromUnboxedRuntimeFunction<AllowLegacyTypes>(func),\n       // TODO Do schema inference without relying on WrapRuntimeKernelFunctor\n       detail::FunctionSchemaInferer<detail::WrapRuntimeKernelFunctor<guts::decay_t<FuncType>>>()()\n     ));\n   }\n",
        "description": "Do schema inference without relying on WrapRuntimeKernelFunctor"
      },
      {
        "file": "atenspace/aten/src/ATen/core/op_registration/op_registration.h",
        "line": 548,
        "type": "TODO",
        "content": "// TODO Do schema inference without relying on WrapRuntimeKernelFunctor",
        "context": "      return std::move(*this).op(std::move(options).schema(schemaOrName).kernel(\n        c10::nullopt,\n        KernelFunction::makeFromUnboxedLambda<AllowLegacyTypes>(std::forward<Lambda>(lambda)),\n        // TODO Do schema inference without relying on WrapRuntimeKernelFunctor\n        detail::FunctionSchemaInferer<detail::WrapRuntimeKernelFunctor<guts::decay_t<Lambda>>>()()\n      ));\n    }\n",
        "description": "Do schema inference without relying on WrapRuntimeKernelFunctor"
      },
      {
        "file": "atenspace/aten/src/ATen/core/op_registration/op_registration.h",
        "line": 564,
        "type": "TODO",
        "content": "// TODO Do schema inference without relying on WrapRuntimeKernelFunctor",
        "context": "      return std::move(*this).op(std::move(options).schema(schemaOrName).kernel(\n        c10::nullopt,\n        KernelFunction::makeFromUnboxedLambda<AllowLegacyTypes>(std::forward<Lambda>(lambda)),\n        // TODO Do schema inference without relying on WrapRuntimeKernelFunctor\n        detail::FunctionSchemaInferer<detail::WrapRuntimeKernelFunctor<guts::decay_t<Lambda>>>()()\n      ));\n    }\n",
        "description": "Do schema inference without relying on WrapRuntimeKernelFunctor"
      },
      {
        "file": "atenspace/aten/src/ATen/core/boxing/kernel_functor.h",
        "line": 8,
        "type": "TODO",
        "content": "using Stack = torch::jit::Stack; // TODO Instead of this, move torch::jit::Stack to the c10 namespace.",
        "context": "\nnamespace c10 {\n\nusing Stack = torch::jit::Stack; // TODO Instead of this, move torch::jit::Stack to the c10 namespace.\n\n/**\n * Inherit from OperatorKernel to implement a c10 kernel.\n",
        "description": "Instead of this, move torch::jit::Stack to the c10 namespace."
      },
      {
        "file": "atenspace/aten/src/ATen/core/boxing/kernel_functor.h",
        "line": 83,
        "type": "TODO",
        "content": "// TODO static_assert(AllowDeprecatedTypes, \"You tried to register a kernel with an unsupported input type: std::vector<T>. Please use List<T> instead.\");",
        "context": "  struct assert_is_valid_input_type<std::vector<T>, AllowDeprecatedTypes>\n  : assert_is_valid_input_type<T, AllowDeprecatedTypes> {\n    static_assert(!std::is_same<T, at::Scalar>::value, \"You tried to register a kernel with an unsupported input type: std::vector<Scalar>. Please use List<int64_t>, List<double> or Tensor instead.\");\n    // TODO static_assert(AllowDeprecatedTypes, \"You tried to register a kernel with an unsupported input type: std::vector<T>. Please use List<T> instead.\");\n  };\n\n  // The following specialisations of assert_is_valid_input_type are technically not\n",
        "description": "static_assert(AllowDeprecatedTypes, \"You tried to register a kernel with an unsupported input type: std::vector<T>. Please use List<T> instead.\");"
      },
      {
        "file": "atenspace/aten/src/ATen/core/boxing/kernel_functor.h",
        "line": 149,
        "type": "TODO",
        "content": "// TODO static_assert(AllowDeprecatedTypes, \"You tried to register a kernel with an unsupported output type: std::vector<T>. Please use List<T> instead.\");",
        "context": "  struct assert_is_valid_output_type<std::vector<T>, AllowDeprecatedTypes>\n  : assert_is_valid_output_type<T, AllowDeprecatedTypes> {\n    static_assert(!std::is_same<T, at::Scalar>::value, \"You tried to register a kernel with an unsupported output type: std::vector<Scalar>. Please use List<int64_t>, List<double> or Tensor instead.\");\n    // TODO static_assert(AllowDeprecatedTypes, \"You tried to register a kernel with an unsupported output type: std::vector<T>. Please use List<T> instead.\");\n  };\n\n  // The following specialisations of assert_is_valid_output_type are technically not\n",
        "description": "static_assert(AllowDeprecatedTypes, \"You tried to register a kernel with an unsupported output type: std::vector<T>. Please use List<T> instead.\");"
      },
      {
        "file": "atenspace/aten/src/ATen/core/boxing/KernelFunction.h",
        "line": 33,
        "type": "TODO",
        "content": "// TODO We want to introduce the invariant that all kernels must be callable in a boxed way, then this should only check boxed_kernel_func_.",
        "context": "  {}\n\n  bool isValid() const {\n    // TODO We want to introduce the invariant that all kernels must be callable in a boxed way, then this should only check boxed_kernel_func_.\n    return boxed_kernel_func_ != nullptr || unboxed_kernel_func_ != nullptr;\n  }\n\n",
        "description": "We want to introduce the invariant that all kernels must be callable in a boxed way, then this should only check boxed_kernel_func_."
      },
      {
        "file": "atenspace/aten/src/ATen/core/boxing/KernelFunction.h",
        "line": 60,
        "type": "TODO",
        "content": "// TODO We want to introduce the invariant that all kernels must be callable in a boxed way, then this case should be impossible.",
        "context": "      if (unboxed_kernel_func_ == nullptr) {\n        TORCH_INTERNAL_ASSERT(false, \"Tried to call KernelFunction::callBoxed() on an uninitialized KernelFunction.\");\n      } else {\n        // TODO We want to introduce the invariant that all kernels must be callable in a boxed way, then this case should be impossible.\n        TORCH_INTERNAL_ASSERT(false, \"Tried to call KernelFunction::callBoxed() on a KernelFunction that can only be called with KernelFunction::callUnboxed().\");\n      }\n    }\n",
        "description": "We want to introduce the invariant that all kernels must be callable in a boxed way, then this case should be impossible."
      },
      {
        "file": "atenspace/aten/src/ATen/core/boxing/KernelFunction.h",
        "line": 90,
        "type": "TODO",
        "content": "// TODO Remove this function once all kernels support a boxed variant",
        "context": "    // forwarding, which would require Args to be deduced, but instead we\n    // want callers to explicitly specify the Args.\n\n    // TODO Remove this function once all kernels support a boxed variant\n\n    if (C10_LIKELY(unboxed_kernel_func_ != nullptr)) {\n      using ActualSignature = Return (OperatorKernel*, Args...);\n",
        "description": "Remove this function once all kernels support a boxed variant"
      },
      {
        "file": "atenspace/aten/src/ATen/core/boxing/KernelFunction.h",
        "line": 230,
        "type": "TODO",
        "content": "// TODO We want to get rid of kernels that have only an unboxed function pointer.",
        "context": "   */\n  template<class KernelFunctor>\n  static KernelFunction makeFromUnboxedOnlyFunctor(std::unique_ptr<OperatorKernel> kernelFunctor) {\n    // TODO We want to get rid of kernels that have only an unboxed function pointer.\n    //      All kernels should have a boxed pointer.\n\n    static_assert(guts::is_functor<KernelFunctor>::value, \"Tried to call KernelFunction::makeFromUnboxedFunctor<KernelFunctor> but the argument is not a functor.\");\n",
        "description": "We want to get rid of kernels that have only an unboxed function pointer."
      },
      {
        "file": "atenspace/aten/src/ATen/core/boxing/KernelFunction.h",
        "line": 284,
        "type": "TODO",
        "content": "// TODO We want to get rid of kernels that have only an unboxed function pointer.",
        "context": "   */\n  template<class FuncType, FuncType* func>\n  static KernelFunction makeFromUnboxedOnlyFunction() {\n    // TODO We want to get rid of kernels that have only an unboxed function pointer.\n    //      All kernels should have a boxed pointer.\n\n    static_assert(guts::is_function_type<FuncType>::value, \"Tried to call KernelFunction::makeFromUnboxedOnlyFunction with invalid template parameters. They must be <FuncType, *func_ptr>.\");\n",
        "description": "We want to get rid of kernels that have only an unboxed function pointer."
      },
      {
        "file": "atenspace/aten/src/ATen/core/boxing/KernelFunction.h",
        "line": 386,
        "type": "TODO",
        "content": "// TODO Reuse stack vector instead of allocating?",
        "context": "template<class Return, class... Args>\nstruct boxAndCallBoxedFunc final {\n  static Return call(KernelFunction::BoxedKernelFunction* boxed_kernel_func, OperatorKernel* functor, Args... args) {\n    // TODO Reuse stack vector instead of allocating?\n    std::vector<IValue> stack {std::forward<Args>(args)...};\n\n    (*boxed_kernel_func)(functor, &stack);\n",
        "description": "Reuse stack vector instead of allocating?"
      },
      {
        "file": "atenspace/aten/src/ATen/core/boxing/KernelFunction.h",
        "line": 398,
        "type": "TODO",
        "content": "// TODO Reuse stack vector instead of allocating?",
        "context": "template<class... Args>\nstruct boxAndCallBoxedFunc<void, Args...> final {\n  static void call(KernelFunction::BoxedKernelFunction* boxed_kernel_func, OperatorKernel* functor, Args... args) {\n    // TODO Reuse stack vector instead of allocating?\n    std::vector<IValue> stack {std::forward<Args>(args)...};\n\n    (*boxed_kernel_func)(functor, &stack);\n",
        "description": "Reuse stack vector instead of allocating?"
      },
      {
        "file": "atenspace/aten/src/ATen/core/boxing/KernelFunction_test.cpp",
        "line": 413,
        "type": "TODO",
        "content": "// TODO Also test different variants of calling unboxed with wrong signatures",
        "context": "\n}\n\n// TODO Also test different variants of calling unboxed with wrong signatures\n",
        "description": "Also test different variants of calling unboxed with wrong signatures"
      },
      {
        "file": "atenspace/aten/src/ATen/detail/CUDAHooksInterface.h",
        "line": 53,
        "type": "TODO",
        "content": "// TODO: Consider putting the stub definitions in another class, so that one",
        "context": "// (2) should filter out many ostensible use-cases, since many times a CUDA\n// function provided by ATen is only really ever used by actual CUDA code.\n//\n// TODO: Consider putting the stub definitions in another class, so that one\n// never forgets to implement each virtual function in the real implementation\n// in CUDAHooks.  This probably doesn't buy us much though.\nstruct CAFFE2_API CUDAHooksInterface {\n",
        "description": "Consider putting the stub definitions in another class, so that one"
      },
      {
        "file": "atenspace/aten/src/ATen/cpu/vec256/functional.h",
        "line": 6,
        "type": "TODO",
        "content": "// TODO: Make this more efficient",
        "context": "\nnamespace at { namespace vec256 {\n\n// TODO: Make this more efficient\ntemplate <typename scalar_t, typename Op>\ninline scalar_t vec_reduce_all(\n    const Op& vec_fun,\n",
        "description": "Make this more efficient"
      },
      {
        "file": "atenspace/aten/src/ATen/cpu/vec256/vec256.h",
        "line": 151,
        "type": "TODO",
        "content": "#else  // TODO Remove else case once switch to C++14 is finished",
        "context": "#if __cpp_binary_literals >= 201304L\n  auto a_swapped = _mm256_permute2f128_pd(a, b, 0b0100000);\n  auto b_swapped = _mm256_permute2f128_pd(a, b, 0b0110001);\n#else  // TODO Remove else case once switch to C++14 is finished\n  static constexpr int swap_ctrl_a = 0 | (2 << 4);  // 0, 2.   4 bits apart\n  static constexpr int swap_ctrl_b = 1 | (3 << 4);  // 1, 3.   4 bits apart\n  auto a_swapped = _mm256_permute2f128_pd(a, b, swap_ctrl_a);\n",
        "description": "Remove else case once switch to C++14 is finished"
      },
      {
        "file": "atenspace/aten/src/ATen/cpu/vec256/vec256.h",
        "line": 164,
        "type": "TODO",
        "content": "#else  // TODO Remove else case once switch to C++14 is finished",
        "context": "#if __cpp_binary_literals >= 201304L\n  return std::make_pair(_mm256_permute4x64_pd(a_swapped, 0b11011000),\n                        _mm256_permute4x64_pd(b_swapped, 0b11011000));\n#else  // TODO Remove else case once switch to C++14 is finished\n  static constexpr int group_ctrl = 0 | (2 << 2) | (1 << 4) | (3 << 6);  // 0, 2, 1, 3\n  return std::make_pair(_mm256_permute4x64_pd(a_swapped, group_ctrl),\n                        _mm256_permute4x64_pd(b_swapped, group_ctrl));\n",
        "description": "Remove else case once switch to C++14 is finished"
      },
      {
        "file": "atenspace/aten/src/ATen/cpu/vec256/vec256.h",
        "line": 181,
        "type": "TODO",
        "content": "// TODO: can we support caching this?",
        "context": "  // swap lanes:\n  //   a_swapped = {a0, a1, a2, a3, b0, b1, b2, b3}\n  //   b_swapped = {a4, a5, a6, a7, b4, b5, b6, b7}\n  // TODO: can we support caching this?\n#if __cpp_binary_literals >= 201304L\n  auto a_swapped = _mm256_permute2f128_ps(a, b, 0b0100000);\n  auto b_swapped = _mm256_permute2f128_ps(a, b, 0b0110001);\n",
        "description": "can we support caching this?"
      },
      {
        "file": "atenspace/aten/src/ATen/cpu/vec256/vec256.h",
        "line": 185,
        "type": "TODO",
        "content": "#else  // TODO Remove else case once switch to C++14 is finished",
        "context": "#if __cpp_binary_literals >= 201304L\n  auto a_swapped = _mm256_permute2f128_ps(a, b, 0b0100000);\n  auto b_swapped = _mm256_permute2f128_ps(a, b, 0b0110001);\n#else  // TODO Remove else case once switch to C++14 is finished\n  static constexpr int swap_ctrl_a = 0 | (2 << 4);  // 0, 2.   4 bits apart\n  static constexpr int swap_ctrl_b = 1 | (3 << 4);  // 1, 3.   4 bits apart\n  auto a_swapped = _mm256_permute2f128_ps(a, b, swap_ctrl_a);\n",
        "description": "Remove else case once switch to C++14 is finished"
      },
      {
        "file": "atenspace/aten/src/ATen/cpu/vec256/vec256.h",
        "line": 215,
        "type": "TODO",
        "content": "#else  // TODO Remove else case once switch to C++14 is finished",
        "context": "#if __cpp_binary_literals >= 201304L\n  auto a_grouped = _mm256_permute4x64_pd(a, 0b11011000);\n  auto b_grouped = _mm256_permute4x64_pd(b, 0b11011000);\n#else  // TODO Remove else case once switch to C++14 is finished\n  static constexpr int group_ctrl = 0 | (2 << 2) | (1 << 4) | (3 << 6);  // 0, 2, 1, 3\n  auto a_grouped = _mm256_permute4x64_pd(a, group_ctrl);\n  auto b_grouped = _mm256_permute4x64_pd(b, group_ctrl);\n",
        "description": "Remove else case once switch to C++14 is finished"
      },
      {
        "file": "atenspace/aten/src/ATen/cpu/vec256/vec256.h",
        "line": 227,
        "type": "TODO",
        "content": "#else  // TODO Remove else case once switch to C++14 is finished",
        "context": "#if __cpp_binary_literals >= 201304L\n  return std::make_pair(_mm256_permute2f128_pd(a_grouped, b_grouped, 0b0100000),\n                        _mm256_permute2f128_pd(a_grouped, b_grouped, 0b0110001));\n#else  // TODO Remove else case once switch to C++14 is finished\n  static constexpr int swap_ctrl_a = 0 | (2 << 4);  // 0, 2.   4 bits apart\n  static constexpr int swap_ctrl_b = 1 | (3 << 4);  // 1, 3.   4 bits apart\n  return std::make_pair(_mm256_permute2f128_pd(a_grouped, b_grouped, swap_ctrl_a),\n",
        "description": "Remove else case once switch to C++14 is finished"
      },
      {
        "file": "atenspace/aten/src/ATen/cpu/vec256/vec256.h",
        "line": 245,
        "type": "TODO",
        "content": "// TODO: can we support caching this?",
        "context": "  // group cols crossing lanes:\n  //   a_grouped = {a0, a1, a2, a3, b0, b1, b2, b3}\n  //   b_grouped = {a4, a5, a6, a7, b4, b5, b6, b7}\n  // TODO: can we support caching this?\n  const __m256i group_ctrl = _mm256_setr_epi32(0, 2, 4, 6, 1, 3, 5, 7);\n  auto a_grouped = _mm256_permutevar8x32_ps(a, group_ctrl);\n  auto b_grouped = _mm256_permutevar8x32_ps(b, group_ctrl);\n",
        "description": "can we support caching this?"
      },
      {
        "file": "atenspace/aten/src/ATen/cpu/vec256/vec256.h",
        "line": 256,
        "type": "TODO",
        "content": "#else  // TODO Remove else case once switch to C++14 is finished",
        "context": "#if __cpp_binary_literals >= 201304L\n  return std::make_pair(_mm256_permute2f128_ps(a_grouped, b_grouped, 0b0100000),\n                        _mm256_permute2f128_ps(a_grouped, b_grouped, 0b0110001));\n#else  // TODO Remove else case once switch to C++14 is finished\n  static constexpr int swap_ctrl_a = 0 | (2 << 4);  // 0, 2.   4 bits apart\n  static constexpr int swap_ctrl_b = 1 | (3 << 4);  // 1, 3.   4 bits apart\n  return std::make_pair(_mm256_permute2f128_ps(a_grouped, b_grouped, swap_ctrl_a),\n",
        "description": "Remove else case once switch to C++14 is finished"
      },
      {
        "file": "atenspace/aten/src/ATen/test/basic.cpp",
        "line": 74,
        "type": "TODO",
        "content": "// TODO:0-dim Tensor d(3.f);",
        "context": "  Tensor a = rand({3, 4}, type);\n  Tensor b = rand({3, 4}, type);\n  Tensor c = add(a, add(a, b));\n  // TODO:0-dim Tensor d(3.f);\n  Scalar d = 3.f;\n  ASSERT_TRUE(add(c, d).allclose(a + a + b + d));\n}\n",
        "description": "0-dim Tensor d(3.f);"
      },
      {
        "file": "atenspace/aten/src/ATen/test/basic.cpp",
        "line": 87,
        "type": "TODO",
        "content": "// TODO TEST PERF?",
        "context": "    add_out(r, r, d);\n  }\n  auto end = std::chrono::high_resolution_clock::now();\n  // TODO TEST PERF?\n  std::cout << std::dec << \"   \"\n            << std::chrono::duration_cast<std::chrono::milliseconds>(\n                   end - begin)\n",
        "description": "TEST PERF?"
      },
      {
        "file": "atenspace/aten/src/ATen/test/basic.cpp",
        "line": 104,
        "type": "TODO",
        "content": "// TODO TEST PERF?",
        "context": "    r = add(r, d);\n  }\n  auto end = std::chrono::high_resolution_clock::now();\n  // TODO TEST PERF?\n  std::cout << std::dec << \"   \"\n            << std::chrono::duration_cast<std::chrono::milliseconds>(\n                   end - begin)\n",
        "description": "TEST PERF?"
      },
      {
        "file": "atenspace/aten/src/ATen/test/basic.cpp",
        "line": 140,
        "type": "TODO",
        "content": "// TODO 0-dim squeeze",
        "context": "  ASSERT_EQ_RESOLVED(b.dim(), 1);\n  a = rand({1}, type);\n  b = squeeze(a);\n  // TODO 0-dim squeeze\n  ASSERT_TRUE(a[0].equal(b));\n}\n\n",
        "description": "0-dim squeeze"
      },
      {
        "file": "atenspace/aten/src/ATen/test/atest.cpp",
        "line": 69,
        "type": "TODO",
        "content": "// TODO(ezyang): maybe do a more precise exception type.",
        "context": "  ASSERT_EQ(f.sizes()[1], 2);\n  ASSERT_EQ(f.sizes()[2], 3);\n\n  // TODO(ezyang): maybe do a more precise exception type.\n  ASSERT_THROW(f.resize_({3, 4, 5}), std::exception);\n  {\n    int isgone = 0;\n",
        "description": "(ezyang): maybe do a more precise exception type."
      },
      {
        "file": "atenspace/aten/src/ATen/native/Copy.cpp",
        "line": 142,
        "type": "TODO",
        "content": "// TODO: if we need to, we can also enable this path for quantized tensor",
        "context": "    device_type = kCUDA;\n  }\n\n  // TODO: if we need to, we can also enable this path for quantized tensor\n  if (device_type == kCPU && copy_transpose_valid(self, src) && !self.is_quantized()) {\n    copy_same_type_transpose_(self, src);\n    return self;\n",
        "description": "if we need to, we can also enable this path for quantized tensor"
      },
      {
        "file": "atenspace/aten/src/ATen/native/NaiveDilatedConvolution.cpp",
        "line": 360,
        "type": "TODO",
        "content": "scalar_t scale = 1; // TODO: expose as argument?",
        "context": "            pad_size,\n            dilation_size,\n            columns.data_ptr<scalar_t>());\n        scalar_t scale = 1; // TODO: expose as argument?\n        /*\n          Compute:\n\n",
        "description": "expose as argument?"
      },
      {
        "file": "atenspace/aten/src/ATen/native/Resize.h",
        "line": 43,
        "type": "FIXME",
        "content": "// FIXME: Don't rely on storage_size being negative because this",
        "context": "    self->set_sizes_and_strides(size, *stride);\n    // NB: storage size can be different from numel.\n    for (size_t dim = 0; dim < size.size(); ++dim) {\n      // FIXME: Don't rely on storage_size being negative because this\n      // may not be true for some edge cases.\n      if (size[dim] == 0) {\n        storage_size = 0;\n",
        "description": "Don't rely on storage_size being negative because this"
      },
      {
        "file": "atenspace/aten/src/ATen/native/MaxUnpooling.cpp",
        "line": 552,
        "type": "TODO",
        "content": "// TODO (from THNN): check gradOutput shape",
        "context": "  max_unpooling3d_shape_check(\n      self, grad_output_, indices_, output_size, stride, padding);\n\n  // TODO (from THNN): check gradOutput shape\n  /* get contiguous gradOutput */\n  auto grad_output = grad_output_.contiguous();\n  auto indices = indices_.contiguous();\n",
        "description": "(from THNN): check gradOutput shape"
      },
      {
        "file": "atenspace/aten/src/ATen/native/RNN.cpp",
        "line": 389,
        "type": "TODO",
        "content": "// TODO: can use inplace ops?",
        "context": "  }\n};\n\n// TODO: can use inplace ops?\ntemplate <typename cell_params>\nstruct LSTMCell : Cell<std::tuple<Tensor, Tensor>, cell_params> {\n  using hidden_type = std::tuple<Tensor, Tensor>;\n",
        "description": "can use inplace ops?"
      },
      {
        "file": "atenspace/aten/src/ATen/native/Integration.cpp",
        "line": 17,
        "type": "TODO",
        "content": "// TODO: if we extend TensorIterator to accept 3 inputs,",
        "context": "//\n// \\sum_{i=1}^{n-1}  dx_i * (y_i + y_{i+1}) / 2\n//\n// TODO: if we extend TensorIterator to accept 3 inputs,\n// we can probably make this a bit more performant.\nTensor do_trapz(const Tensor& y, const Tensor& dx, int64_t dim) {\n    Tensor left = y.slice(dim, 0, -1);\n",
        "description": "if we extend TensorIterator to accept 3 inputs,"
      },
      {
        "file": "atenspace/aten/src/ATen/native/Normalization.cpp",
        "line": 521,
        "type": "TODO",
        "content": "// TODO: _batch_norm_impl_index_backward is only used in JIT. cudnn NHWC",
        "context": "  if (impl_index == 0) {\n    return at::native_batch_norm_backward(grad_output, input, weight, running_mean, running_var, save_mean, save_var_transform, train, epsilon, output_mask);\n  } else if (impl_index == 1) {\n    // TODO: _batch_norm_impl_index_backward is only used in JIT. cudnn NHWC\n    // format conversion is done inside cudnn_batch_norm_backward instead\n    return at::cudnn_batch_norm_backward(input, grad_output, weight, running_mean, running_var, save_mean, save_var_transform, epsilon, reservedSpace);\n  } else if (impl_index == 2) {\n",
        "description": "_batch_norm_impl_index_backward is only used in JIT. cudnn NHWC"
      },
      {
        "file": "atenspace/aten/src/ATen/native/TensorCompare.cpp",
        "line": 54,
        "type": "TODO",
        "content": "// TODO: use bitwise operator overloads once we add them",
        "context": "}\n\nTensor isclose(const Tensor& self, const Tensor& other, double rtol, double atol, bool equal_nan) {\n  // TODO: use bitwise operator overloads once we add them\n\n  TORCH_CHECK(self.scalar_type() == other.scalar_type(), self.scalar_type(), \" did not match \", other.scalar_type())\n\n",
        "description": "use bitwise operator overloads once we add them"
      },
      {
        "file": "atenspace/aten/src/ATen/native/TensorCompare.cpp",
        "line": 194,
        "type": "TODO",
        "content": "// TODO: qscheme",
        "context": "  if (self.is_quantized()) {\n    Tensor max = at::empty({0}, self.options().dtype(toUnderlying(self.scalar_type())));\n    at::native::max_out(max, max_indices, self.int_repr(), dim, keepdim);\n    // TODO: qscheme\n    return std::tuple<Tensor, Tensor>(at::_make_per_tensor_quantized_tensor(max, self.q_scale(), self.q_zero_point()), max_indices);\n  } else {\n    Tensor  max = at::empty({0}, self.options());\n",
        "description": "qscheme"
      },
      {
        "file": "atenspace/aten/src/ATen/native/TensorShape.cpp",
        "line": 683,
        "type": "TODO",
        "content": "// TODO: support negative strides",
        "context": "  dim = maybe_wrap_dim(dim, ndim);\n  auto sizes = self.sizes().vec();\n  auto strides = self.strides().vec();\n  // TODO: support negative strides\n  TORCH_CHECK(step > 0, \"slice step must be positive\");\n  if (start < 0) {\n    start += sizes[dim];\n",
        "description": "support negative strides"
      },
      {
        "file": "atenspace/aten/src/ATen/native/Embedding.cpp",
        "line": 19,
        "type": "TODO",
        "content": "// TODO: use tensor.index() after improving perf",
        "context": "  auto indices_arg = TensorArg(indices, \"indices\", 1);\n  checkScalarType(\"embedding\", indices_arg, kLong);\n\n  // TODO: use tensor.index() after improving perf\n  if (indices.dim() == 1) {\n    return weight.index_select(0, indices);\n  }\n",
        "description": "use tensor.index() after improving perf"
      },
      {
        "file": "atenspace/aten/src/ATen/native/TensorFactories.cpp",
        "line": 179,
        "type": "TODO",
        "content": "// TODO: remove when we have Type support in the IR",
        "context": "// Temporary type cast operators. These are needed to trace type-casts now since\n// Type's are not supported in the IR. Instead, we call down to these\n// specialized operators for each datatype.\n// TODO: remove when we have Type support in the IR\n\n#define DEFINE_CAST_OP(_1, n)                                    \\\n  Tensor _cast_##n(const Tensor& self, bool non_blocking) {      \\\n",
        "description": "remove when we have Type support in the IR"
      },
      {
        "file": "atenspace/aten/src/ATen/native/TensorFactories.cpp",
        "line": 219,
        "type": "TODO",
        "content": "// TODO: To support all features of MemoryFormat::Preserve we need to add",
        "context": "    auto memory_format =\n        optional_memory_format.value_or(MemoryFormat::Contiguous);\n\n    // TODO: To support all features of MemoryFormat::Preserve we need to add\n    // _empty_affine_quantized_strided function and use it similarly to\n    // Tensor clone(const Tensor& src, c10::optional<c10::MemoryFormat> optional_memory_format)\n    // if (self.is_non_overlapping_and_dense()) -> _empty_affine_quantized_strided\n",
        "description": "To support all features of MemoryFormat::Preserve we need to add"
      },
      {
        "file": "atenspace/aten/src/ATen/native/DispatchStub.h",
        "line": 33,
        "type": "stub",
        "content": "//   stub(kCPU, tensor);",
        "context": "//   REGISTER_DISPATCH(stub, &kernel);\n//\n// To call:\n//   stub(kCPU, tensor);\n//\n// TODO: CPU instruction set selection should be folded into whatever\n// the main dispatch mechanism is.\n",
        "description": "//   stub(kCPU, tensor);"
      },
      {
        "file": "atenspace/aten/src/ATen/native/DispatchStub.h",
        "line": 35,
        "type": "TODO",
        "content": "// TODO: CPU instruction set selection should be folded into whatever",
        "context": "// To call:\n//   stub(kCPU, tensor);\n//\n// TODO: CPU instruction set selection should be folded into whatever\n// the main dispatch mechanism is.\n\n// ignore warnings about DispatchStub::DEFAULT, AVX, AVX2 defined elsewhere\n",
        "description": "CPU instruction set selection should be folded into whatever"
      },
      {
        "file": "atenspace/aten/src/ATen/native/DispatchStub.h",
        "line": 134,
        "type": "TODO",
        "content": "// TODO: make this point at hip_dispatch_ptr",
        "context": "template <typename FnPtr, typename T>\nstruct RegisterHIPDispatch {\n  RegisterHIPDispatch(DispatchStub<FnPtr, T>& stub, FnPtr value) {\n    // TODO: make this point at hip_dispatch_ptr\n    stub.cuda_dispatch_ptr = value;\n  }\n};\n",
        "description": "make this point at hip_dispatch_ptr"
      },
      {
        "file": "atenspace/aten/src/ATen/native/DispatchStub.h",
        "line": 186,
        "type": "TODO",
        "content": "// TODO: cut this over to HIP dispatch once we stop pretending that CUDA",
        "context": "#if defined(__CUDACC__)\n#define REGISTER_DISPATCH(name, fn) REGISTER_CUDA_DISPATCH(name, fn)\n#elif defined(__HIPCC__)\n// TODO: cut this over to HIP dispatch once we stop pretending that CUDA\n// is HIP in the PyTorch HIPify build.\n#define REGISTER_DISPATCH(name, fn) REGISTER_CUDA_DISPATCH(name, fn)\n// #define REGISTER_DISPATCH(name, fn) REGISTER_HIP_DISPATCH(name, fn)\n",
        "description": "cut this over to HIP dispatch once we stop pretending that CUDA"
      },
      {
        "file": "atenspace/aten/src/ATen/native/AdaptiveAveragePooling.cpp",
        "line": 329,
        "type": "TODO",
        "content": "// TODO: fastpath for Channels_last should be explored later;",
        "context": "      return at::mkldnn_adaptive_avg_pool2d(input, output_size);\n    }\n\n    // TODO: fastpath for Channels_last should be explored later;\n    if (input.suggest_memory_format() == at::MemoryFormat::Contiguous && !input.is_quantized() && output_size[0] == 1 && output_size[1] == 1) {\n      // in this case, adaptive pooling is just computing mean over hw\n      // dimensions, which can be done more efficiently\n",
        "description": "fastpath for Channels_last should be explored later;"
      },
      {
        "file": "atenspace/aten/src/ATen/native/Convolution.cpp",
        "line": 918,
        "type": "TODO",
        "content": "// TODO: figure out if we can narrow gO and save some compute,",
        "context": "\n        // narrow gI to only relevant portion\n        // we do it this way because negative output_padding is not supported\n        // TODO: figure out if we can narrow gO and save some compute,\n        // rather than narrowing the computed gI\n        auto gI_size = gI.sizes();\n        auto i_size = input.sizes();\n",
        "description": "figure out if we can narrow gO and save some compute,"
      },
      {
        "file": "atenspace/aten/src/ATen/native/Convolution.cpp",
        "line": 938,
        "type": "TODO",
        "content": "// TODO: figure out why this needs to be computed...",
        "context": "        auto gOt = gO.transpose(0, 1);\n\n        // calculate output_padding\n        // TODO: figure out why this needs to be computed...\n        auto kernel_size = weight.sizes().slice(2);\n        auto input_shape = input.sizes().slice(2);\n        auto grad_output_shape = gO.sizes().slice(2);\n",
        "description": "figure out why this needs to be computed..."
      },
      {
        "file": "atenspace/aten/src/ATen/native/Resize.cpp",
        "line": 38,
        "type": "TODO",
        "content": "// TODO(VitalyFedyunin): Move it to HTML docs.",
        "context": "Tensor& resize_as_sparse_(Tensor& self, const Tensor& src);\n\n\n// TODO(VitalyFedyunin): Move it to HTML docs.\n//\n// Strides of the output tensor of `resize_as_` operator is defined by input\n// tensor strides and the value of memory_format argument.\n",
        "description": "(VitalyFedyunin): Move it to HTML docs."
      },
      {
        "file": "atenspace/aten/src/ATen/native/QuantizedLinear.cpp",
        "line": 353,
        "type": "TODO",
        "content": "// TODO(mingzhe09088):",
        "context": "  float* weight_contig_ptr = weight_contig.data_ptr<float>();\n  HandleWeightsSaturation(K * N, weight_contig_ptr);\n\n  // TODO(mingzhe09088):\n  // Consider using a functor here in PackedGemmMatrixFP16\n  // Comments from (XQ): Not entirely sure this make_unique is safe. make_unique\n  // is created with regular \"new\", and freed through TypeMetaData::deleteFn in\n",
        "description": "(mingzhe09088):"
      },
      {
        "file": "atenspace/aten/src/ATen/native/UnaryOps.cpp",
        "line": 275,
        "type": "stub",
        "content": "// stub in CUDAUnaryOps",
        "context": "}\n\n// NB: If you use this macro, you may also need to add a CUDA forwarding\n// stub in CUDAUnaryOps\n\n#define IMPLEMENT_UNARY_OP_CORE(op)                                    \\\n  Tensor op(const Tensor& self) {                                      \\\n",
        "description": "// stub in CUDAUnaryOps"
      },
      {
        "file": "atenspace/aten/src/ATen/native/TensorIterator.cpp",
        "line": 586,
        "type": "TODO",
        "content": "// TODO: check for casting once it's supported",
        "context": "}\n\nbool TensorIterator::is_trivial_1d() const {\n  // TODO: check for casting once it's supported\n  return ndim() == 1;\n}\n\n",
        "description": "check for casting once it's supported"
      },
      {
        "file": "atenspace/aten/src/ATen/native/TensorIterator.cpp",
        "line": 717,
        "type": "TODO",
        "content": "// TODO: This is only really necessary for arg{min,max}",
        "context": "  iter.promote_gpu_output_dtypes_ = true;\n  iter.resize_outputs_ = false;\n  iter.is_reduction_ = true;\n  // TODO: This is only really necessary for arg{min,max}\n  iter.compute_common_dtype_only_for_inputs();\n  iter.build();\n  return iter;\n",
        "description": "This is only really necessary for arg{min,max}"
      },
      {
        "file": "atenspace/aten/src/ATen/native/TensorIterator.cpp",
        "line": 813,
        "type": "TODO",
        "content": "// TODO: issue warning",
        "context": "    if (tensor.defined() && !tensor.sizes().equals(shape_)) {\n      if (resize_outputs_ && !operands_[i].is_read_write) {\n        // Preserve legacy resizing behavior of out=... arguments\n        // TODO: issue warning\n        tensor.resize_(shape_);\n        if (requires_channels_last_output_ && tensor.dim() == 4) {\n          // Temporary stick to 4d tensor, will update with arbitrary batched later on\n",
        "description": "issue warning"
      },
      {
        "file": "atenspace/aten/src/ATen/native/TensorIterator.cpp",
        "line": 914,
        "type": "TODO",
        "content": "//TODO enable fast handling for reductions",
        "context": "void TensorIterator::fast_set_up() {\n  //this function is called if all the inputs are contiguous to avoid needless reordering of dimensions\n  //and tracking output strides\n  //TODO enable fast handling for reductions\n  //TODO enable fast handling for channels_last\n\n  //allocate contiguous tensor for output\n",
        "description": "enable fast handling for reductions"
      },
      {
        "file": "atenspace/aten/src/ATen/native/TensorIterator.cpp",
        "line": 915,
        "type": "TODO",
        "content": "//TODO enable fast handling for channels_last",
        "context": "  //this function is called if all the inputs are contiguous to avoid needless reordering of dimensions\n  //and tracking output strides\n  //TODO enable fast handling for reductions\n  //TODO enable fast handling for channels_last\n\n  //allocate contiguous tensor for output\n  for (int i = 0; i < num_outputs_; i++){\n",
        "description": "enable fast handling for channels_last"
      },
      {
        "file": "atenspace/aten/src/ATen/native/UpSample.h",
        "line": 157,
        "type": "TODO",
        "content": "// TODO: Our current linear mode impls use unbound indices",
        "context": "    // for negative indices as they use 2 pixels to interpolate.\n    // For example, [-1, 0], they both use pixel 0 value so it\n    // doesn't affect if we bound the src_idx to 0 or not.\n    // TODO: Our current linear mode impls use unbound indices\n    // where we should and then remove this cubic flag.\n    // This matters in cubic mode, as we might need [-1, 0, 1, 2]\n    // to interpolate and the weights can be affected.\n",
        "description": "Our current linear mode impls use unbound indices"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/Conv.cpp",
        "line": 122,
        "type": "TODO",
        "content": "// TODO: Go through all the checking code again and make sure",
        "context": "\nnamespace at { namespace native {\n\n// TODO: Go through all the checking code again and make sure\n// we haven't missed anything.\n\n// ---------------------------------------------------------------------\n",
        "description": "Go through all the checking code again and make sure"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/Conv.cpp",
        "line": 197,
        "type": "TODO",
        "content": "// TODO: Move this into the standard library, with a better name?",
        "context": "  return weight_size;\n}\n\n// TODO: Move this into the standard library, with a better name?\nTensor narrowGroup(const Tensor& t, int dim, int group_idx, int64_t groups) {\n  auto group_size = t.size(dim) / groups;\n  return t.narrow(dim, group_idx * group_size, group_size);\n",
        "description": "Move this into the standard library, with a better name?"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/Conv.cpp",
        "line": 269,
        "type": "TODO",
        "content": "// TODO: check that output->size() matches output_sizes",
        "context": "  // Weight\n  checkSameDim(c, input, weight);\n\n  // TODO: check that output->size() matches output_sizes\n  // TODO: check that weight matches output->sizes()\n  checkSameDim(c, input, output);\n}\n",
        "description": "check that output->size() matches output_sizes"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/Conv.cpp",
        "line": 270,
        "type": "TODO",
        "content": "// TODO: check that weight matches output->sizes()",
        "context": "  checkSameDim(c, input, weight);\n\n  // TODO: check that output->size() matches output_sizes\n  // TODO: check that weight matches output->sizes()\n  checkSameDim(c, input, output);\n}\n\n",
        "description": "check that weight matches output->sizes()"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/Conv.cpp",
        "line": 293,
        "type": "TODO",
        "content": "// TODO: Use TensorGeometry here instead of the entire Tensor, which we",
        "context": "\n// NB: This can't be a constructor, because then ConvolutionParams\n// would not be a POD anymore.\n// TODO: Use TensorGeometry here instead of the entire Tensor, which we\n// don't actually need.  (OTOH: We can always pass in\n// grad_input/grad_output, so this is not very pressing)\nvoid setConvolutionParams(\n",
        "description": "Use TensorGeometry here instead of the entire Tensor, which we"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/Conv.cpp",
        "line": 344,
        "type": "TODO",
        "content": "// TODO: Use something less heavy duty than a big honking mutex",
        "context": "//\n// ---------------------------------------------------------------------\n\n// TODO: Use something less heavy duty than a big honking mutex\ntemplate <typename T>\nstruct BenchmarkCache {\n  std::mutex mutex;\n",
        "description": "Use something less heavy duty than a big honking mutex"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/Conv.cpp",
        "line": 370,
        "type": "TODO",
        "content": "// TODO: Stop manually allocating CUDA memory; allocate an ATen byte",
        "context": "BenchmarkCache<cudnnConvolutionBwdDataAlgoPerf_t> bwd_data_algos;\nBenchmarkCache<cudnnConvolutionBwdFilterAlgoPerf_t> bwd_filter_algos;\n\n// TODO: Stop manually allocating CUDA memory; allocate an ATen byte\n// tensor instead.\nstruct Workspace {\n  Workspace(size_t size) : size(size), data(NULL) {\n",
        "description": "Stop manually allocating CUDA memory; allocate an ATen byte"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/Conv.cpp",
        "line": 466,
        "type": "TODO",
        "content": "// TODO: Shouldn't all returned results be successful?",
        "context": "  if (args.params.deterministic) {\n    // iterate over perf results of all algorithms and find the best deterministic algo\n    for (int i = 0; i < n_algo; i++) {\n      // TODO: Shouldn't all returned results be successful?\n      // Double check documentation for cudnnFindConvolutionForwardAlgorithmEx\n      if (perfResults[i].status == CUDNN_STATUS_SUCCESS &&\n          perfResults[i].determinism == CUDNN_DETERMINISTIC) {\n",
        "description": "Shouldn't all returned results be successful?"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/Conv.cpp",
        "line": 838,
        "type": "TODO",
        "content": "// TODO: Consider renaming zero-indexed arguments to \"self\"",
        "context": "//  - Things that happen in TensorArg\n//    - Check arguments (type, GPU, shape)\n//\n// TODO: Consider renaming zero-indexed arguments to \"self\"\n\n\n\n",
        "description": "Consider renaming zero-indexed arguments to \"self\""
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/Conv.cpp",
        "line": 873,
        "type": "TODO",
        "content": "// TODO: when we do legacy group convolution support, we'll repeatedly",
        "context": "  args.odesc.set(output);\n  args.cdesc.set(dataType, input.dim() - 2, args.params.padding, args.params.stride, args.params.dilation, args.params.groups);\n\n  // TODO: when we do legacy group convolution support, we'll repeatedly\n  // reinitialize the workspace for each convolution we do.  This is\n  // wasteful; we'd rather reuse the workspace.  OTOH, legacy group\n  // convolution support is already pretty slow, so this might not\n",
        "description": "when we do legacy group convolution support, we'll repeatedly"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/RNN.cpp",
        "line": 291,
        "type": "TODO",
        "content": "// TODO: Actually, would this make ASAN's job harder catching",
        "context": "          mini_batch = input_sizes[1];\n        }\n        input_size = input_sizes[2];\n        // TODO: Actually, would this make ASAN's job harder catching\n        // an uninitialized access?\n        batch_sizes_sum = -1; // something bogus in case we access it\n      }\n",
        "description": "Actually, would this make ASAN's job harder catching"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/RNN.cpp",
        "line": 297,
        "type": "TODO",
        "content": "// TODO: check x for consistency with input_size?",
        "context": "      }\n    }\n\n    // TODO: check x for consistency with input_size?\n    std::vector<TensorDescriptor> descriptors(Tensor x) const {\n      auto is_input_packed = batch_sizes.size() != 0;\n      if (is_input_packed) {\n",
        "description": "check x for consistency with input_size?"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/RNN.cpp",
        "line": 340,
        "type": "TODO",
        "content": "// TODO: This is annoying, having to put the cudnnTensorDescriptor_t",
        "context": "      }\n    }\n\n    // TODO: This is annoying, having to put the cudnnTensorDescriptor_t\n    // in a contiguous array...\n    std::vector<cudnnTensorDescriptor_t> get_descs(const std::vector<TensorDescriptor>& descs) {\n      std::vector<cudnnTensorDescriptor_t> r;\n",
        "description": "This is annoying, having to put the cudnnTensorDescriptor_t"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/RNN.cpp",
        "line": 438,
        "type": "TODO",
        "content": "// TODO: The use of CPU tensor here is a bit goofy in C++,",
        "context": "          cudnnTensorFormat_t format;\n          int nb_dims;\n          constexpr int min_dim = 3;\n          // TODO: The use of CPU tensor here is a bit goofy in C++,\n          // some sort of alloca would be good enough except that it is\n          // kind of convenient to be able to prod() on it.\n          Tensor filter_dim_a = at::empty(min_dim, at::initialTensorOptions().dtype(kInt));\n",
        "description": "The use of CPU tensor here is a bit goofy in C++,"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/RNN.cpp",
        "line": 701,
        "type": "TODO",
        "content": "// TODO: Set device to input",
        "context": "  fn.dropout.set(fn_train, fn_dropout, fn_dropout_state);\n  fn.tensors.set(input.sizes(), fn_batch_sizes, batch_first);\n\n  // TODO: Set device to input\n\n  if (fn.rnn.mode != CUDNN_LSTM) {\n    TORCH_CHECK(!cx.defined(),\n",
        "description": "Set device to input"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/RNN.cpp",
        "line": 708,
        "type": "TODO",
        "content": "// TODO: can batch_first be a wrapper around this function?",
        "context": "             \"rnn: illegal defined cx for non-LSTM RNN\");\n  }\n\n  // TODO: can batch_first be a wrapper around this function?\n  auto is_input_packed = fn.tensors.batch_sizes.size() != 0;\n  if (batch_first && !is_input_packed) {\n    input = input.transpose(0, 1);\n",
        "description": "can batch_first be a wrapper around this function?"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/RNN.cpp",
        "line": 843,
        "type": "TODO",
        "content": "// TODO: Set device to input",
        "context": "  fn.dropout.set(fn_train, fn_dropout, fn_dropout_state);\n  fn.tensors.set(input.sizes(), fn_batch_sizes, batch_first);\n\n  // TODO: Set device to input\n  auto handle = getCudnnHandle();\n\n  if (fn.rnn.mode != CUDNN_LSTM) {\n",
        "description": "Set device to input"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/RNN.cpp",
        "line": 871,
        "type": "TODO",
        "content": "auto dx = at::empty(input.sizes(), input.options()); // TODO: more compact way of saying this",
        "context": "  auto dy = grad_output.contiguous();\n  auto y = output;\n  auto w = weight_buf;\n  auto dx = at::empty(input.sizes(), input.options()); // TODO: more compact way of saying this\n  auto dhy = grad_hy.contiguous().view(hidden_size);\n  auto dcy = grad_cy.defined() ? grad_cy.contiguous().view(hidden_size) : Tensor();\n  auto dhx = at::empty(hidden_size, hx.options());\n",
        "description": "more compact way of saying this"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/RNN.cpp",
        "line": 915,
        "type": "TODO",
        "content": "// TODO: put this in the correct device???",
        "context": "        x_descs_arr.data(),\n        &workspace_size\n        ));\n  // TODO: put this in the correct device???\n  Tensor workspace = at::empty(workspace_size, input.options().dtype(kByte));\n  setCuDNNStreamToCurrent();\n  AT_CUDNN_CHECK(cudnnRNNBackwardData(\n",
        "description": "put this in the correct device???"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/RNN.cpp",
        "line": 946,
        "type": "TODO",
        "content": "// TODO: I think tensor geometry sufficient for weight_buf/weight",
        "context": "// NB: This MUST BE CALLED AFTER _cudnn_rnn_backward_input.\n// We'll give a user friendly combined function...\nstd::vector<Tensor> _cudnn_rnn_backward_weight(\n    // TODO: I think tensor geometry sufficient for weight_buf/weight\n    const Tensor& input_r, TensorList weight_arr, int64_t weight_stride0,\n    const Tensor& weight_buf, const Tensor& hx, const Tensor& cx,\n    const Tensor& output_r,\n",
        "description": "I think tensor geometry sufficient for weight_buf/weight"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/RNN.cpp",
        "line": 1081,
        "type": "TODO",
        "content": "// TODO: I am not sure if we actually need the 'dropout' and 'train' parameters",
        "context": "  return std::tuple<Tensor, Tensor, Tensor, std::vector<Tensor>>{dx, dhx, dcx, dw};\n}\n\n// TODO: I am not sure if we actually need the 'dropout' and 'train' parameters\n// to initialize just the state tensor\nTensor _cudnn_init_dropout_state(double dropout, bool train, int64_t dropout_seed, const TensorOptions& options) {\n  auto handle = getCudnnHandle();\n",
        "description": "I am not sure if we actually need the 'dropout' and 'train' parameters"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/BatchNorm.cpp",
        "line": 76,
        "type": "TODO",
        "content": "// TODO: is weight required to be contiguous?",
        "context": "    checkAllSameType(c, {input, weight});\n  }\n  checkAllSameType(c, {weight, bias, running_mean, running_var});\n  // TODO: is weight required to be contiguous?\n  checkAllContiguous(c, {weight, bias, running_mean, running_var});\n  // TODO: TensorArg check should start handle memory format\n  TORCH_CHECK(input->is_contiguous(input->suggest_memory_format()));\n",
        "description": "is weight required to be contiguous?"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/BatchNorm.cpp",
        "line": 78,
        "type": "TODO",
        "content": "// TODO: TensorArg check should start handle memory format",
        "context": "  checkAllSameType(c, {weight, bias, running_mean, running_var});\n  // TODO: is weight required to be contiguous?\n  checkAllContiguous(c, {weight, bias, running_mean, running_var});\n  // TODO: TensorArg check should start handle memory format\n  TORCH_CHECK(input->is_contiguous(input->suggest_memory_format()));\n\n  checkDimRange(c, input, 2, 6 /* exclusive */);\n",
        "description": "TensorArg check should start handle memory format"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/BatchNorm.cpp",
        "line": 99,
        "type": "TODO",
        "content": "// TODO: The new CUDNN_BATCHNORM_SPATIAL_PERSISTENT mode was",
        "context": "    mode = CUDNN_BATCHNORM_SPATIAL;\n#endif // CUDNN_VERSION >= 7400\n  } else {\n    // TODO: The new CUDNN_BATCHNORM_SPATIAL_PERSISTENT mode was\n    // introduced in CuDNN 7 for performance optimization, but it results in\n    // accuracy losses in convolution models such as ResNeXt-101 and\n    // video R(2+1)D. We will fall back to the normal CUDNN_BATCHNORM_SPATIAL\n",
        "description": "The new CUDNN_BATCHNORM_SPATIAL_PERSISTENT mode was"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/BatchNorm.cpp",
        "line": 227,
        "type": "TODO",
        "content": "// TODO: Is it worth it to have a contiguous call or maybe we should go with",
        "context": "    const Tensor& save_mean_t, const Tensor& save_var_t,\n    double epsilon, const Tensor& reserveSpace)\n{\n  // TODO: Is it worth it to have a contiguous call or maybe we should go with\n  // whatever format is given here.\n  TensorArg input{ input_t, \"input\", 1 },\n            grad_output{ grad_output_t.contiguous(input_t.suggest_memory_format()), \"grad_output\", 2 },\n",
        "description": "Is it worth it to have a contiguous call or maybe we should go with"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/BatchNorm.cpp",
        "line": 247,
        "type": "TODO",
        "content": "// TODO: is weight required to be contiguous?",
        "context": "  }\n  checkAllSameType(c, {input, grad_output});\n  checkAllSameType(c, {weight, save_mean, save_var});\n  // TODO: is weight required to be contiguous?\n  checkAllContiguous(c, {save_mean, save_var});\n  // TODO: TensorArg check should start handle memory format\n  TORCH_CHECK(input->is_contiguous(input->suggest_memory_format()));\n",
        "description": "is weight required to be contiguous?"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/BatchNorm.cpp",
        "line": 249,
        "type": "TODO",
        "content": "// TODO: TensorArg check should start handle memory format",
        "context": "  checkAllSameType(c, {weight, save_mean, save_var});\n  // TODO: is weight required to be contiguous?\n  checkAllContiguous(c, {save_mean, save_var});\n  // TODO: TensorArg check should start handle memory format\n  TORCH_CHECK(input->is_contiguous(input->suggest_memory_format()));\n  TORCH_CHECK(grad_output->is_contiguous(grad_output->suggest_memory_format()));\n  checkDimRange(c, input, 2, 6 /* exclusive */);\n",
        "description": "TensorArg check should start handle memory format"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/BatchNorm.cpp",
        "line": 269,
        "type": "TODO",
        "content": "// TODO: The new CUDNN_BATCHNORM_SPATIAL_PERSISTENT mode was",
        "context": "    mode = CUDNN_BATCHNORM_SPATIAL;\n#endif // CUDNN_VERSION >= 7400\n  } else {\n    // TODO: The new CUDNN_BATCHNORM_SPATIAL_PERSISTENT mode was\n    // introduced in CuDNN 7 for performance optimization, but it results in\n    // accuracy losses in convolution models such as ResNeXt-101 and\n    // video R(2+1)D. We will fall back to the normal CUDNN_BATCHNORM_SPATIAL\n",
        "description": "The new CUDNN_BATCHNORM_SPATIAL_PERSISTENT mode was"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/GridSampler.cpp",
        "line": 34,
        "type": "TODO",
        "content": "// TODO: descriptor checking",
        "context": "\n#include <ATen/TensorUtils.h>\n\n// TODO: descriptor checking\n\n\nnamespace at { namespace native {\n",
        "description": "descriptor checking"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/GridSampler.cpp",
        "line": 58,
        "type": "TODO",
        "content": "// TODO: Maybe more user friendly to report where the expected size",
        "context": "  // if grid has values >1 or <-1, those values are ignored\n  checkContiguous(c, grid);\n  checkDim(c, grid, 4);\n  // TODO: Maybe more user friendly to report where the expected size\n  // came from\n  checkSize(c, grid, 0, input->size(0));\n  checkSize(c, grid, 3, 2);\n",
        "description": "Maybe more user friendly to report where the expected size"
      },
      {
        "file": "atenspace/aten/src/ATen/native/mkldnn/Normalization.cpp",
        "line": 51,
        "type": "TODO",
        "content": "// TODO: support training",
        "context": "  ideep::tensor y;\n\n  if (train) {\n    // TODO: support training\n    AT_ERROR(\"mkldnn_batch_norm: mkldnn training is not supported in yet.\");\n\n    // ideep::tensor saved_mean;\n",
        "description": "support training"
      },
      {
        "file": "atenspace/aten/src/ATen/native/mkldnn/TensorFactories.cpp",
        "line": 12,
        "type": "TODO",
        "content": "// TODO: support int64_t dims in ideep::tensor to avoid extra conversion",
        "context": "     !optional_memory_format.has_value(),\n     \"'memory_format' argument is incompatible with mkldnn tensor\");\n  // NOTE: int32_t dims from ideep::tensor but sizes needs int64_t\n  // TODO: support int64_t dims in ideep::tensor to avoid extra conversion\n  ideep::tensor::dims dst_dims (sizes.begin(), sizes.end());\n  ideep::tensor it;\n  it.resize<AllocForMKLDNN>(dst_dims, ideep::tensor::data_type::f32);\n",
        "description": "support int64_t dims in ideep::tensor to avoid extra conversion"
      },
      {
        "file": "atenspace/aten/src/ATen/native/mkldnn/MKLDNNCommon.cpp",
        "line": 45,
        "type": "TODO",
        "content": "// TODO: support int64_t dims in ideep::tensor to avoid extra conversion",
        "context": "\nTensor new_with_itensor_mkldnn(ideep::tensor&& it, const TensorOptions& options) {\n  // NOTE: int32_t dims from ideep::tensor but sizes needs int64_t\n  // TODO: support int64_t dims in ideep::tensor to avoid extra conversion\n  auto dims = it.get_dims();\n  IDeepTensorWrapperPtr handle = c10::make_intrusive<IDeepTensorWrapper>(std::move(it));\n  return detail::make_tensor<MKLDNNTensorImpl>(\n",
        "description": "support int64_t dims in ideep::tensor to avoid extra conversion"
      },
      {
        "file": "atenspace/aten/src/ATen/native/mkldnn/MKLDNNConversions.cpp",
        "line": 31,
        "type": "TODO",
        "content": "// TODO: consider to convert non-contiguous tensor to `ideep::tensor` directly.",
        "context": "             \"dense_to_mkldnn expects float tensor input\");\n  AT_ASSERTM(cpu_tensor.dim() <= 5,\n             \"Can't convert cpu tensor with the number of dimensions > 5\");\n  // TODO: consider to convert non-contiguous tensor to `ideep::tensor` directly.\n  auto cpu_tensor_cont = cpu_tensor.contiguous();\n  Tensor mkldnn_tensor = empty_mkldnn(cpu_tensor_cont.sizes(), cpu_tensor_cont.options());\n  ideep::tensor& dtensor = itensor_from_mkldnn(mkldnn_tensor);\n",
        "description": "consider to convert non-contiguous tensor to `ideep::tensor` directly."
      },
      {
        "file": "atenspace/aten/src/ATen/native/miopen/Conv_miopen.cpp",
        "line": 5,
        "type": "TODO",
        "content": "// TODO: Remove the condition on AT_ROCM_ENABLED entirely,",
        "context": "#include <ATen/NativeFunctions.h>\n#include <ATen/Config.h>\n\n// TODO: Remove the condition on AT_ROCM_ENABLED entirely,\n// don't build this file as part of CPU build.\n#include <ATen/cuda/CUDAConfig.h>\n\n",
        "description": "Remove the condition on AT_ROCM_ENABLED entirely,"
      },
      {
        "file": "atenspace/aten/src/ATen/native/miopen/BatchNorm_miopen.cpp",
        "line": 5,
        "type": "TODO",
        "content": "// TODO: Remove the condition on AT_ROCM_ENABLED entirely,",
        "context": "#include <ATen/NativeFunctions.h>\n#include <ATen/Config.h>\n\n// TODO: Remove the condition on AT_ROCM_ENABLED entirely,\n// don't build this file as part of CPU build.\n#include <ATen/cuda/CUDAConfig.h>\n\n",
        "description": "Remove the condition on AT_ROCM_ENABLED entirely,"
      },
      {
        "file": "atenspace/aten/src/ATen/native/sparse/SparseTensor.cpp",
        "line": 381,
        "type": "TODO",
        "content": "// TODO: is there a more idiomatic way to do this?",
        "context": "\n  SparseTensor dst = new_sparse(self.options());\n  get_sparse_impl(dst)->resize_(sparse_dim, dense_dim, self.sizes());\n  // TODO: is there a more idiomatic way to do this?\n  LongTensor newIndices = at::empty(indices.sizes(), indices.options());\n  Tensor newValues = at::empty(values.sizes(), values.options());\n  alias_into_sparse(dst, newIndices, newValues);\n",
        "description": "is there a more idiomatic way to do this?"
      },
      {
        "file": "atenspace/aten/src/ATen/native/sparse/SparseTensor.cpp",
        "line": 501,
        "type": "TODO",
        "content": "// TODO: Re-audit this; it used to be an indexSelect directly into r_values",
        "context": "    }\n\n    Tensor t_view = t.view(view_size);\n    // TODO: Re-audit this; it used to be an indexSelect directly into r_values\n    at::index_select_out(r_values, t_view, 0, indices);\n  } else {\n    AT_DISPATCH_ALL_TYPES(r_values.scalar_type(), \"sparse_mask\", [&] {\n",
        "description": "Re-audit this; it used to be an indexSelect directly into r_values"
      },
      {
        "file": "atenspace/aten/src/ATen/native/sparse/SparseTensorMath.cpp",
        "line": 25,
        "type": "TODO",
        "content": "// TODO: eliminate this conditional when zero-size dims supported correctly",
        "context": "  LongTensor _to_csr(const int64_t* indices, int64_t dim, int64_t nnz) {\n    LongTensor csr = native::zeros({dim + 1}, kLong);\n\n    // TODO: eliminate this conditional when zero-size dims supported correctly\n    if (nnz > 0) {\n      auto csr_accessor = csr.accessor<int64_t, 1>();\n      // Convert the sparse matrix to CSR format\n",
        "description": "eliminate this conditional when zero-size dims supported correctly"
      },
      {
        "file": "atenspace/aten/src/ATen/native/sparse/SparseTensorMath.cpp",
        "line": 96,
        "type": "TODO",
        "content": "// TODO: add in-place variant",
        "context": "// log1p(SparseTensor)\n// --------------------------------------------------------------------\n\n// TODO: add in-place variant\n\nSparseTensor& log1p_out_sparse(SparseTensor& r, const SparseTensor& t) {\n  AT_ASSERT(r.is_sparse());\n",
        "description": "add in-place variant"
      },
      {
        "file": "atenspace/aten/src/ATen/native/sparse/SparseTensorMath.cpp",
        "line": 123,
        "type": "TODO",
        "content": "// TODO: add in-place variant",
        "context": "// pow(SparseTensor, Scalar)\n// --------------------------------------------------------------------\n\n// TODO: add in-place variant\n\nSparseTensor& pow_out_sparse_scalar(SparseTensor& r, const SparseTensor& t_, Scalar value) {\n  AT_ASSERT(r.is_sparse());\n",
        "description": "add in-place variant"
      },
      {
        "file": "atenspace/aten/src/ATen/native/sparse/SparseTensorMath.cpp",
        "line": 206,
        "type": "TODO",
        "content": "// TODO: Why?! Can't we just flip the order here...",
        "context": "// --------------------------------------------------------------------\n\nTensor add_sparse(const Tensor& self, const Tensor& other, Scalar alpha) {\n  // TODO: Why?! Can't we just flip the order here...\n  TORCH_CHECK(!(self.is_sparse() && !other.is_sparse()),\n              \"add(sparse, dense) is not supported. Use add(dense, sparse) instead.\");\n  Tensor result = at::empty({0}, self.options());\n",
        "description": "Why?! Can't we just flip the order here..."
      },
      {
        "file": "atenspace/aten/src/ATen/native/sparse/SparseTensorMath.cpp",
        "line": 330,
        "type": "TODO",
        "content": "// TODO: I think it may be possible to track inside the loop and",
        "context": "    );\n\n    get_sparse_impl(r)->set_nnz_and_narrow(r_i);\n    // TODO: I think it may be possible to track inside the loop and\n    // detect when we are uncoalesced (e.g., by observing that an\n    // index goes backwards) which may be more precise than using the\n    // coalesced flag here.  But this is easy.\n",
        "description": "I think it may be possible to track inside the loop and"
      },
      {
        "file": "atenspace/aten/src/ATen/native/sparse/SparseTensorMath.cpp",
        "line": 721,
        "type": "TODO",
        "content": "// TODO: Make this a real argument",
        "context": "// --------------------------------------------------------------------\n\nSparseTensor& hspmm_out_sparse_cpu(SparseTensor& r, const SparseTensor& sparse_, const Tensor& dense) {\n  // TODO: Make this a real argument\n  Scalar alpha = 1;\n\n  AT_ASSERT(!sparse_.is_cuda()); // dispatch argument\n",
        "description": "Make this a real argument"
      },
      {
        "file": "atenspace/aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cpp",
        "line": 50,
        "type": "TODO",
        "content": "// TODO: Re-audit this; it used to be an indexSelect directly into r_values",
        "context": "  }\n\n  Tensor t_view = t.view(view_size);\n  // TODO: Re-audit this; it used to be an indexSelect directly into r_values\n  at::index_select_out(r_values, t_view, 0, indices);\n\n  return r;\n",
        "description": "Re-audit this; it used to be an indexSelect directly into r_values"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp",
        "line": 41,
        "type": "TODO",
        "content": "// TODO This is an inefficient way to compite sign, and can be much faster",
        "context": "  // implementation of the general backward pass when p is less than two, so\n  // there's a struct with only a backward pass for this case.\n\n  // TODO This is an inefficient way to compite sign, and can be much faster\n  // using native SSE instructions that should be added to Vec256.\n  static inline Vec sign(Vec val) {\n    return vec256::minimum(vec256::maximum(Vec(0), val.ceil()), Vec(1)) +\n",
        "description": "This is an inefficient way to compite sign, and can be much faster"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp",
        "line": 113,
        "type": "TODO",
        "content": "// TODO This can probably use fused add multiply to get better perf",
        "context": "  // Two norm\n  template<typename data_t>\n  struct tdist_calc {\n    // TODO This can probably use fused add multiply to get better perf\n    static inline data_t map(const data_t& diff, const data_t& p) { return diff * diff; }\n    static inline data_t red(const data_t& agg, const data_t& up) { return agg + up; }\n    static inline scalar_t finish(const scalar_t agg, const scalar_t p) { return std::sqrt(agg); }\n",
        "description": "This can probably use fused add multiply to get better perf"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp",
        "line": 135,
        "type": "TODO",
        "content": "// TODO This backward pass uses a very complext expression to compute (diff",
        "context": "    static inline data_t map(const data_t& diff, const data_t& p) { return diff; }\n    static inline data_t red(const data_t& agg, const data_t& up) { return max(agg, up); }\n    static inline scalar_t finish(const scalar_t agg, const scalar_t p) { return agg; }\n    // TODO This backward pass uses a very complext expression to compute (diff\n    // == dist) that could be much faster if using SSE instructions.\n    static inline Vec backward(const Vec& diff, const scalar_t grad, const scalar_t dist, const Vec& p) { return Vec(grad) * sign(diff) * (Vec(1) - vec256::minimum(Vec(1), (diff.abs() - Vec(dist)).abs().ceil())); }\n  };\n",
        "description": "This backward pass uses a very complext expression to compute (diff"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cpu/IndexKernel.cpp",
        "line": 107,
        "type": "TODO",
        "content": "// TODO: investigate parallelization of the accumulate kernel. Unlike the non-accumulate case,",
        "context": "  // NOTE: duplicate indices are only supported if accumulate is true.\n  AT_DISPATCH_ALL_TYPES_AND2(at::ScalarType::Half, at::ScalarType::Bool, iter.dtype(), \"index_put\", [&] {\n    if (accumulate) {\n      // TODO: investigate parallelization of the accumulate kernel. Unlike the non-accumulate case,\n      // this needs to be thread-safe.\n      cpu_index_kernel<scalar_t>(iter, index_size, index_stride, [](char* dst, char* src, int64_t offset) {\n        *(scalar_t*)(dst + offset) += *(scalar_t*)src;\n",
        "description": "investigate parallelization of the accumulate kernel. Unlike the non-accumulate case,"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cpu/Activation.cpp",
        "line": 138,
        "type": "TODO",
        "content": "// TODO(yangxm): Add another fast kernel using formula",
        "context": "\n#endif // AT_MKL_ENABLED()\n\n// TODO(yangxm): Add another fast kernel using formula\n// y = 0.5x * (1 + tanh(sqrt(2/Pi) * (x + 0.044715x^3)))\n// and the fast tanh impl from Eigen.\nvoid GeluKernelImpl(TensorIterator& it) {\n",
        "description": "(yangxm): Add another fast kernel using formula"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp",
        "line": 325,
        "type": "TODO",
        "content": "// TODO: Disable cont. branch to test more risky code",
        "context": "  });\n}\n\n// TODO: Disable cont. branch to test more risky code\n\n#define IMPLEMENT_FLOAT_KERNEL(dispatchtypes, op)                             \\\n  static void op##_kernel(TensorIterator& iter) {                             \\\n",
        "description": "Disable cont. branch to test more risky code"
      },
      {
        "file": "atenspace/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp",
        "line": 67,
        "type": "TODO",
        "content": "// TODO: if the divisor is a scalar, rewrite as multiplication by a constant.",
        "context": "void div_kernel(TensorIterator& iter) {\n  if (isIntegralType(iter.dtype(), /*includeBool*/ false)) {\n    // There's no SIMD integer division, so don't try to vectorize it.\n    // TODO: if the divisor is a scalar, rewrite as multiplication by a constant.\n    AT_DISPATCH_INTEGRAL_TYPES(iter.dtype(), \"div_cpu\", [&]() {\n      cpu_kernel(iter, [](scalar_t a, scalar_t b) -> scalar_t {\n        return a / b;\n",
        "description": "if the divisor is a scalar, rewrite as multiplication by a constant."
      },
      {
        "file": "atenspace/aten/src/ATen/native/cpu/DepthwiseConvKernel.cpp",
        "line": 175,
        "type": "TODO",
        "content": "// TODO: avoid spilling W by breaking out the non-padded vs padded case.",
        "context": "  // Non-padded regime.\n\n  // Iterate over non-padded output tiles.\n  // TODO: avoid spilling W by breaking out the non-padded vs padded case.\n  for (int64_t oth = 0; oth < (args.out_rows + 1) / 2; ++oth) {\n    for (int64_t otw = 0; otw < (args.out_cols + 1) / 2; ++otw) {\n      // load input tile for [oth, otw];\n",
        "description": "avoid spilling W by breaking out the non-padded vs padded case."
      },
      {
        "file": "atenspace/aten/src/ATen/native/quantized/TensorCompare.cpp",
        "line": 21,
        "type": "TODO",
        "content": "// TODO: move to TensorMath.cpp",
        "context": "  return std::get<0>(self.reshape({-1}).min(/*dim=*/0));\n}\n\n// TODO: move to TensorMath.cpp\n\nstd::tuple<Tensor, Tensor> sort_quant(\n    const Tensor& self,\n",
        "description": "move to TensorMath.cpp"
      },
      {
        "file": "atenspace/aten/src/ATen/native/quantized/QTensor.cpp",
        "line": 159,
        "type": "TODO",
        "content": "// TODO: add per channel support",
        "context": "}\n\nTensor quantized_clone(const Tensor& self, c10::optional<c10::MemoryFormat> optional_memory_format) {\n  // TODO: add per channel support\n  TORCH_INTERNAL_ASSERT(\n      self.qscheme() == at::kPerTensorAffine,\n      \"clone for quantized Tensor only works for PerTensorAffine scheme right now\");\n",
        "description": "add per channel support"
      },
      {
        "file": "atenspace/aten/src/ATen/native/quantized/QTensor.cpp",
        "line": 167,
        "type": "TODO",
        "content": "// TODO: To support all features of MemoryFormat::Preserve we need to add",
        "context": "  auto memory_format =\n      optional_memory_format.value_or(MemoryFormat::Contiguous);\n\n  // TODO: To support all features of MemoryFormat::Preserve we need to add\n  // _empty_affine_quantized_strided function and use it similarly to\n  // Tensor clone(const Tensor& src, c10::optional<c10::MemoryFormat> optional_memory_format)\n  // if (self.is_non_overlapping_and_dense()) -> _empty_affine_quantized_strided\n",
        "description": "To support all features of MemoryFormat::Preserve we need to add"
      },
      {
        "file": "atenspace/aten/src/ATen/native/quantized/cpu/qpool.cpp",
        "line": 305,
        "type": "TODO",
        "content": "// TODO: change it to contiguous(MemoryFormat::ChannelsLast) once a perf",
        "context": "     int64_t inC = input.size(1);\n     int64_t inH = input.size(2);\n     int64_t inW = input.size(3);\n     // TODO: change it to contiguous(MemoryFormat::ChannelsLast) once a perf\n     // regression of it is fixed.\n     Tensor input_contig = input.permute({0, 2, 3, 1}).contiguous();\n\n",
        "description": "change it to contiguous(MemoryFormat::ChannelsLast) once a perf"
      },
      {
        "file": "atenspace/aten/src/ATen/native/quantized/cpu/qpool.cpp",
        "line": 392,
        "type": "TODO",
        "content": "//TODO: remove permute once MemoryLayout is added above",
        "context": "     TORCH_INTERNAL_ASSERT(\n         runStatus == pytorch_qnnp_status_success,\n         \"failed to run QNNPACK MaxPool operator\");\n     //TODO: remove permute once MemoryLayout is added above\n     return qy.permute({0, 3, 1, 2});\n   }\n   #endif\n",
        "description": "remove permute once MemoryLayout is added above"
      },
      {
        "file": "atenspace/aten/src/ATen/native/quantized/cpu/qlinear_prepack.cpp",
        "line": 63,
        "type": "TODO",
        "content": "// TODO: contiguous is called for further JIT optimizations.",
        "context": "    auto N = weight.size(0);\n    auto K = weight.size(1);\n\n    // TODO: contiguous is called for further JIT optimizations.\n    auto weight_contig = weight.contiguous();\n    const auto qtype = weight.qscheme();\n    std::vector<int32_t> weight_zero_points_int32(1, 0);\n",
        "description": "contiguous is called for further JIT optimizations."
      },
      {
        "file": "atenspace/aten/src/ATen/native/quantized/cpu/qlinear_prepack.cpp",
        "line": 122,
        "type": "TODO",
        "content": "// TODO: we will need to replace this with torchscript classes at a later",
        "context": "        weight_zero_points_int32,\n        qtype});\n\n    // TODO: we will need to replace this with torchscript classes at a later\n    // point.\n    return cpp_custom_type_hack::create(std::move(ret_ptr), weight.options());\n  }\n",
        "description": "we will need to replace this with torchscript classes at a later"
      },
      {
        "file": "atenspace/aten/src/ATen/native/quantized/cpu/q_adaavgpool.cpp",
        "line": 82,
        "type": "TODO",
        "content": "// TODO: add the max/min clip",
        "context": "          }\n\n          /* set output to local average */\n          // TODO: add the max/min clip\n          op->val_ = static_cast<typename scalar_t::underlying>(\n              std::nearbyint(sum * kHWr));\n        }\n",
        "description": "add the max/min clip"
      },
      {
        "file": "atenspace/aten/src/ATen/native/quantized/cpu/fake_quantize_per_channel_affine.cpp",
        "line": 32,
        "type": "TODO",
        "content": "// TODO: Use REGISTER_DISPATCH",
        "context": "    int64_t axis,\n    int64_t quant_min,\n    int64_t quant_max) {\n  // TODO: Use REGISTER_DISPATCH\n  TORCH_CHECK(self.scalar_type() == ScalarType::Float);\n  TORCH_CHECK(\n      scale.dim() == 1, \"scale should be a 1-D tensor\");\n",
        "description": "Use REGISTER_DISPATCH"
      },
      {
        "file": "atenspace/aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp",
        "line": 141,
        "type": "TODO",
        "content": "// TODO: Change this when ChannelsLast3d is ready.",
        "context": "    }\n\n    // FBGEMM expects weights to be in channels last\n    // TODO: Change this when ChannelsLast3d is ready.\n    const Tensor weight_nhwc = kSpatialDim == 2\n        ? weight.contiguous(MemoryFormat::ChannelsLast)\n        : fbgemm_utils::ConvertToChannelsLast3dTensor(weight);\n",
        "description": "Change this when ChannelsLast3d is ready."
      },
      {
        "file": "atenspace/aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp",
        "line": 203,
        "type": "TODO",
        "content": "// TODO: we will need to replace this with torchscript classes at a later",
        "context": "            zero_points,\n            qtype});\n\n    // TODO: we will need to replace this with torchscript classes at a later\n    // point.\n    return cpp_custom_type_hack::create(std::move(ret_ptr), weight.options());\n  }\n",
        "description": "we will need to replace this with torchscript classes at a later"
      },
      {
        "file": "atenspace/aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp",
        "line": 28,
        "type": "TODO",
        "content": "// TODO: contiguous is called for further jit optimizations.",
        "context": "    TORCH_CHECK(\n        fbgemm::fbgemmSupportedCPU(), \"Your CPU does not support FBGEMM.\");\n\n    // TODO: contiguous is called for further jit optimizations.\n    auto input_contig = input.contiguous();\n    const auto* input_ptr = input_contig.data_ptr<float>();\n\n",
        "description": "contiguous is called for further jit optimizations."
      },
      {
        "file": "atenspace/aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp",
        "line": 91,
        "type": "TODO",
        "content": "// TODO: contiguous is called for further jit optimizations.",
        "context": "      TORCH_CHECK(\n          bias_vec.size(0) == N,\n          \"bias should have N elements: \" + std::to_string(N));\n      // TODO: contiguous is called for further jit optimizations.\n      auto bias_contig = bias_vec.contiguous();\n      bias_ptr = bias_contig.data_ptr<float>();\n    }\n",
        "description": "contiguous is called for further jit optimizations."
      },
      {
        "file": "atenspace/aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp",
        "line": 130,
        "type": "TODO",
        "content": "// TODO: Consider a way to pre-allocate and reuse",
        "context": "          /*pmat=*/nullptr, // Currently, packA manages ownership of `pmat`.\n          /*scale=*/q_params.scale,\n          /*zero_pt=*/q_params.zero_point);\n      // TODO: Consider a way to pre-allocate and reuse\n      // pmat buffer.\n\n      // This is the end of the pipeline, pass the resulting matrix through.\n",
        "description": "Consider a way to pre-allocate and reuse"
      },
      {
        "file": "atenspace/aten/src/ATen/native/quantized/cpu/qconv.cpp",
        "line": 523,
        "type": "TODO",
        "content": "// TODO Can be replaced with packB->getOutputChannels() when update pre-pack",
        "context": "\n    const uint32_t kernel_h = kernel[0];\n    const uint32_t kernel_w = kernel[1];\n    // TODO Can be replaced with packB->getOutputChannels() when update pre-pack\n    // to actually do the packing.\n    const auto out_ch = pack_data.bias.size(0);\n    // inputs are in semantic NCHW format\n",
        "description": "Can be replaced with packB->getOutputChannels() when update pre-pack"
      },
      {
        "file": "atenspace/aten/src/ATen/native/quantized/cpu/qlinear.cpp",
        "line": 33,
        "type": "TODO",
        "content": "// TODO: contiguous is called for further jit optimizations.",
        "context": "    TORCH_CHECK(\n        fbgemm::fbgemmSupportedCPU(), \"Your CPU does not support FBGEMM.\");\n\n    // TODO: contiguous is called for further jit optimizations.\n    auto input_contig = input.contiguous();\n    const auto* input_ptr =\n        reinterpret_cast<uint8_t*>(input_contig.data_ptr<c10::quint8>());\n",
        "description": "contiguous is called for further jit optimizations."
      },
      {
        "file": "atenspace/aten/src/ATen/native/quantized/cpu/qlinear.cpp",
        "line": 131,
        "type": "TODO",
        "content": "// TODO: Consider a way to pre-allocate and reuse",
        "context": "            /*smat=*/input_ptr,\n            /*ld=*/K,\n            /*pmat=*/nullptr); // Currently, packA manages ownership of `pmat`.\n                               // TODO: Consider a way to pre-allocate and reuse\n                               // pmat buffer.\n\n        // ReQuantizeOutput requires pointers to the zero point values,\n",
        "description": "Consider a way to pre-allocate and reuse"
      },
      {
        "file": "atenspace/aten/src/ATen/native/quantized/cpu/q_avgpool.cpp",
        "line": 379,
        "type": "TODO",
        "content": "// TODO: remove permute once MemoryLayout is added above",
        "context": "  TORCH_INTERNAL_ASSERT(\n      runStatus == pytorch_qnnp_status_success,\n      \"failed to run QNNPACK Average Pool operator\");\n  // TODO: remove permute once MemoryLayout is added above\n  return output.permute({0, 3, 1, 2});\n}\n#endif\n",
        "description": "remove permute once MemoryLayout is added above"
      },
      {
        "file": "atenspace/aten/src/ATen/native/quantized/cpu/qconv_unpack.cpp",
        "line": 76,
        "type": "TODO",
        "content": "// TODO: Unify 2d and 3d when ChannelsLast3d is ready.",
        "context": "    // Unpacked format would be physical KRS(C/G) but logical KCRS (channels\n    // first) because that's how\n    // ChannelsLast3d is not available now.FBGEMM stores the weights\n    // TODO: Unify 2d and 3d when ChannelsLast3d is ready.\n    Tensor unpacked_weights;\n    if (pack_ptr.q_scheme == kPerTensorAffine) {\n      unpacked_weights = kSpatialDim == 2\n",
        "description": "Unify 2d and 3d when ChannelsLast3d is ready."
      },
      {
        "file": "atenspace/aten/src/ATen/native/quantized/cpu/fbgemm_utils.h",
        "line": 76,
        "type": "TODO",
        "content": "// TODO: Remove functions below when ChannelsLast3d is ready.",
        "context": "    const std::vector<int>& pads,\n    const std::vector<int>& dilations);\n\n// TODO: Remove functions below when ChannelsLast3d is ready.\nTensor MakeStridedQTensorCPU(\n    const IntArrayRef& sizes,\n    const IntArrayRef& strides,\n",
        "description": "Remove functions below when ChannelsLast3d is ready."
      },
      {
        "file": "atenspace/aten/src/ATen/native/quantized/cpu/qreduction.cpp",
        "line": 29,
        "type": "TODO",
        "content": "// TODO: change it to contiguous(MemoryFormat::ChannelsLast) once a perf",
        "context": "  const int64_t inH = input.size(2);\n  const int64_t inW = input.size(3);\n\n  // TODO: change it to contiguous(MemoryFormat::ChannelsLast) once a perf\n  // regression of it is fixed. Today it's equivalent because `input` sizes\n  // are not used below\n  Tensor input_contig = input.permute({0, 2, 3, 1}).contiguous();\n",
        "description": "change it to contiguous(MemoryFormat::ChannelsLast) once a perf"
      },
      {
        "file": "atenspace/aten/src/ATen/native/quantized/cpu/qnnpack/src/conv-run.cc",
        "line": 350,
        "type": "FIXME",
        "content": "// FIXME temporary solution to create a qnnp_op struct for indirection buffer.",
        "context": "      stride_width);\n  const size_t output_size = output_height * output_width;\n\n  // FIXME temporary solution to create a qnnp_op struct for indirection buffer.\n  const bool any_padding =\n      (conv_p.pad[0] | conv_p.pad[1] | conv_p.pad[2] | conv_p.pad[3]) != 0;\n  size_t zero_size = 0, zero_offset = 0;\n",
        "description": "temporary solution to create a qnnp_op struct for indirection buffer."
      },
      {
        "file": "atenspace/aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp",
        "line": 247,
        "type": "TODO",
        "content": "// TODO: fbgemm::Quantize doesn't support taking in the",
        "context": "            }\n            retvals[i] = c;\n          }\n          // TODO: fbgemm::Quantize doesn't support taking in the\n          // pre-broadcasted parameters. We might be able to save some cycles by\n          // enabling that in the API.\n          // TODO: specialize fbgemm::Quantize for a single vector and make it\n",
        "description": "fbgemm::Quantize doesn't support taking in the"
      },
      {
        "file": "atenspace/aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp",
        "line": 250,
        "type": "TODO",
        "content": "// TODO: specialize fbgemm::Quantize for a single vector and make it",
        "context": "          // TODO: fbgemm::Quantize doesn't support taking in the\n          // pre-broadcasted parameters. We might be able to save some cycles by\n          // enabling that in the API.\n          // TODO: specialize fbgemm::Quantize for a single vector and make it\n          // inlineable. This could help with interleaving as suggested by the\n          // TensorIterator implementations\n          auto rv = Vec::quantize(retvals, scale, zero_point, inv_scale);\n",
        "description": "specialize fbgemm::Quantize for a single vector and make it"
      },
      {
        "file": "atenspace/aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp",
        "line": 444,
        "type": "TODO",
        "content": "// TODO: support 16bit, 32bit, and etc.",
        "context": "        int64_t c = 0;\n        // For int8 or uint8quantization, we implicitly use int32 as\n        // accumulation Or else, it will go to the slow path\n        // TODO: support 16bit, 32bit, and etc.\n        auto* internal_i_p = i_p + istartH * istrideH + istartW * istrideW;\n\n        // TODO: more vectorization with loop interleaving\n",
        "description": "support 16bit, 32bit, and etc."
      },
      {
        "file": "atenspace/aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp",
        "line": 447,
        "type": "TODO",
        "content": "// TODO: more vectorization with loop interleaving",
        "context": "        // TODO: support 16bit, 32bit, and etc.\n        auto* internal_i_p = i_p + istartH * istrideH + istartW * istrideW;\n\n        // TODO: more vectorization with loop interleaving\n        do_avg_pool_on_AVX2<scalar_t>(\n            internal_i_p,\n            o_p,\n",
        "description": "more vectorization with loop interleaving"
      },
      {
        "file": "atenspace/aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp",
        "line": 542,
        "type": "TODO",
        "content": "// TODO: support 16bit, 32bit, and etc.",
        "context": "        int64_t c = 0;\n        // For int8 quantization, we implicitly use int32 as accumulation\n        // Or else, it will go to the slow path\n        // TODO: support 16bit, 32bit, and etc.\n        float multiplier = qx.q_scale() / qy.q_scale() / divide_factor;\n        do_avg_pool_on_AVX2<scalar_t>(\n            i_p,\n",
        "description": "support 16bit, 32bit, and etc."
      },
      {
        "file": "atenspace/aten/src/ATen/templates/OpsAlreadyMovedToC10.cpp",
        "line": 12,
        "type": "TODO",
        "content": "// TODO Once all ATen ops are moved to c10, this file should be removed",
        "context": "\n// ${generated_comment}\n\n// TODO Once all ATen ops are moved to c10, this file should be removed\n\nnamespace at {\n\n",
        "description": "Once all ATen ops are moved to c10, this file should be removed"
      },
      {
        "file": "atenspace/aten/src/ATen/templates/TensorMethods.h",
        "line": 34,
        "type": "TODO",
        "content": "// TODO: The Python version also accepts arguments",
        "context": "  return to(options().device(DeviceType::CPU), /*non_blocking*/ false, /*copy*/ false);\n}\n\n// TODO: The Python version also accepts arguments\ninline Tensor Tensor::cuda() const {\n  return to(options().device(DeviceType::CUDA), /*non_blocking*/ false, /*copy*/ false);\n}\n",
        "description": "The Python version also accepts arguments"
      },
      {
        "file": "atenspace/aten/src/ATen/templates/TensorMethods.h",
        "line": 47,
        "type": "TODO",
        "content": "// TODO: Deprecate me",
        "context": "  return to(options().dtype(t), /*non_blocking*/ false, /*copy*/ false);\n}\n\n// TODO: Deprecate me\ninline Tensor Tensor::toBackend(Backend b) const {\n  return to(options().device(backendToDeviceType(b)).layout(layout_from_backend(b)), /*non_blocking*/ false, /*copy*/ false);\n}\n",
        "description": "Deprecate me"
      },
      {
        "file": "atenspace/aten/src/ATen/templates/TensorBody.h",
        "line": 291,
        "type": "TODO",
        "content": "/// TODO: it's not in native_functions.yaml yet as it's not exposed to python",
        "context": "  bool is_quantized() const;\n\n  /// If a tensor is a quantized tensor, returns its quantizer\n  /// TODO: it's not in native_functions.yaml yet as it's not exposed to python\n  QuantizerPtr quantizer() const;\n\n#ifdef BUILD_NAMEDTENSOR\n",
        "description": "it's not in native_functions.yaml yet as it's not exposed to python"
      },
      {
        "file": "atenspace/aten/src/ATen/templates/TensorBody.h",
        "line": 419,
        "type": "TODO",
        "content": "// TODO: remove following two after at::kDouble and its friends are TypeMeta's.",
        "context": "  // at::kDouble and its friends to be TypeMeta's, but that hasn't happened yet.\n  // Before that change, we make this method to maintain BC for C++ usage like\n  // `x.to(y.dtype)`.\n  // TODO: remove following two after at::kDouble and its friends are TypeMeta's.\n  inline Tensor to(caffe2::TypeMeta type_meta, bool non_blocking=false, bool copy=false) const {\n    return this->to(/*scalar_type=*/typeMetaToScalarType(type_meta), non_blocking, copy);\n  }\n",
        "description": "remove following two after at::kDouble and its friends are TypeMeta's."
      },
      {
        "file": "atenspace/aten/src/ATen/quantized/Quantizer.h",
        "line": 14,
        "type": "TODO",
        "content": "// TODO: move to c10 namespace after we",
        "context": "#include <cmath>\n#include <memory>\n\n// TODO: move to c10 namespace after we\n// unified caffe2::Tensor and at::Tensor\n\nnamespace at {\n",
        "description": "move to c10 namespace after we"
      },
      {
        "file": "atenspace/aten/src/ATen/quantized/Quantizer.cpp",
        "line": 203,
        "type": "TODO",
        "content": "// TODO combine this with quantize_val once the numerics for ARM are aligned with it",
        "context": "  }\n}\n\n// TODO combine this with quantize_val once the numerics for ARM are aligned with it\ninline uint8_t quantize_val_arm(const float scale, const int32_t zero_point, const float value) {\n  const int32_t qmin = std::numeric_limits<uint8_t>::min();\n  const int32_t qmax = std::numeric_limits<uint8_t>::max();\n",
        "description": "combine this with quantize_val once the numerics for ARM are aligned with it"
      },
      {
        "file": "atenspace/aten/src/ATen/quantized/Quantizer.cpp",
        "line": 232,
        "type": "TODO",
        "content": "// TODO Make quantize_tensor_arm work for other datatypes too (int8, int32).",
        "context": "// There may be slight accuracy difference between this and implementation of quantize_val\n// TODO Update quantize_tensor_arm implementation to follow quantize_val,\n// i.e. f = Round(value/scale + zero_point)\n// TODO Make quantize_tensor_arm work for other datatypes too (int8, int32).\ntemplate <>\nvoid quantize_tensor_arm<c10::quint8>(\n    const float* in,\n",
        "description": "Make quantize_tensor_arm work for other datatypes too (int8, int32)."
      },
      {
        "file": "atenspace/aten/src/ATen/quantized/Quantizer.cpp",
        "line": 363,
        "type": "TODO",
        "content": "// TODO: add fbgemm for per channel",
        "context": "template CAFFE2_API quint8 requantize_val<qint32, quint8>(double, int64_t, double, int64_t, qint32);\ntemplate CAFFE2_API qint32 requantize_val<qint32, qint32>(double, int64_t, double, int64_t, qint32);\n\n// TODO: add fbgemm for per channel\ntemplate <typename T>\nTensor quantize_tensor_per_channel_affine(Tensor rtensor,\n                                          Tensor qtensor,\n",
        "description": "add fbgemm for per channel"
      },
      {
        "file": "atenspace/aten/src/ATen/quantized/QTensorImpl.h",
        "line": 23,
        "type": "TODO",
        "content": "// TODO: Expose in PyTorch Frontend",
        "context": "      TensorTypeSet type_set,\n      QuantizerPtr quantizer);\n\n  // TODO: Expose in PyTorch Frontend\n  QuantizerPtr quantizer() {\n    return quantizer_;\n  }\n",
        "description": "Expose in PyTorch Frontend"
      },
      {
        "file": "atenspace/aten/src/TH/vector/VSX.cpp",
        "line": 1345,
        "type": "TODO",
        "content": "//        TODO",
        "context": "//    $ gcc VSX.c -O2 -D RUN_VSX_TESTS -o vsxtest\n//    $ ./vsxtest\n//\n//        TODO\n//\n//\n//    Finished running all tests. All tests PASSED.\n",
        "description": ""
      },
      {
        "file": "atenspace/aten/src/TH/generic/THTensor.cpp",
        "line": 622,
        "type": "FIXME",
        "content": "// FIXME: warn if this is the case",
        "context": "  // to cat empty tensors unless all the other tensors were 1-dimensional, so we allowed these tensors\n  // to be \"skipped\".  We maintain this behavior for backwards compatibility, but only for this specific\n  // size (i.e. other empty sizes are not skipped).\n  // FIXME: warn if this is the case\n  bool allSkipped= true;\n  int64_t nDims = 0;\n  THTensor *notSkippedTensor;  // non-owning reference\n",
        "description": "warn if this is the case"
      },
      {
        "file": "atenspace/aten/src/THC/THCGeneral.cpp",
        "line": 175,
        "type": "TODO",
        "content": "// TODO: delete me",
        "context": "  return &(state->resourcesPerDevice[device]);\n}\n\n// TODO: delete me\ncudaStream_t THCState_getCurrentStreamOnDevice(THCState *state, int device) {\n  return at::cuda::getCurrentCUDAStream(device).stream();\n}\n",
        "description": "delete me"
      },
      {
        "file": "atenspace/aten/src/THC/THCGeneral.cpp",
        "line": 180,
        "type": "TODO",
        "content": "// TODO: delete me",
        "context": "  return at::cuda::getCurrentCUDAStream(device).stream();\n}\n\n// TODO: delete me\ncudaStream_t THCState_getCurrentStream(THCState *state) {\n  return at::cuda::getCurrentCUDAStream().stream();\n}\n",
        "description": "delete me"
      },
      {
        "file": "atenspace/aten/src/THC/THCGeneral.hpp",
        "line": 16,
        "type": "TODO",
        "content": "// TODO: Make this statically obvious",
        "context": "  // NB: These allocators (specifically, cudaHostAllocator) MUST implement\n  // maybeGlobalBoundDeleter, because we have a few use-cases where we need to\n  // do raw allocations with them (for Thrust).\n  // TODO: Make this statically obvious\n  at::Allocator* cudaHostAllocator;\n  at::Allocator* cudaDeviceAllocator;\n\n",
        "description": "Make this statically obvious"
      },
      {
        "file": "atenspace/aten/src/THC/generic/THCTensorMasked.h",
        "line": 16,
        "type": "FIXME",
        "content": "// FIXME: remove now that we have THCudaByteTensor?",
        "context": "                                        THCudaBoolTensor *mask,\n                                        scalar_t value);\n\n// FIXME: remove now that we have THCudaByteTensor?\nTHC_API void THCTensor_(maskedFillByte)(THCState *state,\n                                        THCTensor *tensor,\n                                        THByteTensor *mask,\n",
        "description": "remove now that we have THCudaByteTensor?"
      },
      {
        "file": "atenspace/aten/src/THC/generic/THCTensorMasked.h",
        "line": 32,
        "type": "FIXME",
        "content": "// FIXME: remove now that we have THCudaByteTensor?",
        "context": "                                        THCudaBoolTensor *mask,\n                                        THCTensor *src);\n\n// FIXME: remove now that we have THCudaByteTensor?\nTHC_API void THCTensor_(maskedCopyByte)(THCState *state,\n                                        THCTensor *tensor,\n                                        THByteTensor *mask,\n",
        "description": "remove now that we have THCudaByteTensor?"
      },
      {
        "file": "atenspace/aten/src/THC/generic/THCTensorMasked.h",
        "line": 48,
        "type": "FIXME",
        "content": "// FIXME: remove now that we have THCudaByteTensor?",
        "context": "                                          THCTensor *src,\n                                          THCudaBoolTensor *mask);\n\n// FIXME: remove now that we have THCudaByteTensor?\nTHC_API void THCTensor_(maskedSelectByte)(THCState *state,\n                                          THCTensor *tensor,\n                                          THCTensor *src,\n",
        "description": "remove now that we have THCudaByteTensor?"
      },
      {
        "file": "atomspace-restful/lib/zmq/zhelpers.hpp",
        "line": 31,
        "type": "TODO",
        "content": "// todo: package updated zmq.hpp",
        "context": "\n#include <zmq.hpp>\n//#include <lib/zmq/zmq.hpp>\n// todo: package updated zmq.hpp\n\n#include <iostream>\n#include <iomanip>\n",
        "description": "package updated zmq.hpp"
      },
      {
        "file": "atomspace-restful/opencog/python/web/api/utilities.py",
        "line": 17,
        "type": "FIXME",
        "content": "# FIXME: Should this moved to the atomspace repo and be part",
        "context": "# https://github.com/opencog/opencog/pull/2012 and,\n# https://github.com/opencog/atomspace/pull/611\n# NOTE: This is similar to scheme `cog-node`.\n# FIXME: Should this moved to the atomspace repo and be part\n# of opencog.atomspace module?\ndef get_atoms_by_name(z_type, name, atomspace):\n    return filter(lambda x: x.name == name, atomspace.get_atoms_by_type(z_type))\n",
        "description": "Should this moved to the atomspace repo and be part"
      }
    ],
    "low_priority": [
      {
        "file": "language-learning/tests/test_grammar_learner.py",
        "line": 212,
        "type": "TODO",
        "content": "# TODO: Remove these commented lines in next cleanup cycle.",
        "context": "        }\n        re = learn_grammar(**kwargs)\n        # NOTE: Legacy test code commented out for historical reference.\n        # TODO: Remove these commented lines in next cleanup cycle.\n        # a, q, qa = pqa_meter(re['grammar_file'], outpath, cp, rp, **kwargs)\n        # print('parse-ability, parse-quality:', a, q)\n        # assert a*q > 0.99\n",
        "description": "Remove these commented lines in next cleanup cycle."
      },
      {
        "file": "language-learning/src/grammar_learner/preprocessing.py",
        "line": 33,
        "type": "TODO",
        "content": "# TODO: cleanup here or in a separate constructor?",
        "context": "            if us[-1] != '\\n' :  us += '\\n'\n        us += s\n        if us[-1] != '\\n' :  us += '\\n'\n    # TODO: cleanup here or in a separate constructor?\n    re = OrderedDict([('read_files', UTC()),\n                      ('input_path', kwargs['input_path']),\n                      ('read_files_number', len(files)),\n",
        "description": "cleanup here or in a separate constructor?"
      },
      {
        "file": "language-learning/src/grammar_learner/hyperwords.py",
        "line": 466,
        "type": "TODO",
        "content": "# TODO: refactor, control disk writes, ... PPMI \u21d2 +frequency?",
        "context": "# Notes:\n\n# 80329 added vector_space_dim\n# TODO: refactor, control disk writes, ... PPMI \u21d2 +frequency?\n# 90221 minor updates for Grammar Learner tutorial\n",
        "description": "refactor, control disk writes, ... PPMI \u21d2 +frequency?"
      },
      {
        "file": "scripts/implementation/print_resolution_card.py",
        "line": 108,
        "type": "TODO",
        "content": "// TODO: document this          /**",
        "context": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n  BEFORE                           AFTER\n  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  // TODO: document this          /**\n  void func() { ... }              * @brief Clear description\n                                   * @param x The input\n                                   * @return The result\n",
        "description": "document this          /**"
      },
      {
        "file": "moses/moses/moses/moses/mpi_moses.cc",
        "line": 486,
        "type": "TODO",
        "content": "// TODO: Optimize statistics printing frequency to reduce output volume.",
        "context": "                thread_count--;\n                });\n\n// TODO: Optimize statistics printing frequency to reduce output volume.\n        // Consider printing detailed stats every N iterations instead of every iteration.\n        // Print stats in a way that makes them easy to graph.\n        // (columns of tab-seprated numbers)\n",
        "description": "Optimize statistics printing frequency to reduce output volume."
      },
      {
        "file": "moses/moses/comboreduct/table/table_io.cc",
        "line": 934,
        "type": "TODO",
        "content": "// TODO could be simplified, optimized, etc",
        "context": "            // It is sparse\n            is_sparse = is_sparse || string::npos != line.find(sparse_delim);\n            if (is_sparse) { // just get out\n                // TODO could be simplified, optimized, etc\n                in.seekg(beg);\n                in.clear();         // in case it has reached the eof\n                return in;\n",
        "description": "could be simplified, optimized, etc"
      },
      {
        "file": "components/learning/moses/moses/moses/moses/mpi_moses.cc",
        "line": 482,
        "type": "TODO",
        "content": "// TODO: Optimize statistics printing frequency to reduce output volume.",
        "context": "                thread_count--;\n                });\n\n// TODO: Optimize statistics printing frequency to reduce output volume.\n        // Consider printing detailed stats every N iterations instead of every iteration.\n        // Print stats in a way that makes them easy to graph.\n        // (columns of tab-seprated numbers)\n",
        "description": "Optimize statistics printing frequency to reduce output volume."
      },
      {
        "file": "components/learning/moses/moses/comboreduct/table/table_io.cc",
        "line": 955,
        "type": "TODO",
        "content": "// TODO could be simplified, optimized, etc",
        "context": "            // It is sparse\n            is_sparse = is_sparse || string::npos != line.find(sparse_delim);\n            if (is_sparse) { // just get out\n                // TODO could be simplified, optimized, etc\n                in.seekg(beg);\n                in.clear();         // in case it has reached the eof\n                return in;\n",
        "description": "could be simplified, optimized, etc"
      },
      {
        "file": "components/learning/moses/moses/comboreduct/table/table_io.cc",
        "line": 1078,
        "type": "TODO",
        "content": "// TODO: this could definitely be optimized",
        "context": "\n    if (is_sparse) {\n        // fallback on the old loader\n        // TODO: this could definitely be optimized\n        OC_ASSERT(timestamp_feature.empty(), \"Timestamp feature not implemented\");\n        return istreamTable_OLD(in, tab, target_feature, ignore_features);\n    } else {\n",
        "description": "this could definitely be optimized"
      },
      {
        "file": "atomspace-storage/opencog/persist/sexcom/Commands.cc",
        "line": 165,
        "type": "FIXME",
        "content": "// FIXME read above comment.",
        "context": "// (cog-execute-cache! (GetLink ...) (Predicate \"key\") ...)\n// This is complicated, and subject to change...\n// XXX this should be nuked, and replaced by appropriate kind of proxy.\n// FIXME read above comment.\nstd::string Commands::cog_execute_cache(const std::string& cmd)\n{\n\tsize_t pos = 0;\n",
        "description": "read above comment."
      },
      {
        "file": "ure/opencog/ure/Rule.cc",
        "line": 58,
        "type": "TODO",
        "content": "// TODO: could certainly be optimized by not systematically",
        "context": "\nvoid RuleSet::expand_meta_rules(AtomSpace& as)\n{\n\t// TODO: could certainly be optimized by not systematically\n\t// recollecting and re-instantiating meta-rules.\n\tRuleSet meta_rules;\n\tfor (RulePtr rule : *this) {\n",
        "description": "could certainly be optimized by not systematically"
      },
      {
        "file": "ure/opencog/ure/backwardchainer/BIT.cc",
        "line": 121,
        "type": "TODO",
        "content": "set_leaf2bitnode();         // TODO: might differ till needed to optimize",
        "context": "AndBIT::AndBIT(const Handle& f, double cpx, const AtomSpace* qas)\n\t: fcs(f), complexity(cpx), exhausted(false), queried_as(qas)\n{\n\tset_leaf2bitnode();         // TODO: might differ till needed to optimize\n}\n\nAndBIT::~AndBIT() {}\n",
        "description": "might differ till needed to optimize"
      },
      {
        "file": "atenspace/aten/src/ATen/native/TensorIterator.cpp",
        "line": 117,
        "type": "TODO",
        "content": "//TODO refactor so that no tensor copies are done",
        "context": "  if (all_same_type) {\n    return std::make_tuple(device, common_type, true);\n  }\n  //TODO refactor so that no tensor copies are done\n  std::vector<Tensor> tensors;\n  std::transform(std::begin(operands), std::end(operands), std::back_inserter(tensors),\n                  [](const OperandInfo& op) { return op.tensor; });\n",
        "description": "refactor so that no tensor copies are done"
      },
      {
        "file": "atenspace/aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp",
        "line": 209,
        "type": "TODO",
        "content": "// TODO: we can optimize dequantization by doing a premultiplication",
        "context": "\n  // Broadcast out the parameters here to amortize out that cost across\n  // loop iterations.\n  // TODO: we can optimize dequantization by doing a premultiplication\n  // of the zero point by scale and doing FMA on scale*x_q - (scale*zero_point)\n  auto self_zero_point_vec = Vec256<float>((float)self_zero_point);\n  auto self_scale_vec = Vec256<float>(self_scale);\n",
        "description": "we can optimize dequantization by doing a premultiplication"
      }
    ],
    "informational": [
      {
        "file": "components/integration/opencog/opencog/openpsi/OpenPsiRules.cc",
        "line": 69,
        "type": "TODO",
        "content": "// TODO: Test thoroughly, or develop an alternative. See discussion",
        "context": "      _psi_rules[rule] = std::make_tuple(context, action, goal, query);\n  } else {\n    // This is for backward compatability.\n    // TODO: Test thoroughly, or develop an alternative. See discussion\n    // @ https://github.com/opencog/opencog/pull/2899 for what the\n    // alternative might be.\n\n",
        "description": "Test thoroughly, or develop an alternative. See discussion"
      },
      {
        "file": "components/learning/moses/examples/example-progs/continmax.cc",
        "line": 67,
        "type": "TODO",
        "content": "// TODO: Expand documentation to match style of onemax/nmax examples.",
        "context": "// -- the number that is -log_2(epsilon) where epsilon is the smallest\n//    distinction between continuous variables what will be drawn.\n//\n// TODO: Expand documentation to match style of onemax/nmax examples.\n// Add parameter descriptions, usage examples, and expected output.\n\nint main(int argc, char** argv)\n",
        "description": "Expand documentation to match style of onemax/nmax examples."
      },
      {
        "file": "atomspace/opencog/query/RewriteMixin.cc",
        "line": 162,
        "type": "FIXME",
        "content": "/// XXX FIXME now I see how it can be done. The groupings should",
        "context": "/// to dribble in. Perhaps the engine search could be modified in some\n/// clever way to find groupings in a single batch; but for now, I don't\n/// see how this could be done.\n/// XXX FIXME now I see how it can be done. The groupings should\n/// be converted to marginals, and handled the same way. So this\n/// needs a rewrite. Good thing that almost no one uses this ...\nbool RewriteMixin::propose_grouping(const GroundingMap &var_soln,\n",
        "description": "now I see how it can be done. The groupings should"
      },
      {
        "file": "atenspace/aten/src/ATen/native/Distributions.cpp",
        "line": 125,
        "type": "TODO",
        "content": "// TODO: Fix resize_as_. See pytorch/pytorch#11665.",
        "context": "Tensor& bernoulli_out(Tensor& result, const Tensor& self, Generator* gen) {\n  // result.resize_as_(self) requires self to have same dtype as result, so we\n  // use resize_ instead.\n  // TODO: Fix resize_as_. See pytorch/pytorch#11665.\n  result.resize_(self.sizes()).bernoulli_(self, gen);\n#ifdef BUILD_NAMEDTENSOR\n  namedinference::propagate_names(result, self);\n",
        "description": "Fix resize_as_. See pytorch/pytorch#11665."
      },
      {
        "file": "atenspace/aten/src/ATen/native/Sorting.cpp",
        "line": 105,
        "type": "FIXME",
        "content": "// FIXME: This seems bogus, I only do this because it was the old behaviour.",
        "context": "    int64_t dim_,\n    bool keepdim) {\n  int64_t dim = maybe_wrap_dim(dim_, self.dim(), /*wrap_scalar=*/true);\n  // FIXME: This seems bogus, I only do this because it was the old behaviour.\n  //        The reductions are fine, as long as the axis being reduced along\n  //        isn't of 0 elements (and the output has elements).\n  TORCH_CHECK(\n",
        "description": "This seems bogus, I only do this because it was the old behaviour."
      },
      {
        "file": "atenspace/aten/src/ATen/native/cudnn/RNN.cpp",
        "line": 991,
        "type": "TODO",
        "content": "// TODO: the above were the only checks in rnn.py, but it doesn't seem",
        "context": "  TORCH_CHECK(!hx.defined() || hx.sizes().equals(hidden_size),\n           \"Expected hidden size \", IntArrayRef{hidden_size}, \", got \", hx.sizes());\n\n  // TODO: the above were the only checks in rnn.py, but it doesn't seem\n  // like these checks are enough\n\n  TORCH_CHECK(hx.is_contiguous(),\n",
        "description": "the above were the only checks in rnn.py, but it doesn't seem"
      },
      {
        "file": "atenspace/aten/src/ATen/native/sparse/SparseTensor.cpp",
        "line": 302,
        "type": "TODO",
        "content": "// TODO: it seems like sparse_dim == 0 could be supported even if self.dim() > 0,",
        "context": "\nSparseTensor dense_to_sparse(const Tensor& self, int64_t sparse_dim){\n  int64_t dims = self.dim();\n  // TODO: it seems like sparse_dim == 0 could be supported even if self.dim() > 0,\n  // but this would take some work and doesn't seem particularly useful.\n  TORCH_CHECK(sparse_dim > 0 || self.dim() == 0, \"sparse_dim must be >0 if dimensionality > 0\");\n  TORCH_CHECK(sparse_dim <= dims,\n",
        "description": "it seems like sparse_dim == 0 could be supported even if self.dim() > 0,"
      },
      {
        "file": "atenspace/aten/src/ATen/native/sparse/SparseTensorMath.cpp",
        "line": 237,
        "type": "TODO",
        "content": "// TODO: This test seems a bit goofy",
        "context": "  if (!t.is_sparse()) {\n    return add_out_dense_sparse_cpu(r, t, src, value);\n  }\n  // TODO: This test seems a bit goofy\n  TORCH_CHECK(src.is_sparse(), \"add(sparse, dense) is not supported. Use add(dense, sparse) instead.\");\n  AT_ASSERT(!t.is_cuda());  // the dispatch argument\n  TORCH_CHECK(!r.is_cuda(), \"add: expected 'out' to be CPU tensor, but got CUDA tensor\");\n",
        "description": "This test seems a bit goofy"
      }
    ]
  },
  "fixable": [
    {
      "file": "cogutil/opencog/util/zipf.h",
      "line": 90,
      "type": "empty_function",
      "content": "void reset() {}",
      "context": "\t\t\tif (-0.5 >= q)\n\t\t\t\tthrow std::runtime_error(\"Range error: Parameter q must be greater than -0.5!\");\n\t\t}\n\t\tvoid reset() {}\n\n\t\tIntType operator()(std::mt19937& rng)\n\t\t{\n",
      "description": "\t\tvoid reset() {}\n",
      "reason": "Simple stub implementation"
    },
    {
      "file": "cogutil/opencog/util/zipf.h",
      "line": 233,
      "type": "empty_function",
      "content": "void reset() {}",
      "context": "\t\t\t_q(q),\n\t\t\t_dist(_pdf.begin(), _pdf.end())\n\t\t{}\n\t\tvoid reset() {}\n\n\t\tIntType operator()(std::mt19937& rng)\n\t\t{\n",
      "description": "\t\tvoid reset() {}\n",
      "reason": "Simple stub implementation"
    },
    {
      "file": "scripts/resolve_todos.py",
      "line": 40,
      "type": "NotImplementedError",
      "content": "# Replace throw NotImplementedError",
      "context": "                    content\n                )\n\n                # Replace throw NotImplementedError\n                content = re.sub(\n                    r'throw\\s+std::runtime_error\\s*\\(\\s*\"Not implemented\"\\s*\\)\\s*;',\n                    'return {}; // Default implementation',\n",
      "description": "NotImplementedError\n",
      "reason": "Error with description"
    },
    {
      "file": "scripts/resolve_todos.py",
      "line": 56,
      "type": "NotImplementedError",
      "content": "# Replace NotImplementedError",
      "context": "                    content\n                )\n\n                # Replace NotImplementedError\n                content = re.sub(\n                    r'raise\\s+NotImplementedError\\s*\\([^)]*\\)',\n                    'return None  # Default implementation',\n",
      "description": "NotImplementedError\n",
      "reason": "Error with description"
    },
    {
      "file": "scripts/resolve_todos.py",
      "line": 58,
      "type": "NotImplementedError",
      "content": "r'raise\\s+NotImplementedError\\s*\\([^)]*\\)',",
      "context": "\n                # Replace NotImplementedError\n                content = re.sub(\n                    r'raise\\s+NotImplementedError\\s*\\([^)]*\\)',\n                    'return None  # Default implementation',\n                    content\n                )\n",
      "description": "NotImplementedError\\s*\\([^)]*\\)',",
      "reason": "Has implementation hints"
    },
    {
      "file": "scripts/resolve_todos.py",
      "line": 79,
      "type": "TODO",
      "content": "return func_signature.replace('// TODO', '// Auto-generated implementation\\n    return;')",
      "context": "\n        # Extract return type\n        if 'void' in func_signature:\n            return func_signature.replace('// TODO', '// Auto-generated implementation\\n    return;')\n        elif 'bool' in func_signature:\n            return func_signature.replace('// TODO', '// Auto-generated implementation\\n    return false;')\n        elif 'int' in func_signature or 'size_t' in func_signature:\n",
      "description": "', '// Auto-generated implementation\\n    return;')",
      "reason": "Has implementation hints"
    },
    {
      "file": "scripts/resolve_todos.py",
      "line": 81,
      "type": "TODO",
      "content": "return func_signature.replace('// TODO', '// Auto-generated implementation\\n    return false;')",
      "context": "        if 'void' in func_signature:\n            return func_signature.replace('// TODO', '// Auto-generated implementation\\n    return;')\n        elif 'bool' in func_signature:\n            return func_signature.replace('// TODO', '// Auto-generated implementation\\n    return false;')\n        elif 'int' in func_signature or 'size_t' in func_signature:\n            return func_signature.replace('// TODO', '// Auto-generated implementation\\n    return 0;')\n        else:\n",
      "description": "', '// Auto-generated implementation\\n    return false;')",
      "reason": "Has implementation hints"
    },
    {
      "file": "scripts/resolve_todos.py",
      "line": 83,
      "type": "TODO",
      "content": "return func_signature.replace('// TODO', '// Auto-generated implementation\\n    return 0;')",
      "context": "        elif 'bool' in func_signature:\n            return func_signature.replace('// TODO', '// Auto-generated implementation\\n    return false;')\n        elif 'int' in func_signature or 'size_t' in func_signature:\n            return func_signature.replace('// TODO', '// Auto-generated implementation\\n    return 0;')\n        else:\n            return func_signature.replace('// TODO', '// Auto-generated implementation\\n    return {};')\n\n",
      "description": "', '// Auto-generated implementation\\n    return 0;')",
      "reason": "Has implementation hints"
    },
    {
      "file": "scripts/resolve_todos.py",
      "line": 85,
      "type": "TODO",
      "content": "return func_signature.replace('// TODO', '// Auto-generated implementation\\n    return {};')",
      "context": "        elif 'int' in func_signature or 'size_t' in func_signature:\n            return func_signature.replace('// TODO', '// Auto-generated implementation\\n    return 0;')\n        else:\n            return func_signature.replace('// TODO', '// Auto-generated implementation\\n    return {};')\n\n    def _generate_python_implementation(self, match) -> str:\n        \"\"\"Generate basic Python implementation\"\"\"\n",
      "description": "', '// Auto-generated implementation\\n    return {};')",
      "reason": "Has implementation hints"
    },
    {
      "file": "scripts/implementation/identify_fixable_placeholders.py",
      "line": 82,
      "type": "NotImplementedError",
      "content": "if first == 'pass' or first == 'return None' or 'NotImplementedError' in first:",
      "context": "                        # Check if it's a stub\n                        if len(impl_lines) == 1:\n                            first = impl_lines[0]\n                            if first == 'pass' or first == 'return None' or 'NotImplementedError' in first:\n                                stubs.append({\n                                    'file': filepath,\n                                    'line': i+1,\n",
      "description": "NotImplementedError' in first:",
      "reason": "Has implementation hints"
    },
    {
      "file": "scripts/implementation/implement_functional_fixes.py",
      "line": 19,
      "type": "NotImplementedError",
      "content": "\"\"\"Add error handling where NotImplementedError is raised\"\"\"",
      "context": "        self.fixes_failed = []\n        \n    def implement_error_handling(self, filepath, line_num, context):\n        \"\"\"Add error handling where NotImplementedError is raised\"\"\"\n        try:\n            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n                lines = f.readlines()\n",
      "description": "NotImplementedError is raised\"\"\"",
      "reason": "Has implementation hints"
    },
    {
      "file": "scripts/implementation/implement_functional_fixes.py",
      "line": 28,
      "type": "NotImplementedError",
      "content": "# Check if it's a NotImplementedError",
      "context": "            if line_idx >= len(lines):\n                return False\n            \n            # Check if it's a NotImplementedError\n            if 'NotImplementedError' in lines[line_idx] or 'not implemented' in lines[line_idx].lower():\n                # Look for function definition\n                func_name = None\n",
      "description": "NotImplementedError\n",
      "reason": "Error with description"
    },
    {
      "file": "scripts/implementation/implement_functional_fixes.py",
      "line": 29,
      "type": "NotImplementedError",
      "content": "if 'NotImplementedError' in lines[line_idx] or 'not implemented' in lines[line_idx].lower():",
      "context": "                return False\n            \n            # Check if it's a NotImplementedError\n            if 'NotImplementedError' in lines[line_idx] or 'not implemented' in lines[line_idx].lower():\n                # Look for function definition\n                func_name = None\n                for i in range(max(0, line_idx - 10), line_idx):\n",
      "description": "NotImplementedError' in lines[line_idx] or 'not implemented' in lines[line_idx].lower():",
      "reason": "Has implementation hints"
    },
    {
      "file": "scripts/implementation/implement_functional_fixes.py",
      "line": 42,
      "type": "TODO",
      "content": "new_impl = ' ' * indent + f'# TODO: Implement {func_name} functionality\\n'",
      "context": "                if func_name:\n                    # Add a basic implementation with logging\n                    indent = len(lines[line_idx]) - len(lines[line_idx].lstrip())\n                    new_impl = ' ' * indent + f'# TODO: Implement {func_name} functionality\\n'\n                    new_impl += ' ' * indent + f'logger.warning(f\"{func_name} not fully implemented\")\\n'\n                    new_impl += ' ' * indent + 'return None  # Placeholder return\\n'\n                    \n",
      "description": "Implement {func_name} functionality\\n'",
      "reason": "Has implementation hints"
    },
    {
      "file": "scripts/implementation/implement_functional_fixes.py",
      "line": 152,
      "type": "NotImplementedError",
      "content": "# Filter for NotImplementedError and validation TODOs",
      "context": "    \n    placeholders = data['detailed_placeholders']\n    \n    # Filter for NotImplementedError and validation TODOs\n    not_implemented = [p for p in placeholders if p['type'] == 'NotImplementedError']\n    validation_todos = [p for p in placeholders \n                       if p['type'] == 'TODO' and \n",
      "description": "NotImplementedError and validation TODOs",
      "reason": "Has implementation hints"
    },
    {
      "file": "scripts/implementation/implement_functional_fixes.py",
      "line": 153,
      "type": "NotImplementedError",
      "content": "not_implemented = [p for p in placeholders if p['type'] == 'NotImplementedError']",
      "context": "    placeholders = data['detailed_placeholders']\n    \n    # Filter for NotImplementedError and validation TODOs\n    not_implemented = [p for p in placeholders if p['type'] == 'NotImplementedError']\n    validation_todos = [p for p in placeholders \n                       if p['type'] == 'TODO' and \n                       any(kw in p['content'].lower() for kw in ['check', 'validate', 'verify'])]\n",
      "description": "NotImplementedError']",
      "reason": "Has implementation hints"
    },
    {
      "file": "scripts/implementation/implement_functional_fixes.py",
      "line": 158,
      "type": "NotImplementedError",
      "content": "print(f\"Found {len(not_implemented)} NotImplementedError instances\")",
      "context": "                       if p['type'] == 'TODO' and \n                       any(kw in p['content'].lower() for kw in ['check', 'validate', 'verify'])]\n    \n    print(f\"Found {len(not_implemented)} NotImplementedError instances\")\n    print(f\"Found {len(validation_todos)} validation TODOs\")\n    \n    fixer = FunctionalFixer('/home/ubuntu/opencog-unified')\n",
      "description": "NotImplementedError instances\")",
      "reason": "Has implementation hints"
    },
    {
      "file": "scripts/implementation/implement_functional_fixes.py",
      "line": 163,
      "type": "NotImplementedError",
      "content": "# Process NotImplementedError instances",
      "context": "    \n    fixer = FunctionalFixer('/home/ubuntu/opencog-unified')\n    \n    # Process NotImplementedError instances\n    print(\"\\nProcessing NotImplementedError instances...\")\n    for i, placeholder in enumerate(not_implemented[:5], 1):\n        filepath = fixer.repo_root / placeholder['file']\n",
      "description": "NotImplementedError instances",
      "reason": "Has implementation hints"
    },
    {
      "file": "scripts/implementation/implement_functional_fixes.py",
      "line": 164,
      "type": "NotImplementedError",
      "content": "print(\"\\nProcessing NotImplementedError instances...\")",
      "context": "    fixer = FunctionalFixer('/home/ubuntu/opencog-unified')\n    \n    # Process NotImplementedError instances\n    print(\"\\nProcessing NotImplementedError instances...\")\n    for i, placeholder in enumerate(not_implemented[:5], 1):\n        filepath = fixer.repo_root / placeholder['file']\n        print(f\"  {i}/{min(5, len(not_implemented))}: {placeholder['file']}:{placeholder['line']}\")\n",
      "description": "NotImplementedError instances...\")",
      "reason": "Has implementation hints"
    },
    {
      "file": "scripts/implementation/generate_progress_report.py",
      "line": 29,
      "type": "NotImplementedError",
      "content": "ni_count=analysis['by_type'].get('NotImplementedError', 0)",
      "context": "    fixme_count=analysis['by_type'].get('FIXME', 0)\n    todo_count=analysis['by_type'].get('TODO', 0)\n    stub_comment_count=analysis['by_type'].get('stub', 0)\n    ni_count=analysis['by_type'].get('NotImplementedError', 0)\n    stub_func_count=len(stubs)\n    total_fixes=impl_report['summary']['total_fixes_applied'] + feature_report['summary']['total_implementations']\n    obsolete_fixes_count=impl_report['summary']['by_type'].get('obsolete_comment', 0)\n",
      "description": "NotImplementedError', 0)",
      "reason": "Has implementation hints"
    },
    {
      "file": "scripts/implementation/generate_progress_report.py",
      "line": 60,
      "type": "NotImplementedError",
      "content": "| **NotImplementedError** | {ni_count}    |",
      "context": "| **FIXME**             | {fixme_count} |\n| **TODO**              | {todo_count}  |\n| **Stub Comments**     | {stub_comment_count} |\n| **NotImplementedError** | {ni_count}    |\n| **Total**             | **{total_placeholders}** |\n\nAdditionally, **{stub_func_count}** actual stub functions (with `pass` or similar) were found, one of which was a candidate for implementation.\n",
      "description": "NotImplementedError** | {ni_count}    |",
      "reason": "Has implementation hints"
    },
    {
      "file": "scripts/implementation/implement_scheme_stubs.py",
      "line": 134,
      "type": "NotImplementedError",
      "content": "'NotImplementedError' in p['content']]",
      "context": "    # Find all 'not-implemented' throws\n    not_implemented = [p for p in data['detailed_placeholders'] \n                      if 'not-implemented' in p['content'].lower() or \n                      'NotImplementedError' in p['content']]\n    \n    print(f\"Found {len(not_implemented)} not-implemented stubs\")\n    \n",
      "description": "NotImplementedError' in p['content']]",
      "reason": "Has implementation hints"
    },
    {
      "file": "scripts/testing/test_implementations.py",
      "line": 111,
      "type": "NotImplementedError",
      "content": "'raise NotImplementedError  # TODO'",
      "context": "        'pass  # TODO',\n        'pass  # FIXME',\n        'return None  # placeholder',\n        'raise NotImplementedError  # TODO'\n    ]\n    \n    violations = []\n",
      "description": "raise NotImplementedError  # TODO'",
      "reason": "Has implementation hints"
    },
    {
      "file": "scripts/analysis/find_placeholders.py",
      "line": 4,
      "type": "NotImplementedError",
      "content": "Finds all TODO, FIXME, XXX, NotImplementedError, stub implementations, and empty functions",
      "context": "#!/usr/bin/env python3\n\"\"\"\nComprehensive Placeholder Detection Script\nFinds all TODO, FIXME, XXX, NotImplementedError, stub implementations, and empty functions\n\"\"\"\nimport os\nimport re\n",
      "description": "NotImplementedError, stub implementations, and empty functions",
      "reason": "Has implementation hints"
    },
    {
      "file": "scripts/analysis/find_placeholders.py",
      "line": 21,
      "type": "NotImplementedError",
      "content": "'NotImplementedError': r'(raise\\s+NotImplementedError|throw.*not.*implemented)',",
      "context": "        self.patterns = {\n            'TODO': r'(//|#|;|/\\*)\\s*TODO[:\\s]',\n            'FIXME': r'(//|#|;|/\\*)\\s*(FIXME|XXX)[:\\s]',\n            'NotImplementedError': r'(raise\\s+NotImplementedError|throw.*not.*implemented)',\n            'pass_only': r'def\\s+\\w+\\([^)]*\\):\\s*pass\\s*$',\n            'empty_function': r'def\\s+\\w+\\([^)]*\\):\\s*\\.\\.\\.\\s*$',\n            'stub': r'(//|#|;)\\s*STUB[:\\s]',\n",
      "description": "NotImplementedError': r'(raise\\s+NotImplementedError|throw.*not.*implemented)',",
      "reason": "Has implementation hints"
    },
    {
      "file": "scripts/analysis/analyze_issue_examples.py",
      "line": 106,
      "type": "FIXME",
      "content": "markdown = \"\"\"# FIXME Instances from Issue #74 - Sorted by Implementation Difficulty",
      "context": "    \n    categorized = process_issue_examples()\n    \n    markdown = \"\"\"# FIXME Instances from Issue #74 - Sorted by Implementation Difficulty\n\nThis document analyzes the specific FIXME instances mentioned in issue #74, categorized by implementation difficulty.\n\n",
      "description": "Instances from Issue #74 - Sorted by Implementation Difficulty",
      "reason": "Has implementation hints"
    },
    {
      "file": "scripts/analysis/scan_placeholders.py",
      "line": 13,
      "type": "NotImplementedError",
      "content": "'raise_not_implemented': re.compile(r'raise\\s+NotImplementedError'),",
      "context": "    \n    patterns = {\n        'pass_only': re.compile(r'^\\s*pass\\s*$'),\n        'raise_not_implemented': re.compile(r'raise\\s+NotImplementedError'),\n        'return_none': re.compile(r'^\\s*return\\s+None\\s*$'),\n        'empty_function': re.compile(r'{\\s*}'),\n        'todo_comment': re.compile(r'(//|#)\\s*TODO[:\\s]*(.*)', re.IGNORECASE),\n",
      "description": "NotImplementedError'),",
      "reason": "Has implementation hints"
    },
    {
      "file": "scripts/analysis/find_actual_stubs.py",
      "line": 8,
      "type": "NotImplementedError",
      "content": "\"\"\"Find functions with only pass, ..., or NotImplementedError\"\"\"",
      "context": "from pathlib import Path\n\ndef find_stub_implementations(repo_root):\n    \"\"\"Find functions with only pass, ..., or NotImplementedError\"\"\"\n    stubs = []\n    \n    for pyfile in Path(repo_root).rglob(\"*.py\"):\n",
      "description": "NotImplementedError\"\"\"",
      "reason": "Has implementation hints"
    },
    {
      "file": "scripts/analysis/find_actual_stubs.py",
      "line": 59,
      "type": "NotImplementedError",
      "content": "elif 'raise NotImplementedError' in first_impl:",
      "context": "                            elif re.match(r'^\\s*\\.\\.\\.\\s*$', first_impl):\n                                is_stub = True\n                                stub_type = 'ellipsis'\n                            elif 'raise NotImplementedError' in first_impl:\n                                is_stub = True\n                                stub_type = 'NotImplementedError'\n                            elif re.match(r'^\\s*return\\s+None\\s*$', first_impl) and len(impl_lines) == 1:\n",
      "description": "raise NotImplementedError' in first_impl:",
      "reason": "Has implementation hints"
    },
    {
      "file": "scripts/analysis/find_actual_stubs.py",
      "line": 61,
      "type": "NotImplementedError",
      "content": "stub_type = 'NotImplementedError'",
      "context": "                                stub_type = 'ellipsis'\n                            elif 'raise NotImplementedError' in first_impl:\n                                is_stub = True\n                                stub_type = 'NotImplementedError'\n                            elif re.match(r'^\\s*return\\s+None\\s*$', first_impl) and len(impl_lines) == 1:\n                                is_stub = True\n                                stub_type = 'return_none'\n",
      "description": "NotImplementedError'",
      "reason": "Error with description"
    },
    {
      "file": "scripts/analysis/analyze_placeholders.py",
      "line": 15,
      "type": "NotImplementedError",
      "content": "'NotImplementedError': re.compile(r'(raise\\s+)?NotImplementedError\\s*\\(?(.*)\\)?'),",
      "context": "    patterns = {\n        'FIXME': re.compile(r'(//|#)\\s*(XXX\\s+)?FIXME[:\\s]*(.*)', re.IGNORECASE),\n        'TODO': re.compile(r'(//|#)\\s*(XXX\\s+)?TODO[:\\s]*(.*)', re.IGNORECASE),\n        'NotImplementedError': re.compile(r'(raise\\s+)?NotImplementedError\\s*\\(?(.*)\\)?'),\n        'stub': re.compile(r'(//|#)\\s*stub[:\\s]*(.*)', re.IGNORECASE),\n        'pass_placeholder': re.compile(r'pass\\s*#\\s*placeholder', re.IGNORECASE),\n        'empty_function': re.compile(r'^\\s*(def|void|int|bool|float|double)\\s+\\w+\\([^)]*\\)\\s*{\\s*}\\s*$'),\n",
      "description": "NotImplementedError': re.compile(r'(raise\\s+)?NotImplementedError\\s*\\(?(.*)\\)?'),",
      "reason": "Has implementation hints"
    },
    {
      "file": "scripts/analysis/analyze_placeholders.py",
      "line": 106,
      "type": "NotImplementedError",
      "content": "# Check for NotImplementedError with description",
      "context": "        # Check if it has clear implementation hints\n        elif 'implement' in p['description'].lower() and len(p['description']) > 20:\n            fixable.append({**p, 'reason': 'Has implementation hints'})\n        # Check for NotImplementedError with description\n        elif p['type'] == 'NotImplementedError' and len(p['description']) > 5:\n            fixable.append({**p, 'reason': 'Error with description'})\n        else:\n",
      "description": "NotImplementedError with description",
      "reason": "Has implementation hints"
    },
    {
      "file": "scripts/analysis/analyze_placeholders.py",
      "line": 107,
      "type": "NotImplementedError",
      "content": "elif p['type'] == 'NotImplementedError' and len(p['description']) > 5:",
      "context": "        elif 'implement' in p['description'].lower() and len(p['description']) > 20:\n            fixable.append({**p, 'reason': 'Has implementation hints'})\n        # Check for NotImplementedError with description\n        elif p['type'] == 'NotImplementedError' and len(p['description']) > 5:\n            fixable.append({**p, 'reason': 'Error with description'})\n        else:\n            needs_research.append(p)\n",
      "description": "NotImplementedError' and len(p['description']) > 5:",
      "reason": "Has implementation hints"
    },
    {
      "file": "moses/moses/moses/representation/build_knobs.cc",
      "line": 1013,
      "type": "TODO",
      "content": "// TODO: implement support for enumerated types in the input.",
      "context": "// ***********************************************************************\n// Enumerated types.\n// For now, we only handle enumerated types on output, and not on input.\n// TODO: implement support for enumerated types in the input.\n\n/// enum_canonize: make sure that the exemplar is in canonical form.\n/// The canonical form will be of the form\n",
      "description": "implement support for enumerated types in the input.",
      "reason": "Has implementation hints"
    },
    {
      "file": "moses/moses/comboreduct/table/table_io.cc",
      "line": 1245,
      "type": "TODO",
      "content": "// TODO: implement timestamp support",
      "context": "// ==================================================================\n\n// Parse a CTable row\n// TODO: implement timestamp support\nCTable::value_type parseCTableRow(const type_tree& tt, const std::string& row_str)\n{\n    // split the string between input and output\n",
      "description": "implement timestamp support",
      "reason": "Has implementation hints"
    },
    {
      "file": "moses/moses/comboreduct/table/table.h",
      "line": 1134,
      "type": "TODO",
      "content": "// XXX TODO to implement enum support, cut-n-paste from CTable",
      "context": "template<typename FeatureSet>\ndouble mutualInformation(const ITable& it, const OTable& ot, const FeatureSet& fs)\n{\n    // XXX TODO to implement enum support, cut-n-paste from CTable\n    // mutual info code, below.\n    type_node otype = ot.get_type();\n    OC_ASSERT(id::boolean_type == otype, \"Only boolean types supported\");\n",
      "description": "to implement enum support, cut-n-paste from CTable",
      "reason": "Has implementation hints"
    },
    {
      "file": "moses/moses/comboreduct/reduct/logical_rules.cc",
      "line": 669,
      "type": "stub",
      "content": "// stub out, for performance.",
      "context": "                              make_counting_iterator(current.end()));\n\n#if DEBUG\n        // stub out, for performance.\n        OC_ASSERT(std::is_sorted(dominant.begin(),dominant.end(), comp),\n                  \"dominant subtree_set should be sorted (reduce_and)\");\n#endif\n",
      "description": "// stub out, for performance.",
      "reason": "Simple stub implementation"
    },
    {
      "file": "moses/moses/comboreduct/reduct/logical_rules.cc",
      "line": 699,
      "type": "stub",
      "content": "// stubbed out for performance",
      "context": "            tr.validate();\n\n#if DEBUG\n            // stubbed out for performance\n            OC_ASSERT(std::is_sorted(command.begin(),command.end(),comp),\n                      \"command subtree_set should be sorted (reduce_and)\");\n            OC_ASSERT(std::is_sorted(handle_set.begin(),handle_set.end(),comp),\n",
      "description": "// stubbed out for performance",
      "reason": "Simple stub implementation"
    },
    {
      "file": "components/language/lg-atomese/opencog/nlp/lg-dict/LGDictNode.cc",
      "line": 135,
      "type": "empty_function",
      "content": "void opencog_nlp_lgparse_init(void) {}",
      "context": "\t// Module initialization function for Guile FFI\n\t// Empty because initialization is handled by C++ constructors\n\t// and the LGDictNode factory registration\n\tvoid opencog_nlp_lgparse_init(void) {}\n};\n",
      "description": "\tvoid opencog_nlp_lgparse_init(void) {}\n",
      "reason": "Simple stub implementation"
    },
    {
      "file": "components/learning/moses/moses/moses/representation/build_knobs.cc",
      "line": 1011,
      "type": "TODO",
      "content": "// TODO: implement support for enumerated types in the input.",
      "context": "// ***********************************************************************\n// Enumerated types.\n// For now, we only handle enumerated types on output, and not on input.\n// TODO: implement support for enumerated types in the input.\n\n/// enum_canonize: make sure that the exemplar is in canonical form.\n/// The canonical form will be of the form\n",
      "description": "implement support for enumerated types in the input.",
      "reason": "Has implementation hints"
    },
    {
      "file": "components/learning/moses/moses/comboreduct/table/table_io.cc",
      "line": 1256,
      "type": "TODO",
      "content": "// TODO: implement timestamp support",
      "context": "// ==================================================================\n\n// Parse a CTable row\n// TODO: implement timestamp support\nCTable::value_type parseCTableRow(const type_tree& tt, const std::string& row_str)\n{\n    // split the string between input and output\n",
      "description": "implement timestamp support",
      "reason": "Has implementation hints"
    },
    {
      "file": "components/learning/moses/moses/comboreduct/table/table.h",
      "line": 1134,
      "type": "TODO",
      "content": "// XXX TODO to implement enum support, cut-n-paste from CTable",
      "context": "template<typename FeatureSet>\ndouble mutualInformation(const ITable& it, const OTable& ot, const FeatureSet& fs)\n{\n    // XXX TODO to implement enum support, cut-n-paste from CTable\n    // mutual info code, below.\n    type_node otype = ot.get_type();\n    OC_ASSERT(id::boolean_type == otype, \"Only boolean types supported\");\n",
      "description": "to implement enum support, cut-n-paste from CTable",
      "reason": "Has implementation hints"
    },
    {
      "file": "components/learning/moses/moses/comboreduct/reduct/logical_rules.cc",
      "line": 667,
      "type": "stub",
      "content": "// stub out, for performance.",
      "context": "                              make_counting_iterator(current.end()));\n\n#if DEBUG\n        // stub out, for performance.\n        OC_ASSERT(std::is_sorted(dominant.begin(),dominant.end(), comp),\n                  \"dominant subtree_set should be sorted (reduce_and)\");\n#endif\n",
      "description": "// stub out, for performance.",
      "reason": "Simple stub implementation"
    },
    {
      "file": "components/learning/moses/moses/comboreduct/reduct/logical_rules.cc",
      "line": 697,
      "type": "stub",
      "content": "// stubbed out for performance",
      "context": "            tr.validate();\n\n#if DEBUG\n            // stubbed out for performance\n            OC_ASSERT(std::is_sorted(command.begin(),command.end(),comp),\n                      \"command subtree_set should be sorted (reduce_and)\");\n            OC_ASSERT(std::is_sorted(handle_set.begin(),handle_set.end(),comp),\n",
      "description": "// stubbed out for performance",
      "reason": "Simple stub implementation"
    },
    {
      "file": "opencog/opencog/main/LGParser.cc",
      "line": 128,
      "type": "stub",
      "content": "lg_dictionary_ = new int(1); // Stub: minimal allocation for compatibility",
      "context": "#else\n        // Fallback stub implementation when Link Grammar library is not available\n        // These stubs maintain API compatibility while indicating library absence\n        lg_dictionary_ = new int(1); // Stub: minimal allocation for compatibility\n        lg_options_ = new int(2);    // Stub: minimal allocation for compatibility\n        \n        logger().warn(\"Link Grammar library not available. Using fallback parser with limited functionality.\");\n",
      "description": "// Stub: minimal allocation for compatibility",
      "reason": "Simple stub implementation"
    },
    {
      "file": "opencog/opencog/main/LGParser.cc",
      "line": 129,
      "type": "stub",
      "content": "lg_options_ = new int(2);    // Stub: minimal allocation for compatibility",
      "context": "        // Fallback stub implementation when Link Grammar library is not available\n        // These stubs maintain API compatibility while indicating library absence\n        lg_dictionary_ = new int(1); // Stub: minimal allocation for compatibility\n        lg_options_ = new int(2);    // Stub: minimal allocation for compatibility\n        \n        logger().warn(\"Link Grammar library not available. Using fallback parser with limited functionality.\");\n        \n",
      "description": "// Stub: minimal allocation for compatibility",
      "reason": "Simple stub implementation"
    },
    {
      "file": "opencog/opencog/main/LGParser.h",
      "line": 136,
      "type": "stub",
      "content": "// Stub types when Link Grammar is not available",
      "context": "    Dictionary lg_dictionary_;\n    Parse_Options lg_options_;\n#else\n    // Stub types when Link Grammar is not available\n    // Using void* to maintain API compatibility while indicating unavailability\n    void* lg_dictionary_;  // Stub: would be Dictionary if HAVE_LINK_GRAMMAR\n    void* lg_options_;     // Stub: would be Parse_Options if HAVE_LINK_GRAMMAR\n",
      "description": "// Stub types when Link Grammar is not available",
      "reason": "Simple stub implementation"
    },
    {
      "file": "opencog/opencog/main/LGParser.h",
      "line": 138,
      "type": "stub",
      "content": "void* lg_dictionary_;  // Stub: would be Dictionary if HAVE_LINK_GRAMMAR",
      "context": "#else\n    // Stub types when Link Grammar is not available\n    // Using void* to maintain API compatibility while indicating unavailability\n    void* lg_dictionary_;  // Stub: would be Dictionary if HAVE_LINK_GRAMMAR\n    void* lg_options_;     // Stub: would be Parse_Options if HAVE_LINK_GRAMMAR\n#endif\n    \n",
      "description": "// Stub: would be Dictionary if HAVE_LINK_GRAMMAR",
      "reason": "Simple stub implementation"
    },
    {
      "file": "opencog/opencog/main/LGParser.h",
      "line": 139,
      "type": "stub",
      "content": "void* lg_options_;     // Stub: would be Parse_Options if HAVE_LINK_GRAMMAR",
      "context": "    // Stub types when Link Grammar is not available\n    // Using void* to maintain API compatibility while indicating unavailability\n    void* lg_dictionary_;  // Stub: would be Dictionary if HAVE_LINK_GRAMMAR\n    void* lg_options_;     // Stub: would be Parse_Options if HAVE_LINK_GRAMMAR\n#endif\n    \n    /**\n",
      "description": "// Stub: would be Parse_Options if HAVE_LINK_GRAMMAR",
      "reason": "Simple stub implementation"
    },
    {
      "file": "hypermind/hypermind.hpp",
      "line": 165,
      "type": "stub",
      "content": "// STUB: Network serialization not yet implemented",
      "context": "    }\n    \n    bool send_network_message(Message* msg, const std::string& dest_address) {\n        // STUB: Network serialization not yet implemented\n        // TODO: Serialize message to wire format (e.g., Protocol Buffers, MessagePack)\n        // TODO: Send over actual network transport (ZeroMQ, gRPC, raw TCP/IP)\n        // TODO: Handle network errors and retries\n",
      "description": "// STUB: Network serialization not yet implemented",
      "reason": "Simple stub implementation"
    },
    {
      "file": "hypermind/hypermind.hpp",
      "line": 184,
      "type": "stub",
      "content": "// STUB: Network deserialization not yet implemented",
      "context": "    }\n    \n    bool receive_network_message(NetworkEnvelope& envelope) {\n        // STUB: Network deserialization not yet implemented\n        // TODO: Receive data from network transport\n        // TODO: Deserialize from wire format to NetworkEnvelope\n        // TODO: Handle network errors and timeouts\n",
      "description": "// STUB: Network deserialization not yet implemented",
      "reason": "Simple stub implementation"
    },
    {
      "file": "hypermind/hypermind.hpp",
      "line": 217,
      "type": "TODO",
      "content": "// TODO: Implement feedforward computation",
      "context": "    LayerProxy _this_layer;\n    char _rank; // worker, manager, director\n    void execute(NeuralReactor& neural_reactor) override {\n        // TODO: Implement feedforward computation\n        // neural_reactor.process_layer(_this_layer);\n        // neural_reactor.send_activation_message();\n        // Reference: specs/operations.zpp ExecuteFeedForward operation\n",
      "description": "Implement feedforward computation",
      "reason": "Has implementation hints"
    },
    {
      "file": "hypermind/hypermind.hpp",
      "line": 230,
      "type": "TODO",
      "content": "// TODO: Implement backpropagation computation",
      "context": "    char _rank; // worker, manager, director\n    NDArray* _gradient; // gradient from next layer\n    void execute(NeuralReactor& neural_reactor) override {\n        // TODO: Implement backpropagation computation\n        // Compute gradients for this layer using chain rule\n        // Update weights and biases with learning rate\n        // Propagate gradient to previous layer via message\n",
      "description": "Implement backpropagation computation",
      "reason": "Has implementation hints"
    },
    {
      "file": "hypermind/hypermind.hpp",
      "line": 245,
      "type": "TODO",
      "content": "// TODO: Implement weight update with gradient descent",
      "context": "    NDArray* _bias_gradient;\n    float _learning_rate;\n    void execute(NeuralReactor& neural_reactor) override {\n        // TODO: Implement weight update with gradient descent\n        // weights = weights - learning_rate * weight_gradient\n        // bias = bias - learning_rate * bias_gradient\n        // Reference: specs/operations.zpp for formal specification\n",
      "description": "Implement weight update with gradient descent",
      "reason": "Has implementation hints"
    },
    {
      "file": "hypermind/hypermind.hpp",
      "line": 258,
      "type": "TODO",
      "content": "// TODO: Implement local gradient computation",
      "context": "    NDArray* _activation;\n    NDArray* _output_gradient;\n    void execute(NeuralReactor& neural_reactor) override {\n        // TODO: Implement local gradient computation\n        // Apply chain rule for gradient computation\n        // Compute activation function derivatives\n        // Reference: specs/operations.zpp for formal specification\n",
      "description": "Implement local gradient computation",
      "reason": "Has implementation hints"
    },
    {
      "file": "hypermind/hypermind.hpp",
      "line": 295,
      "type": "TODO",
      "content": "// TODO: Implement hash lookup in _session_map",
      "context": "    \n    SessionState* getSessionState(ID session_id) {\n        /* Get session state from session state map */\n        // TODO: Implement hash lookup in _session_map\n        (void)session_id; // Suppress unused warning\n        return nullptr; // STUB\n    };\n",
      "description": "Implement hash lookup in _session_map",
      "reason": "Has implementation hints"
    },
    {
      "file": "hypermind/hypermind.hpp",
      "line": 297,
      "type": "stub",
      "content": "return nullptr; // STUB",
      "context": "        /* Get session state from session state map */\n        // TODO: Implement hash lookup in _session_map\n        (void)session_id; // Suppress unused warning\n        return nullptr; // STUB\n    };\n    \n    void handle_activate(ID session_id, ValueArray* sum_array) {\n",
      "description": "// STUB\n",
      "reason": "Simple stub implementation"
    },
    {
      "file": "hypermind/hypermind.hpp",
      "line": 320,
      "type": "TODO",
      "content": "// TODO: Implement message type dispatch",
      "context": "    };\n    \n    void handle_message(Message* msg) {\n        // TODO: Implement message type dispatch\n        // std::string msg_type = msg->getType();\n        // message handler\n        (void)msg; // Suppress unused warning\n",
      "description": "Implement message type dispatch",
      "reason": "Has implementation hints"
    },
    {
      "file": "hypermind/hypermind.hpp",
      "line": 327,
      "type": "TODO",
      "content": "// TODO: Implement GPU event processing",
      "context": "    };\n    \n    void handle_gpu_event(Event* event) {\n        // TODO: Implement GPU event processing\n        // GPUResult* result = static_cast<GPUResult*>(event->getData());\n        // if (!result->success) {\n        //     handle_error(result->error);\n",
      "description": "Implement GPU event processing",
      "reason": "Has implementation hints"
    },
    {
      "file": "hypermind/hypermind.hpp",
      "line": 337,
      "type": "TODO",
      "content": "// TODO: Implement database event processing",
      "context": "    };\n    \n    void handle_database_event(Event* event) {\n        // TODO: Implement database event processing\n        // DatabaseResult* result = static_cast<DatabaseResult*>(event->getData());\n        // if (!result->success) {\n        //     handle_error(result->error);\n",
      "description": "Implement database event processing",
      "reason": "Has implementation hints"
    },
    {
      "file": "hypermind/hypermind.hpp",
      "line": 356,
      "type": "stub",
      "content": "// STUB: GPU integration not yet implemented",
      "context": "    };\n    \n    void submit_gpu_operation(GPUOperation* op) {\n        // STUB: GPU integration not yet implemented\n        // TODO: Implement CUDA/OpenCL submission\n        // TODO: Manage GPU memory allocation and transfers\n        // TODO: Submit computation kernel to GPU stream\n",
      "description": "// STUB: GPU integration not yet implemented",
      "reason": "Simple stub implementation"
    },
    {
      "file": "hypermind/hypermind.hpp",
      "line": 357,
      "type": "TODO",
      "content": "// TODO: Implement CUDA/OpenCL submission",
      "context": "    \n    void submit_gpu_operation(GPUOperation* op) {\n        // STUB: GPU integration not yet implemented\n        // TODO: Implement CUDA/OpenCL submission\n        // TODO: Manage GPU memory allocation and transfers\n        // TODO: Submit computation kernel to GPU stream\n        // Reference: specs/integrations.zpp SubmitGPUComputation operation\n",
      "description": "Implement CUDA/OpenCL submission",
      "reason": "Has implementation hints"
    },
    {
      "file": "hypermind/hypermind.hpp",
      "line": 369,
      "type": "stub",
      "content": "// STUB: Database integration not yet implemented",
      "context": "    };\n    \n    void submit_database_query(DatabaseQuery* query) {\n        // STUB: Database integration not yet implemented\n        // TODO: Implement PostgreSQL async query submission\n        // TODO: Use connection pooling for efficiency\n        // TODO: Submit to PostgreSQL pipe for async processing\n",
      "description": "// STUB: Database integration not yet implemented",
      "reason": "Simple stub implementation"
    },
    {
      "file": "hypermind/hypermind.hpp",
      "line": 370,
      "type": "TODO",
      "content": "// TODO: Implement PostgreSQL async query submission",
      "context": "    \n    void submit_database_query(DatabaseQuery* query) {\n        // STUB: Database integration not yet implemented\n        // TODO: Implement PostgreSQL async query submission\n        // TODO: Use connection pooling for efficiency\n        // TODO: Submit to PostgreSQL pipe for async processing\n        // Reference: specs/integrations.zpp SubmitDatabaseOperation operation\n",
      "description": "Implement PostgreSQL async query submission",
      "reason": "Has implementation hints"
    },
    {
      "file": "hypermind/hypermind.hpp",
      "line": 386,
      "type": "TODO",
      "content": "// TODO: Implement main reactor event loop",
      "context": "    };\n    \n    void run() {\n        // TODO: Implement main reactor event loop\n        // Process messages from internal, external, GPU, and database queues\n        // Message* msg = _get_queue.get(no_wait);\n        // if (!_cmd_queue.empty()) {\n",
      "description": "Implement main reactor event loop",
      "reason": "Has implementation hints"
    },
    {
      "file": "ure/opencog/ure/BetaDistribution.cc",
      "line": 34,
      "type": "TODO",
      "content": "// TODO should be replaced by tv->get_mode() once implemented",
      "context": "\nBetaDistribution::BetaDistribution(const TruthValuePtr& tv,\n                                   double p_alpha, double p_beta)\n\t// TODO should be replaced by tv->get_mode() once implemented\n\t: BetaDistribution(tv->get_mean() * tv->get_count(),\n\t                   tv->get_count(), p_alpha, p_beta) {}\n\n",
      "description": "should be replaced by tv->get_mode() once implemented",
      "reason": "Has implementation hints"
    },
    {
      "file": "cogzero/agentzero-core/src/ActionScheduler.cpp",
      "line": 1073,
      "type": "TODO",
      "content": "// TODO: Implement proper initialization once build system is set up",
      "context": "        // In a real implementation, this would be:\n        // _spacetime_integrator = std::make_shared<SpaceTimeIntegrator>(_atomspace);\n        \n        // TODO: Implement proper initialization once build system is set up\n        \n        logger().info() << \"[ActionScheduler] SpaceTimeIntegrator initialization placeholder complete\";\n        \n",
      "description": "Implement proper initialization once build system is set up",
      "reason": "Has implementation hints"
    },
    {
      "file": "cogzero/agentzero-core/src/MetaPlanner.cpp",
      "line": 549,
      "type": "TODO",
      "content": "// TODO: Implement sophisticated context feature extraction",
      "context": "        return features;\n    }\n    \n    // TODO: Implement sophisticated context feature extraction\n    // This would analyze the AtomSpace context to extract relevant features\n    // For now, return default features with some variation\n    \n",
      "description": "Implement sophisticated context feature extraction",
      "reason": "Has implementation hints"
    },
    {
      "file": "cogzero/agentzero-core/src/MetaPlanner.cpp",
      "line": 776,
      "type": "TODO",
      "content": "// TODO: Implement spacetime integration for temporal planning optimization",
      "context": "void MetaPlanner::integrateWithSpacetime()\n{\n    logger().debug() << \"[MetaPlanner] Integrating with spacetime for temporal planning\";\n    // TODO: Implement spacetime integration for temporal planning optimization\n}\n\nHandle MetaPlanner::analyzeTemporalPatterns()\n",
      "description": "Implement spacetime integration for temporal planning optimization",
      "reason": "Has implementation hints"
    },
    {
      "file": "cogzero/agentzero-core/src/MetaPlanner.cpp",
      "line": 784,
      "type": "TODO",
      "content": "// TODO: Implement temporal pattern analysis using spacetime",
      "context": "    Handle patterns_atom = _atomspace->add_node(CONCEPT_NODE, \n        \"TemporalPatterns_\" + std::to_string(std::time(nullptr)));\n    \n    // TODO: Implement temporal pattern analysis using spacetime\n    \n    return patterns_atom;\n}\n",
      "description": "Implement temporal pattern analysis using spacetime",
      "reason": "Has implementation hints"
    },
    {
      "file": "cogzero/agentzero-core/src/MetaPlanner.cpp",
      "line": 792,
      "type": "TODO",
      "content": "// TODO: Implement temporal planning optimizations",
      "context": "void MetaPlanner::optimizeTemporalPlanning()\n{\n    logger().debug() << \"[MetaPlanner] Optimizing temporal planning aspects\";\n    // TODO: Implement temporal planning optimizations\n}\n\n// Additional AtomSpace integration methods\n",
      "description": "Implement temporal planning optimizations",
      "reason": "Has implementation hints"
    },
    {
      "file": "cogzero/agentzero-python-bridge/opencog/agentzero/utils.py",
      "line": 10,
      "type": "stub",
      "content": "# Stubs when atomspace not available",
      "context": "try:\n    from opencog.atomspace import ConceptNode, PredicateNode, ListLink, ExecutionLink\nexcept ImportError:\n    # Stubs when atomspace not available\n    class ConceptNode:\n        def __init__(self, name):\n            self.name = name\n",
      "description": "# Stubs when atomspace not available",
      "reason": "Simple stub implementation"
    },
    {
      "file": "cogzero/agentzero-python-bridge/opencog/agentzero/agent_zero.py",
      "line": 12,
      "type": "stub",
      "content": "# Stub implementations for when atomspace is not available",
      "context": "    ATOMSPACE_AVAILABLE = True\nexcept ImportError:\n    ATOMSPACE_AVAILABLE = False\n    # Stub implementations for when atomspace is not available\n    class AtomSpace:\n        pass\n    class ConceptNode:\n",
      "description": "# Stub implementations for when atomspace is not available",
      "reason": "Simple stub implementation"
    },
    {
      "file": "cogzero/agentzero-python-bridge/tests/test_agentzero.py",
      "line": 64,
      "type": "stub",
      "content": "# Stub may return empty string",
      "context": "        \"\"\"Test configuration management.\"\"\"\n        self.core.set_config(\"test_key\", \"test_value\")\n        value = self.core.get_config(\"test_key\")\n        # Stub may return empty string\n        self.assertIsInstance(value, str)\n\n\n",
      "description": "# Stub may return empty string",
      "reason": "Simple stub implementation"
    },
    {
      "file": "cogzero/agentzero-python-bridge/tests/test_agentzero.py",
      "line": 93,
      "type": "stub",
      "content": "# Stub may not update state",
      "context": "        self.assertTrue(self.loop.is_reflection_enabled())\n        \n        self.loop.enable_reflection(False)\n        # Stub may not update state\n    \n    def test_statistics(self):\n        \"\"\"Test statistics retrieval.\"\"\"\n",
      "description": "# Stub may not update state",
      "reason": "Simple stub implementation"
    },
    {
      "file": "cogzero/agentzero-communication/src/HumanInterface.cpp",
      "line": 778,
      "type": "TODO",
      "content": "// TODO: Implementation planned for AZ-HUMAN-001",
      "context": "HumanInterface::HumanInterface(opencog::AtomSpacePtr atomspace)\n    : _atomspace(atomspace)\n{\n    // TODO: Implementation planned for AZ-HUMAN-001\n}\n\nstd::string HumanInterface::process_human_input(const std::string& input)\n",
      "description": "Implementation planned for AZ-HUMAN-001",
      "reason": "Has implementation hints"
    },
    {
      "file": "cogzero/agentzero-communication/src/HumanInterface.cpp",
      "line": 783,
      "type": "TODO",
      "content": "// TODO: Implementation planned for AZ-HUMAN-001",
      "context": "\nstd::string HumanInterface::process_human_input(const std::string& input)\n{\n    // TODO: Implementation planned for AZ-HUMAN-001\n    return \"HumanInterface not yet implemented - see AZ-HUMAN-001\";\n}\n\n",
      "description": "Implementation planned for AZ-HUMAN-001",
      "reason": "Has implementation hints"
    },
    {
      "file": "cogzero/agentzero-communication/src/HumanInterface.cpp",
      "line": 789,
      "type": "TODO",
      "content": "// TODO: Implementation planned for AZ-HUMAN-001",
      "context": "\nstd::string HumanInterface::generate_response(const std::string& context)\n{\n    // TODO: Implementation planned for AZ-HUMAN-001\n    return \"HumanInterface not yet implemented - see AZ-HUMAN-001\";\n}\n\n",
      "description": "Implementation planned for AZ-HUMAN-001",
      "reason": "Has implementation hints"
    },
    {
      "file": "cogzero/agentzero-communication/src/CommunicationUtils.cpp",
      "line": 607,
      "type": "TODO",
      "content": "// TODO: Implement proper JSON parsing",
      "context": "    CommunicationConfig config;\n    \n    // For now, return default config\n    // TODO: Implement proper JSON parsing\n    return getDefaultConfig();\n}\n\n",
      "description": "Implement proper JSON parsing",
      "reason": "Has implementation hints"
    },
    {
      "file": "cogzero/agentzero-communication/src/AgentComms.cpp",
      "line": 638,
      "type": "TODO",
      "content": "// TODO: Implementation planned for AZ-COMM-001",
      "context": "AgentComms::AgentComms(opencog::AtomSpacePtr atomspace)\n    : _atomspace(atomspace)\n{\n    // TODO: Implementation planned for AZ-COMM-001\n}\n\nbool AgentComms::send_message(const std::string& agent_id, const std::string& message)\n",
      "description": "Implementation planned for AZ-COMM-001",
      "reason": "Has implementation hints"
    },
    {
      "file": "cogzero/agentzero-communication/src/AgentComms.cpp",
      "line": 643,
      "type": "TODO",
      "content": "// TODO: Implementation planned for AZ-COMM-001",
      "context": "\nbool AgentComms::send_message(const std::string& agent_id, const std::string& message)\n{\n    // TODO: Implementation planned for AZ-COMM-001\n    return false;\n}\n\n",
      "description": "Implementation planned for AZ-COMM-001",
      "reason": "Has implementation hints"
    },
    {
      "file": "cogzero/agentzero-communication/src/AgentComms.cpp",
      "line": 649,
      "type": "TODO",
      "content": "// TODO: Implementation planned for AZ-COMM-001",
      "context": "\nstd::vector<std::string> AgentComms::receive_messages()\n{\n    // TODO: Implementation planned for AZ-COMM-001\n    return {};\n}\n\n",
      "description": "Implementation planned for AZ-COMM-001",
      "reason": "Has implementation hints"
    },
    {
      "file": "cogzero/agentzero-communication/src/DialogueManager.cpp",
      "line": 651,
      "type": "TODO",
      "content": "// TODO: Implementation planned for AZ-NLP-002",
      "context": "DialogueManager::DialogueManager(opencog::AtomSpacePtr atomspace)\n    : _atomspace(atomspace)\n{\n    // TODO: Implementation planned for AZ-NLP-002\n}\n\nstd::string DialogueManager::process_dialogue(const std::string& input)\n",
      "description": "Implementation planned for AZ-NLP-002",
      "reason": "Has implementation hints"
    },
    {
      "file": "cogzero/agentzero-communication/src/DialogueManager.cpp",
      "line": 656,
      "type": "TODO",
      "content": "// TODO: Implementation planned for AZ-NLP-002",
      "context": "\nstd::string DialogueManager::process_dialogue(const std::string& input)\n{\n    // TODO: Implementation planned for AZ-NLP-002\n    return \"DialogueManager not yet implemented - see AZ-NLP-002\";\n}\n\n",
      "description": "Implementation planned for AZ-NLP-002",
      "reason": "Has implementation hints"
    },
    {
      "file": "cogzero/agentzero-communication/src/DialogueManager.cpp",
      "line": 662,
      "type": "TODO",
      "content": "// TODO: Implementation planned for AZ-NLP-002",
      "context": "\nvoid DialogueManager::reset_context()\n{\n    // TODO: Implementation planned for AZ-NLP-002\n}\n\n} // namespace communication\n",
      "description": "Implementation planned for AZ-NLP-002",
      "reason": "Has implementation hints"
    },
    {
      "file": "cogzero/agentzero-tools/src/ToolWrapper.cpp",
      "line": 296,
      "type": "TODO",
      "content": "// TODO: Implement actual async execution using thread pool or async framework",
      "context": "    std::string exec_id = _tool_id + \"_\" + \n        std::to_string(std::chrono::system_clock::now().time_since_epoch().count());\n    \n    // TODO: Implement actual async execution using thread pool or async framework\n    // For now, this is a placeholder that would be implemented based on specific needs\n    \n    logger().warn() << \"[ToolWrapper] Async execution not yet fully implemented\";\n",
      "description": "Implement actual async execution using thread pool or async framework",
      "reason": "Has implementation hints"
    },
    {
      "file": "cogzero/agentzero-tools/src/ToolWrapper.cpp",
      "line": 310,
      "type": "TODO",
      "content": "// TODO: Implement actual REST API call",
      "context": "    \n    ToolResult result(ToolStatus::COMPLETED);\n    \n    // TODO: Implement actual REST API call\n    // This would use a library like libcurl or Boost.Beast to make HTTP requests\n    // For now, this is a placeholder implementation\n    \n",
      "description": "Implement actual REST API call",
      "reason": "Has implementation hints"
    },
    {
      "file": "cogzero/agentzero-tools/src/ToolWrapper.cpp",
      "line": 335,
      "type": "TODO",
      "content": "// TODO: Implement actual ROS topic/service interaction",
      "context": "    \n    ToolResult result(ToolStatus::COMPLETED);\n    \n    // TODO: Implement actual ROS topic/service interaction\n    // This would use ROS client libraries to publish/subscribe or call services\n    // For now, this is a placeholder implementation\n    \n",
      "description": "Implement actual ROS topic/service interaction",
      "reason": "Has implementation hints"
    },
    {
      "file": "cogzero/agentzero-tools/src/ToolWrapper.cpp",
      "line": 359,
      "type": "TODO",
      "content": "// TODO: Implement actual Python script execution",
      "context": "    \n    ToolResult result(ToolStatus::COMPLETED);\n    \n    // TODO: Implement actual Python script execution\n    // This would use system() call or better process management\n    // For now, this is a placeholder implementation\n    \n",
      "description": "Implement actual Python script execution",
      "reason": "Has implementation hints"
    },
    {
      "file": "cogzero/agentzero-tools/src/ToolWrapper.cpp",
      "line": 383,
      "type": "TODO",
      "content": "// TODO: Implement actual shell command execution with proper security",
      "context": "    \n    ToolResult result(ToolStatus::COMPLETED);\n    \n    // TODO: Implement actual shell command execution with proper security\n    // This would use popen() or better process management with timeout\n    // For now, this is a placeholder implementation\n    \n",
      "description": "Implement actual shell command execution with proper security",
      "reason": "Has implementation hints"
    },
    {
      "file": "atenspace/aten/src/ATen/cuda/CUDAMultiStreamGuard.h",
      "line": 12,
      "type": "TODO",
      "content": "// TODO: Implement this generically in c10.  You'll need some way to get",
      "context": "\nnamespace at { namespace cuda {\n\n// TODO: Implement this generically in c10.  You'll need some way to get\n// the number of GPUs from the GuardImpl, in that case.\nclass CUDAMultiStreamGuard final {\npublic:\n",
      "description": "Implement this generically in c10.  You'll need some way to get",
      "reason": "Has implementation hints"
    },
    {
      "file": "atenspace/aten/src/ATen/native/Embedding.cpp",
      "line": 50,
      "type": "TODO",
      "content": "// TODO: implement scale_grad_by_freq",
      "context": "  auto indices_arg = TensorArg(indices_, \"indices\", 2);\n  checkScalarType(\"embedding_backward\", indices_arg, kLong);\n\n  // TODO: implement scale_grad_by_freq\n  if (scale_grad_by_freq) {\n    AT_ERROR(\n        \"embedding_backward: scale_grad_by_freq not supported with sparse gradients\");\n",
      "description": "implement scale_grad_by_freq",
      "reason": "Has implementation hints"
    },
    {
      "file": "atenspace/aten/src/ATen/native/DispatchStub.h",
      "line": 33,
      "type": "stub",
      "content": "//   stub(kCPU, tensor);",
      "context": "//   REGISTER_DISPATCH(stub, &kernel);\n//\n// To call:\n//   stub(kCPU, tensor);\n//\n// TODO: CPU instruction set selection should be folded into whatever\n// the main dispatch mechanism is.\n",
      "description": "//   stub(kCPU, tensor);",
      "reason": "Simple stub implementation"
    },
    {
      "file": "atenspace/aten/src/ATen/native/UnaryOps.cpp",
      "line": 275,
      "type": "stub",
      "content": "// stub in CUDAUnaryOps",
      "context": "}\n\n// NB: If you use this macro, you may also need to add a CUDA forwarding\n// stub in CUDAUnaryOps\n\n#define IMPLEMENT_UNARY_OP_CORE(op)                                    \\\n  Tensor op(const Tensor& self) {                                      \\\n",
      "description": "// stub in CUDAUnaryOps",
      "reason": "Simple stub implementation"
    },
    {
      "file": "atenspace/aten/src/ATen/native/ReduceOps.cpp",
      "line": 333,
      "type": "TODO",
      "content": "// TODO: the TensorIterator reduction implementation of mean",
      "context": "      toString(scalarType),\n      \" instead.\");\n  ScalarType dtype = get_dtype(result, self, opt_dtype, true);\n  // TODO: the TensorIterator reduction implementation of mean\n  // (mean_kernel_impl()) is unvectorized and leads to very poor performance\n  // for production workloads. Once that's fixed, the following code can be used\n  // in lieu of the sum + divide implementation below.\n",
      "description": "the TensorIterator reduction implementation of mean",
      "reason": "Has implementation hints"
    },
    {
      "file": "atenspace/aten/src/ATen/quantized/Quantizer.cpp",
      "line": 230,
      "type": "TODO",
      "content": "// TODO Update quantize_tensor_arm implementation to follow quantize_val,",
      "context": "\n// Specialized implementation from caffe2::Int8Quantize.\n// There may be slight accuracy difference between this and implementation of quantize_val\n// TODO Update quantize_tensor_arm implementation to follow quantize_val,\n// i.e. f = Round(value/scale + zero_point)\n// TODO Make quantize_tensor_arm work for other datatypes too (int8, int32).\ntemplate <>\n",
      "description": "Update quantize_tensor_arm implementation to follow quantize_val,",
      "reason": "Has implementation hints"
    }
  ],
  "needs_research": [
    {
      "file": "language-learning/tests/test_grammar_learner.py",
      "line": 26,
      "type": "FIXME",
      "content": "def setUp(self):    # FIXME: should run before every test, but would not?!",
      "context": "\nclass TestGrammarLearner(unittest.TestCase):\n\n    def setUp(self):    # FIXME: should run before every test, but would not?!\n        input_parses = module_path + '/tests/data/POC-Turtle/MST_fixed_manually/'\n        batch_dir = module_path + '/output/Test_Grammar_Learner_' + str(UTC())[:10] + '/'\n        kwargs = {  # defaults\n",
      "description": "should run before every test, but would not?!"
    },
    {
      "file": "language-learning/tests/test_grammar_learner.py",
      "line": 65,
      "type": "FIXME",
      "content": "# 'template_path': 'poc-turtle',  # FIXME: changed in June 2018 Grammar Tester",
      "context": "        # Additional (optional) parameters for parse_metrics (_abiity & _quality):\n        # 'test_corpus': module_path + '/data/POC-Turtle/poc-turtle-corpus.txt',\n        # 'reference_path': module_path + '/data/POC-Turtle/poc-turtle-parses-expected.txt',\n        # 'template_path': 'poc-turtle',  # FIXME: changed in June 2018 Grammar Tester\n        pass\n\n    '''Legacy ~ POC.0.3 test ~ as it was before 2018-09-29\n",
      "description": "changed in June 2018 Grammar Tester"
    },
    {
      "file": "language-learning/tests/test_grammar_learner.py",
      "line": 212,
      "type": "TODO",
      "content": "# TODO: Remove these commented lines in next cleanup cycle.",
      "context": "        }\n        re = learn_grammar(**kwargs)\n        # NOTE: Legacy test code commented out for historical reference.\n        # TODO: Remove these commented lines in next cleanup cycle.\n        # a, q, qa = pqa_meter(re['grammar_file'], outpath, cp, rp, **kwargs)\n        # print('parse-ability, parse-quality:', a, q)\n        # assert a*q > 0.99\n",
      "description": "Remove these commented lines in next cleanup cycle."
    },
    {
      "file": "language-learning/tests/test_grammar_learner.py",
      "line": 310,
      "type": "FIXME",
      "content": "# FIXME: check with further test_grammar updates and delete.",
      "context": "            'verbose'       :   'min'\n        }\n        # Sometimes pqa_meter(with test_grammar updated 2018-10-19) returns pa,recall = 0,0\n        # FIXME: check with further test_grammar updates and delete.\n        x = 0.\n        n = 0\n        while x < 0.1 :\n",
      "description": "check with further test_grammar updates and delete."
    },
    {
      "file": "language-learning/src/grammar_learner/preprocessing.py",
      "line": 33,
      "type": "TODO",
      "content": "# TODO: cleanup here or in a separate constructor?",
      "context": "            if us[-1] != '\\n' :  us += '\\n'\n        us += s\n        if us[-1] != '\\n' :  us += '\\n'\n    # TODO: cleanup here or in a separate constructor?\n    re = OrderedDict([('read_files', UTC()),\n                      ('input_path', kwargs['input_path']),\n                      ('read_files_number', len(files)),\n",
      "description": "cleanup here or in a separate constructor?"
    },
    {
      "file": "language-learning/src/grammar_learner/preprocessing.py",
      "line": 141,
      "type": "FIXME",
      "content": "# else:  # FIXME: raise error / assert ?",
      "context": "    if 'corpus_stats' in re:\n        list2file(re['corpus_stats'], corpus_stats_file)\n        re.update({'corpus_stats_file': corpus_stats_file})\n    # else:  # FIXME: raise error / assert ?\n    #    return {'error': 'input_files'}, re\n\n    return links, re\n",
      "description": "raise error / assert ?"
    },
    {
      "file": "language-learning/src/grammar_learner/category_learner.py",
      "line": 32,
      "type": "FIXME",
      "content": "log = OrderedDict()  # FIXME: log \u00bb response",
      "context": "    algorithm = kwa('kmeans', 'clustering', **kwargs)\n    verbose = kwa('none', 'verbose', **kwargs)\n\n    log = OrderedDict()  # FIXME: log \u00bb response\n    log.update({'category_learner': 'v.0.7.81231'})\n\n    cdf = pd.DataFrame(columns = ['cluster', 'cluster_words'])\n",
      "description": "log \u00bb response"
    },
    {
      "file": "language-learning/src/grammar_learner/category_learner.py",
      "line": 53,
      "type": "FIXME",
      "content": "except Exception:  # FIXME",
      "context": "        try:\n            dim = vector_space_dim(links, dict_path, tmpath, dim_max, sv_min,\n                                   verbose)\n        except Exception:  # FIXME\n            dim = dim_max\n        log.update({'vector_space_dim': dim})\n\n",
      "description": ""
    },
    {
      "file": "language-learning/src/grammar_learner/hyperwords.py",
      "line": 27,
      "type": "FIXME",
      "content": "if cds != 1: sum_c = sum_c ** cds   # FIXME: cds = 1.0 ?!",
      "context": "def calc_pmi(counts, cds):  # Calculates e^PMI; PMI without the log().\n    sum_w = np.array(counts.sum(axis=1))[:, 0]\n    sum_c = np.array(counts.sum(axis=0))[0, :]\n    if cds != 1: sum_c = sum_c ** cds   # FIXME: cds = 1.0 ?!\n    sum_total = sum_c.sum()\n    sum_w = np.reciprocal(sum_w)\n    sum_c = np.reciprocal(sum_c)\n",
      "description": "cds = 1.0 ?!"
    },
    {
      "file": "language-learning/src/grammar_learner/hyperwords.py",
      "line": 138,
      "type": "FIXME",
      "content": "print('SVDEmbedding: transpose')    #FIXME:DEL",
      "context": "    # Context embeddings can be created with \"transpose\".\n    def __init__(self, path, normalize=True, eig=0.0, transpose=False):\n        if transpose:\n            print('SVDEmbedding: transpose')    #FIXME:DEL\n            ut = np.load(path + '.vt.npy')\n            self.wi, self.iw = load_vocabulary(path + '.contexts.vocab')\n        else:\n",
      "description": "DEL"
    },
    {
      "file": "language-learning/src/grammar_learner/hyperwords.py",
      "line": 358,
      "type": "TODO",
      "content": "svd = SVDEmbedding(svd_path, True, eig)   # TODO: move code here, RAM2RAM",
      "context": "    logger.info(f'SVD matrix (3 files .npy) saved: {len(ut[0])} vectors, ut: {len(ut)} s: {len(s)} vt:{len(vt)}')\n\n    '''SVD => vectors.txt'''\n    svd = SVDEmbedding(svd_path, True, eig)   # TODO: move code here, RAM2RAM\n    if len(svd.m[0]) < dim: dim = len(svd.m[0])   # 80216\n    vectors_df = pd.DataFrame(columns=['word'] + list(range(1,dim+1)))\n    for i, w in enumerate(svd.iw):\n",
      "description": "move code here, RAM2RAM"
    },
    {
      "file": "language-learning/src/grammar_learner/hyperwords.py",
      "line": 439,
      "type": "TODO",
      "content": "svd = SVDEmbedding(svd_path, True, eig)   # TODO: move code here, RAM2RAM",
      "context": "    list2tsv(explicit.ic, svd_path + '.contexts.vocab')\n\n    '''SVD => vectors.txt'''\n    svd = SVDEmbedding(svd_path, True, eig)   # TODO: move code here, RAM2RAM\n    if len(svd.m[0]) < dim: dim = len(svd.m[0])   # 80216\n    vectors_df = pd.DataFrame(columns=['word'] + list(range(1,dim+1)))\n    for i, w in enumerate(svd.iw):\n",
      "description": "move code here, RAM2RAM"
    },
    {
      "file": "language-learning/src/grammar_learner/hyperwords.py",
      "line": 466,
      "type": "TODO",
      "content": "# TODO: refactor, control disk writes, ... PPMI \u21d2 +frequency?",
      "context": "# Notes:\n\n# 80329 added vector_space_dim\n# TODO: refactor, control disk writes, ... PPMI \u21d2 +frequency?\n# 90221 minor updates for Grammar Learner tutorial\n",
      "description": "refactor, control disk writes, ... PPMI \u21d2 +frequency?"
    },
    {
      "file": "language-learning/src/grammar_learner/widgets.py",
      "line": 70,
      "type": "TODO",
      "content": "#  TODO: To be reviewed and changed if necessary",
      "context": "                tree.append(['', m+1, cats[j][2], cats[j][3]])\n        else:\n            print('WTF?', k, v)\n    #  TODO: To be reviewed and changed if necessary\n    if verbose not in ['none', 'min']:\n        display(html_table([['Code', 'Parent', 'Id', 'Words']] + tree))\n\n",
      "description": "To be reviewed and changed if necessary"
    },
    {
      "file": "language-learning/src/grammar_learner/clustering.py",
      "line": 361,
      "type": "TODO",
      "content": "# TODO: n_clusters \u21d2 best_clusters: return best clusters (word lists), centroids",
      "context": "# 80809 update: (30,60,3,[3]) - old range + repeat / (120,30,3) -- search opt\n# 80825 random_clusters\n# 81022 refactoring\n# TODO: n_clusters \u21d2 best_clusters: return best clusters (word lists), centroids\n# 81231 cleanup\n# 90104 resolve Turtle MST LW crash: 1 cluster\n# 90209 group_links: add min_word_count to 80925 legacy version\n",
      "description": "n_clusters \u21d2 best_clusters: return best clusters (word lists), centroids"
    },
    {
      "file": "language-learning/src/grammar_learner/skl_clustering.py",
      "line": 215,
      "type": "TODO",
      "content": "# TODO: int / dict",
      "context": "            if len(clustering) > 3:  # connectivity\n                if type(clustering[3]) is int and clustering[3] > 0:\n                    neighbors = clustering[3]\n                    # TODO: int / dict \n                    connectivity = kneighbors_graph(cd, neighbors,\n                                                    include_self=False)\n            if len(clustering) > 4:  # compute_full_tree\n",
      "description": "int / dict "
    },
    {
      "file": "language-learning/src/grammar_learner/skl_clustering.py",
      "line": 343,
      "type": "TODO",
      "content": "elif len(crange) == 3:  # TODO: replace with SGD?",
      "context": "                    l, m, c = skl_clustering(cd, crange[0], **kwargs)\n                    if m['silhouette_index'] > metrics['silhouette_index']:\n                        labels, metrics, centroids = l, m, c\n        elif len(crange) == 3:  # TODO: replace with SGD?\n            n_min = min(crange[0], crange[1])\n            n_max = max(crange[0], crange[1])\n            labels, metrics, centroids = \\\n",
      "description": "replace with SGD?"
    },
    {
      "file": "language-learning/src/grammar_learner/pqa_table.py",
      "line": 641,
      "type": "FIXME",
      "content": "continue  # FIXME: check case",
      "context": "                     linkage, affinity, gen, ' ---', 'fail',\n                     ' ---', ' ---', ' ---', ' ---', ' ---', ' ---']\n            details.append(dline)\n            continue  # FIXME: check case\n        if kwargs['linkage_limit'] > 0:\n            start = time.time()\n            a, f1, precision, q = pqa_meter(re['grammar_file'],\n",
      "description": "check case"
    },
    {
      "file": "scripts/implementation/fix_placeholders.py",
      "line": 74,
      "type": "TODO",
      "content": "// TODO: Enhance with specific logic as needed",
      "context": "            if return_type == 'void':\n                impl = f'''{return_type} {func_name}({match.group(0).split('(')[1].split(')')[0]}) {{\n    // Implementation added by automated code quality improvement\n    // TODO: Enhance with specific logic as needed\n    logger().debug(\"Executing {func_name}\");\n}}'''\n            elif return_type == 'bool':\n",
      "description": "Enhance with specific logic as needed"
    },
    {
      "file": "scripts/implementation/fix_placeholders.py",
      "line": 131,
      "type": "TODO",
      "content": "if '// TODO' in content and 'logger()' not in content:",
      "context": "    def add_logging(self, file_path, content):\n        \"\"\"Add logging statements where appropriate\"\"\"\n        # Add logging to functions that have TODO comments\n        if '// TODO' in content and 'logger()' not in content:\n            # Check if it's a .cc file (implementation)\n            if file_path.suffix == '.cc':\n                # Add include if not present\n",
      "description": "' in content and 'logger()' not in content:"
    },
    {
      "file": "scripts/implementation/print_resolution_card.py",
      "line": 108,
      "type": "TODO",
      "content": "// TODO: document this          /**",
      "context": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n  BEFORE                           AFTER\n  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  // TODO: document this          /**\n  void func() { ... }              * @brief Clear description\n                                   * @param x The input\n                                   * @return The result\n",
      "description": "document this          /**"
    },
    {
      "file": "scripts/implementation/print_resolution_card.py",
      "line": 119,
      "type": "TODO",
      "content": "// TODO: validate input          bool validate() {",
      "context": "  void experimental() { ... }      // under active development\n                                   void experimental() { ... }\n  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  // TODO: validate input          bool validate() {\n  bool validate() {                  if (!initialized_) return false;\n    return true;                     if (data_.empty()) return false;\n  }                                  return true;\n",
      "description": "validate input          bool validate() {"
    },
    {
      "file": "scripts/implementation/fixme_resolution_tracker.py",
      "line": 169,
      "type": "FIXME",
      "content": "report.append(\"# FIXME Resolution Progress Report\")",
      "context": "    def generate_next_steps_report(self) -> str:\n        \"\"\"Generate a report implementing the next steps from the catalog.\"\"\"\n        report = []\n        report.append(\"# FIXME Resolution Progress Report\")\n        report.append(f\"Generated: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n        report.append(\"\")\n        \n",
      "description": "Resolution Progress Report\")"
    },
    {
      "file": "scripts/testing/test_implementations.py",
      "line": 108,
      "type": "TODO",
      "content": "'pass  # TODO',",
      "context": "def verify_no_mock_implementations():\n    \"\"\"Verify we didn't create any mock implementations\"\"\"\n    mock_patterns = [\n        'pass  # TODO',\n        'pass  # FIXME',\n        'return None  # placeholder',\n        'raise NotImplementedError  # TODO'\n",
      "description": "',"
    },
    {
      "file": "scripts/testing/test_implementations.py",
      "line": 109,
      "type": "FIXME",
      "content": "'pass  # FIXME',",
      "context": "    \"\"\"Verify we didn't create any mock implementations\"\"\"\n    mock_patterns = [\n        'pass  # TODO',\n        'pass  # FIXME',\n        'return None  # placeholder',\n        'raise NotImplementedError  # TODO'\n    ]\n",
      "description": "',"
    },
    {
      "file": "scripts/testing/test_implementations.py",
      "line": 111,
      "type": "TODO",
      "content": "'raise NotImplementedError  # TODO'",
      "context": "        'pass  # TODO',\n        'pass  # FIXME',\n        'return None  # placeholder',\n        'raise NotImplementedError  # TODO'\n    ]\n    \n    violations = []\n",
      "description": "'"
    },
    {
      "file": "scripts/analysis/fragmentation_detector.py",
      "line": 22,
      "type": "TODO",
      "content": "marker_type: str  # TODO, FIXME, STUB",
      "context": "    \"\"\"Represents a specific code fragmentation\"\"\"\n    file_path: str\n    line_number: int\n    marker_type: str  # TODO, FIXME, STUB\n    content: str\n    context_before: List[str]\n    context_after: List[str]\n",
      "description": ", FIXME, STUB"
    },
    {
      "file": "scripts/analysis/analyze_issue_examples.py",
      "line": 14,
      "type": "FIXME",
      "content": "\"./atomspace/opencog/atomspace/Transient.cc:/// XXX FIXME. Performance has not been recently measured; there\",",
      "context": "    \n    issue_examples = [\n        \"./atomspace/examples/atomspace/queue.scm:; XXX FIXME, this example is not yet complete and does not yet work...\",\n        \"./atomspace/opencog/atomspace/Transient.cc:/// XXX FIXME. Performance has not been recently measured; there\",\n        \"./atomspace/opencog/atomspace/AtomTable.cc:    // atom in the parent. What??? XXX NOT TRUE FIXME\",\n        \"./atomspace/opencog/atomspace/AtomSpace.cc:\t// Fixme maybe later someday, if/when this is needed.\",\n        \"./atomspace/opencog/atomspace/AtomSpace.cc:// XXX FIXME -- The recursive design of the depth() routine below makes\",\n",
      "description": ". Performance has not been recently measured; there\","
    },
    {
      "file": "scripts/analysis/analyze_issue_examples.py",
      "line": 16,
      "type": "FIXME",
      "content": "\"./atomspace/opencog/atomspace/AtomSpace.cc:\t// Fixme maybe later someday, if/when this is needed.\",",
      "context": "        \"./atomspace/examples/atomspace/queue.scm:; XXX FIXME, this example is not yet complete and does not yet work...\",\n        \"./atomspace/opencog/atomspace/Transient.cc:/// XXX FIXME. Performance has not been recently measured; there\",\n        \"./atomspace/opencog/atomspace/AtomTable.cc:    // atom in the parent. What??? XXX NOT TRUE FIXME\",\n        \"./atomspace/opencog/atomspace/AtomSpace.cc:\t// Fixme maybe later someday, if/when this is needed.\",\n        \"./atomspace/opencog/atomspace/AtomSpace.cc:// XXX FIXME -- The recursive design of the depth() routine below makes\",\n        \"./atomspace/opencog/atomspace/AtomSpace.h:    // XXX FIXME Users should call StorageNode::add_nocheck() instead.\",\n        \"./atomspace/opencog/cython/PythonEval.cc:    // XXX FIXME this does a lot of wasteful string copying.\",\n",
      "description": "maybe later someday, if/when this is needed.\","
    },
    {
      "file": "scripts/analysis/analyze_issue_examples.py",
      "line": 17,
      "type": "FIXME",
      "content": "\"./atomspace/opencog/atomspace/AtomSpace.cc:// XXX FIXME -- The recursive design of the depth() routine below makes\",",
      "context": "        \"./atomspace/opencog/atomspace/Transient.cc:/// XXX FIXME. Performance has not been recently measured; there\",\n        \"./atomspace/opencog/atomspace/AtomTable.cc:    // atom in the parent. What??? XXX NOT TRUE FIXME\",\n        \"./atomspace/opencog/atomspace/AtomSpace.cc:\t// Fixme maybe later someday, if/when this is needed.\",\n        \"./atomspace/opencog/atomspace/AtomSpace.cc:// XXX FIXME -- The recursive design of the depth() routine below makes\",\n        \"./atomspace/opencog/atomspace/AtomSpace.h:    // XXX FIXME Users should call StorageNode::add_nocheck() instead.\",\n        \"./atomspace/opencog/cython/PythonEval.cc:    // XXX FIXME this does a lot of wasteful string copying.\",\n        \"./atomspace/opencog/cython/PyIncludeWrapper.h:// 0.15.1 and maybe other versions)  FIXME someday...\",\n",
      "description": "-- The recursive design of the depth() routine below makes\","
    },
    {
      "file": "scripts/analysis/analyze_issue_examples.py",
      "line": 18,
      "type": "FIXME",
      "content": "\"./atomspace/opencog/atomspace/AtomSpace.h:    // XXX FIXME Users should call StorageNode::add_nocheck() instead.\",",
      "context": "        \"./atomspace/opencog/atomspace/AtomTable.cc:    // atom in the parent. What??? XXX NOT TRUE FIXME\",\n        \"./atomspace/opencog/atomspace/AtomSpace.cc:\t// Fixme maybe later someday, if/when this is needed.\",\n        \"./atomspace/opencog/atomspace/AtomSpace.cc:// XXX FIXME -- The recursive design of the depth() routine below makes\",\n        \"./atomspace/opencog/atomspace/AtomSpace.h:    // XXX FIXME Users should call StorageNode::add_nocheck() instead.\",\n        \"./atomspace/opencog/cython/PythonEval.cc:    // XXX FIXME this does a lot of wasteful string copying.\",\n        \"./atomspace/opencog/cython/PyIncludeWrapper.h:// 0.15.1 and maybe other versions)  FIXME someday...\",\n        \"./atomspace/opencog/haskell/AtomSpace_CWrapper.h:     * XXX FIXME no one should be using Handle's to work with atoms,\",\n",
      "description": "Users should call StorageNode::add_nocheck() instead.\","
    },
    {
      "file": "scripts/analysis/analyze_issue_examples.py",
      "line": 19,
      "type": "FIXME",
      "content": "\"./atomspace/opencog/cython/PythonEval.cc:    // XXX FIXME this does a lot of wasteful string copying.\",",
      "context": "        \"./atomspace/opencog/atomspace/AtomSpace.cc:\t// Fixme maybe later someday, if/when this is needed.\",\n        \"./atomspace/opencog/atomspace/AtomSpace.cc:// XXX FIXME -- The recursive design of the depth() routine below makes\",\n        \"./atomspace/opencog/atomspace/AtomSpace.h:    // XXX FIXME Users should call StorageNode::add_nocheck() instead.\",\n        \"./atomspace/opencog/cython/PythonEval.cc:    // XXX FIXME this does a lot of wasteful string copying.\",\n        \"./atomspace/opencog/cython/PyIncludeWrapper.h:// 0.15.1 and maybe other versions)  FIXME someday...\",\n        \"./atomspace/opencog/haskell/AtomSpace_CWrapper.h:     * XXX FIXME no one should be using Handle's to work with atoms,\",\n        \"./atomspace/opencog/haskell/PatternMatcher_CWrapper.h: * XXX FIXME: atoms must never be accessed by UUID except by the\",\n",
      "description": "this does a lot of wasteful string copying.\","
    },
    {
      "file": "scripts/analysis/analyze_issue_examples.py",
      "line": 23,
      "type": "FIXME",
      "content": "\"./atomspace/opencog/ocaml/CamlWrap.cc:\t// XXX FIXME\",",
      "context": "        \"./atomspace/opencog/cython/PyIncludeWrapper.h:// 0.15.1 and maybe other versions)  FIXME someday...\",\n        \"./atomspace/opencog/haskell/AtomSpace_CWrapper.h:     * XXX FIXME no one should be using Handle's to work with atoms,\",\n        \"./atomspace/opencog/haskell/PatternMatcher_CWrapper.h: * XXX FIXME: atoms must never be accessed by UUID except by the\",\n        \"./atomspace/opencog/ocaml/CamlWrap.cc:\t// XXX FIXME\",\n        \"./atomspace/opencog/guile/SchemeSmobAS.cc: * until a better permission system is invented. XXX FIXME.\",\n        \"./atomspace/opencog/guile/modules/ExecSCM.cc:// XXX FIXME: can we fix cython to not do this, already?\",\n        \"./atomspace/opencog/guile/SchemeSmobAtom.cc:// XXX FIXME. Work around the despicable, horrible guile UTF8 handling.\",\n",
      "description": "\","
    },
    {
      "file": "scripts/analysis/analyze_issue_examples.py",
      "line": 25,
      "type": "FIXME",
      "content": "\"./atomspace/opencog/guile/modules/ExecSCM.cc:// XXX FIXME: can we fix cython to not do this, already?\",",
      "context": "        \"./atomspace/opencog/haskell/PatternMatcher_CWrapper.h: * XXX FIXME: atoms must never be accessed by UUID except by the\",\n        \"./atomspace/opencog/ocaml/CamlWrap.cc:\t// XXX FIXME\",\n        \"./atomspace/opencog/guile/SchemeSmobAS.cc: * until a better permission system is invented. XXX FIXME.\",\n        \"./atomspace/opencog/guile/modules/ExecSCM.cc:// XXX FIXME: can we fix cython to not do this, already?\",\n        \"./atomspace/opencog/guile/SchemeSmobAtom.cc:// XXX FIXME. Work around the despicable, horrible guile UTF8 handling.\",\n        \"./atomspace/opencog/guile/SchemeSmobValue.cc: * XXX FIXME Clearly, a factory for values is called for.\",\n        \"./atomspace/opencog/guile/SchemeEval.cc:\t// XXX FIXME This lock is not needed, because in guile-2.2,\",\n",
      "description": "can we fix cython to not do this, already?\","
    },
    {
      "file": "scripts/analysis/analyze_issue_examples.py",
      "line": 26,
      "type": "FIXME",
      "content": "\"./atomspace/opencog/guile/SchemeSmobAtom.cc:// XXX FIXME. Work around the despicable, horrible guile UTF8 handling.\",",
      "context": "        \"./atomspace/opencog/ocaml/CamlWrap.cc:\t// XXX FIXME\",\n        \"./atomspace/opencog/guile/SchemeSmobAS.cc: * until a better permission system is invented. XXX FIXME.\",\n        \"./atomspace/opencog/guile/modules/ExecSCM.cc:// XXX FIXME: can we fix cython to not do this, already?\",\n        \"./atomspace/opencog/guile/SchemeSmobAtom.cc:// XXX FIXME. Work around the despicable, horrible guile UTF8 handling.\",\n        \"./atomspace/opencog/guile/SchemeSmobValue.cc: * XXX FIXME Clearly, a factory for values is called for.\",\n        \"./atomspace/opencog/guile/SchemeEval.cc:\t// XXX FIXME This lock is not needed, because in guile-2.2,\",\n        \"./atomspace/opencog/sheaf/attic/linear-parser.scm:  XXX FIXME WARNING DANGER: As written, this runs in exponential time\",\n",
      "description": ". Work around the despicable, horrible guile UTF8 handling.\","
    },
    {
      "file": "scripts/analysis/analyze_issue_examples.py",
      "line": 28,
      "type": "FIXME",
      "content": "\"./atomspace/opencog/guile/SchemeEval.cc:\t// XXX FIXME This lock is not needed, because in guile-2.2,\",",
      "context": "        \"./atomspace/opencog/guile/modules/ExecSCM.cc:// XXX FIXME: can we fix cython to not do this, already?\",\n        \"./atomspace/opencog/guile/SchemeSmobAtom.cc:// XXX FIXME. Work around the despicable, horrible guile UTF8 handling.\",\n        \"./atomspace/opencog/guile/SchemeSmobValue.cc: * XXX FIXME Clearly, a factory for values is called for.\",\n        \"./atomspace/opencog/guile/SchemeEval.cc:\t// XXX FIXME This lock is not needed, because in guile-2.2,\",\n        \"./atomspace/opencog/sheaf/attic/linear-parser.scm:  XXX FIXME WARNING DANGER: As written, this runs in exponential time\",\n        \"./atomspace/opencog/atoms/pattern/PatternUtils.cc:\t\t// XXX FIXME Are the below needed?\",\n        \"./atomspace/opencog/atoms/pattern/BindLink.cc:\t// Shoot. XXX FIXME. Most of the unit tests require that the atom\",\n",
      "description": "This lock is not needed, because in guile-2.2,\","
    },
    {
      "file": "scripts/analysis/analyze_issue_examples.py",
      "line": 30,
      "type": "FIXME",
      "content": "\"./atomspace/opencog/atoms/pattern/PatternUtils.cc:\t\t// XXX FIXME Are the below needed?\",",
      "context": "        \"./atomspace/opencog/guile/SchemeSmobValue.cc: * XXX FIXME Clearly, a factory for values is called for.\",\n        \"./atomspace/opencog/guile/SchemeEval.cc:\t// XXX FIXME This lock is not needed, because in guile-2.2,\",\n        \"./atomspace/opencog/sheaf/attic/linear-parser.scm:  XXX FIXME WARNING DANGER: As written, this runs in exponential time\",\n        \"./atomspace/opencog/atoms/pattern/PatternUtils.cc:\t\t// XXX FIXME Are the below needed?\",\n        \"./atomspace/opencog/atoms/pattern/BindLink.cc:\t// Shoot. XXX FIXME. Most of the unit tests require that the atom\",\n        \"./atomspace/opencog/atoms/pattern/PatternLink.cc:\t\t// XXX FIXME, more correct would be to loop over\",\n        \"./atomspace/opencog/atoms/truthvalue/FormulaTruthValue.cc:// XXX FIXME This update is not thread-safe.\",\n",
      "description": "Are the below needed?\","
    },
    {
      "file": "scripts/analysis/analyze_issue_examples.py",
      "line": 32,
      "type": "FIXME",
      "content": "\"./atomspace/opencog/atoms/pattern/PatternLink.cc:\t\t// XXX FIXME, more correct would be to loop over\",",
      "context": "        \"./atomspace/opencog/sheaf/attic/linear-parser.scm:  XXX FIXME WARNING DANGER: As written, this runs in exponential time\",\n        \"./atomspace/opencog/atoms/pattern/PatternUtils.cc:\t\t// XXX FIXME Are the below needed?\",\n        \"./atomspace/opencog/atoms/pattern/BindLink.cc:\t// Shoot. XXX FIXME. Most of the unit tests require that the atom\",\n        \"./atomspace/opencog/atoms/pattern/PatternLink.cc:\t\t// XXX FIXME, more correct would be to loop over\",\n        \"./atomspace/opencog/atoms/truthvalue/FormulaTruthValue.cc:// XXX FIXME This update is not thread-safe.\",\n        \"./atomspace/opencog/atoms/core/TypeChoice.cc:\t\t// For now, just avoid throwing an exception. XXX FIXME.\",\n        \"./atomspace/opencog/atoms/core/RandomChoice.cc:// XXX FIXME - fix this so it can also choose a single value\",\n",
      "description": ", more correct would be to loop over\","
    },
    {
      "file": "scripts/analysis/analyze_issue_examples.py",
      "line": 33,
      "type": "FIXME",
      "content": "\"./atomspace/opencog/atoms/truthvalue/FormulaTruthValue.cc:// XXX FIXME This update is not thread-safe.\",",
      "context": "        \"./atomspace/opencog/atoms/pattern/PatternUtils.cc:\t\t// XXX FIXME Are the below needed?\",\n        \"./atomspace/opencog/atoms/pattern/BindLink.cc:\t// Shoot. XXX FIXME. Most of the unit tests require that the atom\",\n        \"./atomspace/opencog/atoms/pattern/PatternLink.cc:\t\t// XXX FIXME, more correct would be to loop over\",\n        \"./atomspace/opencog/atoms/truthvalue/FormulaTruthValue.cc:// XXX FIXME This update is not thread-safe.\",\n        \"./atomspace/opencog/atoms/core/TypeChoice.cc:\t\t// For now, just avoid throwing an exception. XXX FIXME.\",\n        \"./atomspace/opencog/atoms/core/RandomChoice.cc:// XXX FIXME - fix this so it can also choose a single value\",\n        \"./atomspace/opencog/atoms/core/Variables.cc:\t// XXX FIXME URE calls us with broken handle!!\",\n",
      "description": "This update is not thread-safe.\","
    },
    {
      "file": "scripts/analysis/analyze_issue_examples.py",
      "line": 35,
      "type": "FIXME",
      "content": "\"./atomspace/opencog/atoms/core/RandomChoice.cc:// XXX FIXME - fix this so it can also choose a single value\",",
      "context": "        \"./atomspace/opencog/atoms/pattern/PatternLink.cc:\t\t// XXX FIXME, more correct would be to loop over\",\n        \"./atomspace/opencog/atoms/truthvalue/FormulaTruthValue.cc:// XXX FIXME This update is not thread-safe.\",\n        \"./atomspace/opencog/atoms/core/TypeChoice.cc:\t\t// For now, just avoid throwing an exception. XXX FIXME.\",\n        \"./atomspace/opencog/atoms/core/RandomChoice.cc:// XXX FIXME - fix this so it can also choose a single value\",\n        \"./atomspace/opencog/atoms/core/Variables.cc:\t// XXX FIXME URE calls us with broken handle!!\",\n        \"./atomspace/opencog/atoms/core/TypeUtils.cc:\t\t\t\t\\\"Not implemented! TODO XXX FIXME\\\");\",\n        \"./atomspace/opencog/atoms/value/FormulaStream.cc:// XXX FIXME The update here is not thread-safe...\",\n",
      "description": "- fix this so it can also choose a single value\","
    },
    {
      "file": "scripts/analysis/analyze_issue_examples.py",
      "line": 36,
      "type": "FIXME",
      "content": "\"./atomspace/opencog/atoms/core/Variables.cc:\t// XXX FIXME URE calls us with broken handle!!\",",
      "context": "        \"./atomspace/opencog/atoms/truthvalue/FormulaTruthValue.cc:// XXX FIXME This update is not thread-safe.\",\n        \"./atomspace/opencog/atoms/core/TypeChoice.cc:\t\t// For now, just avoid throwing an exception. XXX FIXME.\",\n        \"./atomspace/opencog/atoms/core/RandomChoice.cc:// XXX FIXME - fix this so it can also choose a single value\",\n        \"./atomspace/opencog/atoms/core/Variables.cc:\t// XXX FIXME URE calls us with broken handle!!\",\n        \"./atomspace/opencog/atoms/core/TypeUtils.cc:\t\t\t\t\\\"Not implemented! TODO XXX FIXME\\\");\",\n        \"./atomspace/opencog/atoms/value/FormulaStream.cc:// XXX FIXME The update here is not thread-safe...\",\n        \"./atomspace/opencog/atoms/execution/Instantiator.cc:/// cleanly separated from each other. (XXX FIXME, these need to be\",\n",
      "description": "URE calls us with broken handle!!\","
    },
    {
      "file": "scripts/analysis/analyze_issue_examples.py",
      "line": 38,
      "type": "FIXME",
      "content": "\"./atomspace/opencog/atoms/value/FormulaStream.cc:// XXX FIXME The update here is not thread-safe...\",",
      "context": "        \"./atomspace/opencog/atoms/core/RandomChoice.cc:// XXX FIXME - fix this so it can also choose a single value\",\n        \"./atomspace/opencog/atoms/core/Variables.cc:\t// XXX FIXME URE calls us with broken handle!!\",\n        \"./atomspace/opencog/atoms/core/TypeUtils.cc:\t\t\t\t\\\"Not implemented! TODO XXX FIXME\\\");\",\n        \"./atomspace/opencog/atoms/value/FormulaStream.cc:// XXX FIXME The update here is not thread-safe...\",\n        \"./atomspace/opencog/atoms/execution/Instantiator.cc:/// cleanly separated from each other. (XXX FIXME, these need to be\",\n        \"./atomspace/opencog/atoms/join/JoinLink.cc:/// i.e. to use UnorderedHandleSet instead of HandleSet. XXX FIXME.\",\n        \"./atomspace/opencog/atoms/flow/FilterLink.cc:\t\t\t\t\t\t\\\"Globbing for Values not implemented! FIXME!\\\");\",\n",
      "description": "The update here is not thread-safe...\","
    },
    {
      "file": "scripts/analysis/analyze_issue_examples.py",
      "line": 42,
      "type": "FIXME",
      "content": "\"./atomspace/opencog/query/InitiateSearchMixin.cc:\t\t// XXX FIXME; we should be using ptm->isVariable() instead !?\",",
      "context": "        \"./atomspace/opencog/atoms/execution/Instantiator.cc:/// cleanly separated from each other. (XXX FIXME, these need to be\",\n        \"./atomspace/opencog/atoms/join/JoinLink.cc:/// i.e. to use UnorderedHandleSet instead of HandleSet. XXX FIXME.\",\n        \"./atomspace/opencog/atoms/flow/FilterLink.cc:\t\t\t\t\t\t\\\"Globbing for Values not implemented! FIXME!\\\");\",\n        \"./atomspace/opencog/query/InitiateSearchMixin.cc:\t\t// XXX FIXME; we should be using ptm->isVariable() instead !?\",\n        \"./atomspace/opencog/query/RewriteMixin.cc:\t// See issue #950 and pull req #962. XXX FIXME later.\",\n        \"./atomspace/opencog/query/PatternMatchEngine.cc:/// XXX FIXME: this is currently a weak stop-gap measure to handle\"\n    ]\n",
      "description": "; we should be using ptm->isVariable() instead !?\","
    },
    {
      "file": "scripts/analysis/analyze_issue_examples.py",
      "line": 44,
      "type": "FIXME",
      "content": "\"./atomspace/opencog/query/PatternMatchEngine.cc:/// XXX FIXME: this is currently a weak stop-gap measure to handle\"",
      "context": "        \"./atomspace/opencog/atoms/flow/FilterLink.cc:\t\t\t\t\t\t\\\"Globbing for Values not implemented! FIXME!\\\");\",\n        \"./atomspace/opencog/query/InitiateSearchMixin.cc:\t\t// XXX FIXME; we should be using ptm->isVariable() instead !?\",\n        \"./atomspace/opencog/query/RewriteMixin.cc:\t// See issue #950 and pull req #962. XXX FIXME later.\",\n        \"./atomspace/opencog/query/PatternMatchEngine.cc:/// XXX FIXME: this is currently a weak stop-gap measure to handle\"\n    ]\n    \n    # Categorize these specific examples\n",
      "description": "this is currently a weak stop-gap measure to handle\""
    },
    {
      "file": "scripts/entelechy/entelechy_marker_analyzer.py",
      "line": 31,
      "type": "TODO",
      "content": "marker_type: str  # TODO, FIXME, STUB, PLACEHOLDER, etc.",
      "context": "    \"\"\"Represents a single code marker (TODO, FIXME, etc.).\"\"\"\n    file_path: str\n    line_number: int\n    marker_type: str  # TODO, FIXME, STUB, PLACEHOLDER, etc.\n    content: str\n    context_before: List[str]\n    context_after: List[str]\n",
      "description": ", FIXME, STUB, PLACEHOLDER, etc."
    },
    {
      "file": "scripts/entelechy/entelechy_introspection.py",
      "line": 600,
      "type": "TODO",
      "content": "r'TODO.*list',  # TODO lists in docs",
      "context": "        \n        # Patterns to exclude (informational, not actionable)\n        exclude_patterns = [\n            r'TODO.*list',  # TODO lists in docs\n            r'GNU',         # GNU license references\n            r'Copyright',   # Copyright notices\n            r'from\\s+TODO', # References to TODO files/modules\n",
      "description": "lists in docs"
    },
    {
      "file": "moses/moses/moses/optimization/star-anneal.cc",
      "line": 44,
      "type": "TODO",
      "content": "// XXX TODO the annealing temperature control code should be ported over",
      "context": "// Star-shaped search  //\n/////////////////////////\n\n// XXX TODO the annealing temperature control code should be ported over\n// to the hill-climbing code, thus rendering the below obsolete.  The\n// hill-climbing code is much more sophisticated in every way: correct\n// definition of the temperature, termination conditions, exploration of\n",
      "description": "the annealing temperature control code should be ported over"
    },
    {
      "file": "moses/moses/moses/optimization/particle-swarm.h",
      "line": 347,
      "type": "TODO",
      "content": "// TODO: Wind dispersion, but test without first",
      "context": "    void update_cont_particle(instance& temp, const instance& personal,\n            const instance& global, velocity::iterator vel, const field_set& fs);\n\n    // TODO: Wind dispersion, but test without first\n    // Make it later is easy.\n\npublic:\n",
      "description": "Wind dispersion, but test without first"
    },
    {
      "file": "moses/moses/moses/optimization/hill-climbing.h",
      "line": 110,
      "type": "TODO",
      "content": "// XXX TODO make sure this value is appropriately updated.",
      "context": "\n    // Range of scores for which to keep instances.  This *should* be\n    // set to the value given by metapopulation::useful_score_range().\n    // XXX TODO make sure this value is appropriately updated.\n    //\n    // The range of scores is used to keep the size of the deme in check.\n    // The issue is that, for large feature sets, a large number of knobs\n",
      "description": "make sure this value is appropriately updated."
    },
    {
      "file": "moses/moses/moses/optimization/particle-swarm.cc",
      "line": 198,
      "type": "TODO",
      "content": "// TODO: work in a better way to identify convergence.",
      "context": "            break;\n        }\n\n        // TODO: work in a better way to identify convergence.\n        not_improving = (has_improved) ? 0 : not_improving + 1;\n        if (not_improving > 3) {\n            logger().debug(\"Terminate Local Search: Convergence.\");\n",
      "description": "work in a better way to identify convergence."
    },
    {
      "file": "moses/moses/moses/optimization/particle-swarm.cc",
      "line": 237,
      "type": "TODO",
      "content": "// TODO: Explanation",
      "context": "        \"complexity\";\n}\n\n// TODO: Explanation\n// There's no explanation for this, it's just a temporary solution.\n// Maybe use adaptative pso, something like LPSO (Lander).\nunsigned particle_swarm::calc_swarm_size(const field_set& fs) {\n",
      "description": "Explanation"
    },
    {
      "file": "moses/moses/moses/metapopulation/metapopulation.h",
      "line": 535,
      "type": "TODO",
      "content": "// TODO: we may want to output the visited status as well",
      "context": "    // metapopulation. This function is used for fine logging to\n    // deeply probe the metapopulation.\n    //\n    // TODO: we may want to output the visited status as well\n    std::ostream& ostream_metapop(std::ostream&, int n = INT_MAX) const;\n\nprivate:\n",
      "description": "we may want to output the visited status as well"
    },
    {
      "file": "moses/moses/moses/metapopulation/merging.cc",
      "line": 261,
      "type": "FIXME",
      "content": "// XXX FIXME: we should use a pointer set for scored_combo_tree_set",
      "context": "        logger().debug(\"Compute behavioral score of %d selected candidates\",\n                       candidates.size());\n\n        // XXX FIXME: we should use a pointer set for scored_combo_tree_set\n        // This would avoid some pointless copying here and a few other\n        // places.  This is easier said than done, because the stupid\n        // domination code is so snarky and icky.  Domination should die.\n",
      "description": "we should use a pointer set for scored_combo_tree_set"
    },
    {
      "file": "moses/moses/moses/metapopulation/merging.cc",
      "line": 404,
      "type": "TODO",
      "content": "// TODO: Make population cap size-sensitive to exemplar complexity.",
      "context": "    // formula was arrived at via some ad-hoc experimentation.  A default\n    // value of _params.cap_coef=50 seems to work well.\n    //\n    // TODO: Make population cap size-sensitive to exemplar complexity.\n    // Large exemplars should result in smaller population sizes to maintain\n    // efficiency. Consider implementing adaptive sizing based on exemplar metrics.\n    //\n",
      "description": "Make population cap size-sensitive to exemplar complexity."
    },
    {
      "file": "moses/moses/moses/metapopulation/merging.cc",
      "line": 552,
      "type": "FIXME",
      "content": "// XXX FIXME looks to me like it++ can often be collaed twice within this loop!",
      "context": "                    }\n                }\n\n// XXX FIXME looks to me like it++ can often be collaed twice within this loop!\n                prev_it = it++;\n            }\n\n",
      "description": "looks to me like it++ can often be collaed twice within this loop!"
    },
    {
      "file": "moses/moses/moses/metapopulation/metapopulation.cc",
      "line": 222,
      "type": "FIXME",
      "content": "// XXX FIXME should probably not recompute every time ...",
      "context": "    if (not _params.do_boosting)\n        return _best_cscore;\n\n    // XXX FIXME should probably not recompute every time ...\n    // need to figure who is calling this method, and what they are expecting.\n    return _cscorer.get_cscore(_ensemble.get_ensemble());\n}\n",
      "description": "should probably not recompute every time ..."
    },
    {
      "file": "moses/moses/moses/scoring/scoring_base.h",
      "line": 124,
      "type": "TODO",
      "content": "// XXX TODO should be a std::valarray not a vector.",
      "context": "\n    /// A vector of per-bscore weights, used to tote up the behavioral\n    /// score into a single number.\n    // XXX TODO should be a std::valarray not a vector.\n    virtual void update_weights(const std::vector<double>&);\n\n    /// Return the amount by which the bscore differs from a perfect\n",
      "description": "should be a std::valarray not a vector."
    },
    {
      "file": "moses/moses/moses/scoring/time_dispersion.cc",
      "line": 43,
      "type": "TODO",
      "content": "// TODO multipler other than 1 is not supported yet",
      "context": "      _granularity(granularity), _multiplier(multiplier),\n      _pressure(time_dispersion_pressure), _exponent(time_dispersion_exponent)\n{\n    // TODO multipler other than 1 is not supported yet\n    OC_ASSERT(_multiplier == 1, \"Multiplier other than 1 is not supported yet\");\n\n    // Set of timestamp classes\n",
      "description": "multipler other than 1 is not supported yet"
    },
    {
      "file": "moses/moses/moses/scoring/scoring_base.cc",
      "line": 152,
      "type": "FIXME",
      "content": "// XXX FIXME complexity_t should be a double not an int ...",
      "context": "        norm += w;\n    }\n\n    // XXX FIXME complexity_t should be a double not an int ...\n    return (complexity_t) floor (cpxy / norm + 0.5);\n}\n\n",
      "description": "complexity_t should be a double not an int ..."
    },
    {
      "file": "moses/moses/moses/scoring/discriminating_bscore.cc",
      "line": 512,
      "type": "TODO",
      "content": "// XXX TODO -- should not return the penalties as part of the bscore,",
      "context": "                  float hardness)\n    : discriminating_bscore(ct, min_recall, max_recall, hardness)\n{\n    // XXX TODO -- should not return the penalties as part of the bscore,\n    // since this messes up boosting.\n    _size = ct.size() + 2;\n}\n",
      "description": "-- should not return the penalties as part of the bscore,"
    },
    {
      "file": "moses/moses/moses/scoring/discriminating_bscore.cc",
      "line": 646,
      "type": "TODO",
      "content": "// XXX TODO FIXME is this really correct?",
      "context": "/// Return the break-even-point for this ctable row.\nscore_t bep_bscore::get_variable(score_t pos, score_t neg, unsigned cnt) const\n{\n    // XXX TODO FIXME is this really correct?\n    double best_possible_precision = pos / (cnt * _true_total);\n    double best_possible_recall = 1.0 / _true_total;\n    return (best_possible_precision + best_possible_recall) / 2;\n",
      "description": "FIXME is this really correct?"
    },
    {
      "file": "moses/moses/moses/scoring/discriminating_bscore.cc",
      "line": 655,
      "type": "TODO",
      "content": "// XXX TODO FIXME is this really correct?",
      "context": "/// Return the difference for this ctable row.\nscore_t bep_bscore::get_fixed(score_t pos, score_t neg, unsigned cnt) const\n{\n    // XXX TODO FIXME is this really correct?\n    double best_possible_precision = pos / (cnt);\n    double best_possible_recall = (0.0 < pos) ? 1.0 : 0.0;\n    return fabs(best_possible_precision - best_possible_recall);\n",
      "description": "FIXME is this really correct?"
    },
    {
      "file": "moses/moses/moses/scoring/discriminating_bscore.cc",
      "line": 707,
      "type": "TODO",
      "content": "// XXX TODO FIXME is this really correct?",
      "context": "// generation of best-possible score.\nscore_t f_one_bscore::get_fixed(score_t pos, score_t neg, unsigned cnt) const\n{\n    // XXX TODO FIXME is this really correct?\n    return 1.0;\n}\n\n",
      "description": "FIXME is this really correct?"
    },
    {
      "file": "moses/moses/moses/scoring/discriminating_bscore.cc",
      "line": 714,
      "type": "TODO",
      "content": "// XXX TODO FIXME is this really correct?",
      "context": "/// Return the f_one for this ctable row.\nscore_t f_one_bscore::get_variable(score_t pos, score_t neg, unsigned cnt) const\n{\n    // XXX TODO FIXME is this really correct?\n    double best_possible_precision = pos / cnt;\n    double best_possible_recall = 1.0;\n    double f_one = 2 * best_possible_precision * best_possible_recall\n",
      "description": "FIXME is this really correct?"
    },
    {
      "file": "moses/moses/moses/moses/local_moses.cc",
      "line": 180,
      "type": "TODO",
      "content": "// TODO use the option of the output",
      "context": "                   << \"\\t\" << ds.max;  // max distance\n\n                // diversity stats over all best n candidates of the metapopulation\n                // TODO use the option of the output\n                auto best_ds = mp.gather_diversity_stats(pa.max_cnd_output);\n                ss << \"\\t\" << best_ds.count // number of pairs of candidates\n                   << \"\\t\" << best_ds.mean  // average distance\n",
      "description": "use the option of the output"
    },
    {
      "file": "moses/moses/moses/moses/partial.cc",
      "line": 96,
      "type": "TODO",
      "content": "// TODO: Improve generation tracking by getting actual number",
      "context": "\n        _moses_params.max_evals -= _num_evals;\n\n        // TODO: Improve generation tracking by getting actual number\n        // of generations run from MOSES and subtracting it here.\n        // Currently no easy API exists to retrieve this information.\n        _moses_params.max_gens -= _num_gens;\n",
      "description": "Improve generation tracking by getting actual number"
    },
    {
      "file": "moses/moses/moses/moses/neighborhood_sampling.h",
      "line": 327,
      "type": "TODO",
      "content": "// XXX TODO, unroll the last tail call, just like the single-bit",
      "context": "        else\n        {\n            // Recursive call, moved for one position\n            // XXX TODO, unroll the last tail call, just like the single-bit\n            // knob case, below.\n            out = vary_n_knobs(fs, tmp_inst, dist, starting_index + 1, out, end);\n            // Left<->Right\n",
      "description": ", unroll the last tail call, just like the single-bit"
    },
    {
      "file": "moses/moses/moses/moses/moses_main.h",
      "line": 102,
      "type": "TODO",
      "content": "// XXX TODO this should be fixed, someday...",
      "context": "        // messages.  In fact, the mpi workers should not even have\n        // a printer at all, or use a null_printer.  Unfortunately,\n        // the current code structure makes this hard to implement.\n        // XXX TODO this should be fixed, someday...\n        if (is_mpi && metapop.size() == 0)\n            return;\n\n",
      "description": "this should be fixed, someday..."
    },
    {
      "file": "moses/moses/moses/moses/mpi_moses.cc",
      "line": 202,
      "type": "TODO",
      "content": "// XXX TODO -- trim the deme down, before sending, by using the worst acceptable score.",
      "context": "/// send_deme -- send the completed deme from the worker back to root\n///\n/// This sends a pretty big glob.\n// XXX TODO -- trim the deme down, before sending, by using the worst acceptable score.\nvoid moses_mpi_comm::send_deme(const metapopulation& mp, int n_evals)\n{\n    MPI::COMM_WORLD.Send(&n_evals, 1, MPI::INT, ROOT_NODE, MSG_NUM_EVALS);\n",
      "description": "-- trim the deme down, before sending, by using the worst acceptable score."
    },
    {
      "file": "moses/moses/moses/moses/mpi_moses.cc",
      "line": 318,
      "type": "TODO",
      "content": "// XXX TODO should probably fetch max_time from somewhere...",
      "context": "            continue; // Continue to next exemplar\n        }\n\n        // XXX TODO should probably fetch max_time from somewhere...\n        time_t max_time = INT_MAX;\n        dex.optimize_demes(max_evals, max_time);\n\n",
      "description": "should probably fetch max_time from somewhere..."
    },
    {
      "file": "moses/moses/moses/moses/mpi_moses.cc",
      "line": 486,
      "type": "TODO",
      "content": "// TODO: Optimize statistics printing frequency to reduce output volume.",
      "context": "                thread_count--;\n                });\n\n// TODO: Optimize statistics printing frequency to reduce output volume.\n        // Consider printing detailed stats every N iterations instead of every iteration.\n        // Print stats in a way that makes them easy to graph.\n        // (columns of tab-seprated numbers)\n",
      "description": "Optimize statistics printing frequency to reduce output volume."
    },
    {
      "file": "moses/moses/moses/moses/mpi_moses.cc",
      "line": 616,
      "type": "TODO",
      "content": "// XXX TODO instead of overwritting the demeID it should be",
      "context": "        scored_combo_tree_set candidates;\n        stats.n_expansions ++;\n\n        // XXX TODO instead of overwritting the demeID it should be\n        // correctly defined by the worker and send back to the\n        // dispatcher. That way we can have the breadth_first\n        // componant of the demeID right.\n",
      "description": "instead of overwritting the demeID it should be"
    },
    {
      "file": "moses/moses/moses/moses/types.h",
      "line": 210,
      "type": "TODO",
      "content": "// TODO this should be a std::valarray not std::vector but I am too",
      "context": "/// in reference to a particular table of data.  Exactly which tree it\n/// is, and which table, is implicit.\n//\n// TODO this should be a std::valarray not std::vector but I am too\n// lazy to make the switch right now.\nstruct behavioral_score : public std::vector<score_t>\n{\n",
      "description": "this should be a std::valarray not std::vector but I am too"
    },
    {
      "file": "moses/moses/moses/deme/deme_expander.cc",
      "line": 441,
      "type": "TODO",
      "content": "// TODO: DO NOT CHANGE THE MAX SCORE IF USER SET IT: BUT THAT",
      "context": "                // dynamically selected, it might be less that the global target;\n                // that is, the deme might not be able to reach the best score.)\n                //\n                // TODO: DO NOT CHANGE THE MAX SCORE IF USER SET IT: BUT THAT\n                // OPTION ISN'T GLOBAL WHAT TO DO?\n                //\n                // But why would we want to over-ride the best-possible score?\n",
      "description": "DO NOT CHANGE THE MAX SCORE IF USER SET IT: BUT THAT"
    },
    {
      "file": "moses/moses/moses/deme/deme_expander.cc",
      "line": 457,
      "type": "TODO",
      "content": "// TODO: re-enable that once best_possible_bscore is fixed",
      "context": "                              \"terminate deme search. Except I think this \"\n                              \"is fixed now. It needs review and testing.\");\n\n                // TODO: re-enable that once best_possible_bscore is fixed\n                // I think its now fixed, but I'm not sure.  It needs to be\n                // reviewed and tested.\n#if THIS_IS_DISABLED_UNTIL_ABOVE_IS_FIXED\n",
      "description": "re-enable that once best_possible_bscore is fixed"
    },
    {
      "file": "moses/moses/moses/deme/deme_expander.cc",
      "line": 502,
      "type": "FIXME",
      "content": "// XXX FIXME this is a bug .. the user may have specified that",
      "context": "    if (_params.fstor) {\n        // reset scorer to use all variables (important so that\n        // behavioral score is consistent across generations\n        // XXX FIXME this is a bug .. the user may have specified that\n        // certain incdexes should be ignored, and this just wipes\n        // those out...\n        _cscorer.ignore_cols(std::set<arity_t>());\n",
      "description": "this is a bug .. the user may have specified that"
    },
    {
      "file": "moses/moses/moses/eda/replacement.h",
      "line": 62,
      "type": "TODO",
      "content": "// TODO: I think it might be a little more efficent to use the",
      "context": "// Replace the most similar individual, where similarity is determined by\n// the hamming distance.\n//\n// TODO: I think it might be a little more efficent to use the\n// hamming_distance as a sort comparison operator, and hand off the whole\n// thing to std:nth_element, and let that class figure out who is close or\n// not.  This avoids the use of doubly-nested loops, and multiple redundant\n",
      "description": "I think it might be a little more efficent to use the"
    },
    {
      "file": "moses/moses/moses/representation/representation.cc",
      "line": 51,
      "type": "TODO",
      "content": "// XXX TODO: One might think that varying the stepsize, i.e. shrinking",
      "context": "// Stepsize should be roughly the standard-deviation of the expected\n// distribution of the contin variables.\n//\n// XXX TODO: One might think that varying the stepsize, i.e. shrinking\n// it, as the optimizers tune into a specific value, would be a good\n// thing (so that the optimizer could tune to a more precise value).\n// Unfortunately, a simple experiment in tuning (see below, surrounded\n",
      "description": "One might think that varying the stepsize, i.e. shrinking"
    },
    {
      "file": "moses/moses/moses/representation/representation.cc",
      "line": 238,
      "type": "TODO",
      "content": "// XXX TODO need to add support for \"term algebra\" knobs",
      "context": "/// the instance supplied as the argument.\nvoid representation::transform(const instance& inst)\n{\n    // XXX TODO need to add support for \"term algebra\" knobs\n\n    contin_map_it ckb = contin.begin();\n    for (field_set::const_contin_iterator ci = _fields.begin_contin(inst);\n",
      "description": "need to add support for \"term algebra\" knobs"
    },
    {
      "file": "moses/moses/moses/representation/build_knobs.cc",
      "line": 318,
      "type": "TODO",
      "content": "/// TODO: measure and compare the resulting performance.",
      "context": "/// can be rather incredibly costly, especially when the exemplars start\n/// getting large.  So the real question is: is the performance cost of\n/// this routine worth the eventual savings when scoring instances?\n/// TODO: measure and compare the resulting performance.\n//\n// Notes to self: hmm. in 5-parity problem, about 2/3 of knobs are\n// disallowed! viz of 6738 probes, 4007 knobs are completely disallowed.\n",
      "description": "measure and compare the resulting performance."
    },
    {
      "file": "moses/moses/moses/representation/build_knobs.cc",
      "line": 491,
      "type": "TODO",
      "content": "// TODO: should bias the selection of these, so that",
      "context": "        }\n    }\n\n    // TODO: should bias the selection of these, so that\n    // larger subtrees are preferred .. !? why?\n\n    unsigned max_pairs = permitted_perms.size();\n",
      "description": "should bias the selection of these, so that"
    },
    {
      "file": "moses/moses/moses/representation/build_knobs.cc",
      "line": 583,
      "type": "TODO",
      "content": "// TODO: Benchmark and clarify optimal breakeven point across different problem sizes.",
      "context": "    // The number of 30K is a wild guesstimate, based on recent\n    // measurements of relatively simple exemplars; its maybe even\n    // too low.  For large exemplars, it might be too big !?\n    // TODO: Benchmark and clarify optimal breakeven point across different problem sizes.\n#define BREAKEVEN 30000\n    size_t np = perms.size();\n    int nthr = 1 + np / BREAKEVEN;\n",
      "description": "Benchmark and clarify optimal breakeven point across different problem sizes."
    },
    {
      "file": "moses/moses/moses/representation/build_knobs.cc",
      "line": 692,
      "type": "TODO",
      "content": "// XXX TODO: Is this really optimal?  The below adds an entire copy",
      "context": "        }\n    }\n\n    // XXX TODO: Is this really optimal?  The below adds an entire copy\n    // of the tree at it, which clearly increases the overall complexity.\n    // But is this really a wise thig to do? It seems gratuitous, and it's\n    // not obvious that knobs from this flipped tree will yeild benefits,\n",
      "description": "Is this really optimal?  The below adds an entire copy"
    },
    {
      "file": "moses/moses/moses/representation/build_knobs.cc",
      "line": 1183,
      "type": "TODO",
      "content": "//TODO: should bias the selection of these (and possibly choose larger subtrees)",
      "context": "        perms.push_back(tr);\n\n    //and n random pairs out of the total  2 * choose(n,2) = n * (n - 1) of these\n    //TODO: should bias the selection of these (and possibly choose larger subtrees)\n    lazy_random_selector randpair(n * (n - 1));\n\n    dorepeat(n) {\n",
      "description": "should bias the selection of these (and possibly choose larger subtrees)"
    },
    {
      "file": "moses/moses/moses/representation/build_knobs.cc",
      "line": 1285,
      "type": "TODO",
      "content": "// XXX TODO this below is clearly unfinished, broken, etc.",
      "context": "    }\n}\n\n// XXX TODO this below is clearly unfinished, broken, etc.\n// and can't possibly work ... \nvoid build_knobs::ann_canonize(pre_it it)\n{\n",
      "description": "this below is clearly unfinished, broken, etc."
    },
    {
      "file": "moses/moses/moses/representation/build_knobs.cc",
      "line": 1344,
      "type": "FIXME",
      "content": "//FIXME: now just attaches to the first output",
      "context": "    cout << \"Created node: \" << new_node << endl;\n\n    //now attach the subtree to the hidden nodes\n    //FIXME: now just attaches to the first output\n    sib_it first_hidden = it.begin();\n\n    _exemplar.insert_subtree(first_hidden.begin(),new_node.begin());\n",
      "description": "now just attaches to the first output"
    },
    {
      "file": "moses/moses/moses/main/problem-params.h",
      "line": 46,
      "type": "FIXME",
      "content": "// XXX FIXME TODO The structure below should be split into multiple",
      "context": "\nnamespace opencog { namespace moses {\n\n// XXX FIXME TODO The structure below should be split into multiple\n// parts, with each sub-part responsible for picking out the argv's\n// that it cares about. Unfortunately, this requires getting rid of\n// boost::program_options (because boost::program_options does not\n",
      "description": "TODO The structure below should be split into multiple"
    },
    {
      "file": "moses/moses/moses/main/table-problems.cc",
      "line": 138,
      "type": "FIXME",
      "content": "// XXX FIXME -- the multiple tables should be merged into one.",
      "context": "    }\n    logger().info(\"Number of rows in tables = %d\", num_rows);\n\n    // XXX FIXME -- the multiple tables should be merged into one.\n    ctable = _ctables.front();\n    table = _tables.front();\n\n",
      "description": "-- the multiple tables should be merged into one."
    },
    {
      "file": "moses/moses/moses/main/table-problems.cc",
      "line": 150,
      "type": "FIXME",
      "content": "// XXX FIXME .. check that they all have the same signature.",
      "context": "    arity = table.get_arity();\n\n    // Check that all input data files have the same arity\n    // XXX FIXME .. check that they all have the same signature.\n    if (_tables.size() > 1) {\n        for (size_t i = 1; i < _tables.size(); ++i) {\n            combo::arity_t test_arity = _tables[i].get_arity();\n",
      "description": ".. check that they all have the same signature."
    },
    {
      "file": "moses/moses/moses/main/problem-params.cc",
      "line": 169,
      "type": "TODO",
      "content": "// XXX TODO: make this print correctly, instead of using brackets.",
      "context": "    using namespace std;\n\n    // Declare the supported options.\n    // XXX TODO: make this print correctly, instead of using brackets.\n    desc.add_options()\n\n        // General options\n",
      "description": "make this print correctly, instead of using brackets."
    },
    {
      "file": "moses/moses/comboreduct/type_checker/type_tree.cc",
      "line": 627,
      "type": "TODO",
      "content": "// XXX TODO the code below was modified to allow arg lists of",
      "context": "            // then check that a1 inherits from T1, and that a2, a3\n            // and a4 inherit from T2.  T3 is the output type.\n\n            // XXX TODO the code below was modified to allow arg lists of\n            // mixed type, e.g. so that the cond primitive could be\n            // supported (as the current definition of cond alternates\n            // between boolean-valued predicates, and the result type).\n",
      "description": "the code below was modified to allow arg lists of"
    },
    {
      "file": "moses/moses/comboreduct/type_checker/type_tree.h",
      "line": 235,
      "type": "TODO",
      "content": "// TODO : lambda",
      "context": "//\n// intersection of ill_formed and T is ill_formed\n//\n// TODO : lambda\n//\n// Of course the case if T1 inherit T2 then interection of T1 and T2\n// is T1 is also implemented. If the interection is ill_formed or\n",
      "description": "lambda"
    },
    {
      "file": "moses/moses/comboreduct/interpreter/interpreter.cc",
      "line": 336,
      "type": "TODO",
      "content": "// XXX TODO: contin_if should go away.",
      "context": "            return (i == id::logical_true ? 1.0 : 0.0);\n        }\n\n        // XXX TODO: contin_if should go away.\n        case id::contin_if :\n        case id::cond : {\n            sib_it sib = it.begin();\n",
      "description": "contin_if should go away."
    },
    {
      "file": "moses/moses/comboreduct/interpreter/eval.cc",
      "line": 563,
      "type": "TODO",
      "content": "// XXX TODO: contin_if should go away.",
      "context": "            return eval_throws_tree(new_bmap, lambda_expr);\n        }\n\n        // XXX TODO: contin_if should go away.\n        case id::contin_if :\n        case id::cond : {\n            sib_it sib = it.begin();\n",
      "description": "contin_if should go away."
    },
    {
      "file": "moses/moses/comboreduct/combo/descriptions.cc",
      "line": 45,
      "type": "TODO",
      "content": "// ToDo: would be nice to have a more Caml/Haskell style syntax here,",
      "context": "// with builtins as indicies, within the singleton class builtin_properties.\n// This array should not have any other usages.\n//\n// ToDo: would be nice to have a more Caml/Haskell style syntax here,\n// right?\nstatic const builtin_description bd[] =\n{\n",
      "description": "would be nice to have a more Caml/Haskell style syntax here,"
    },
    {
      "file": "moses/moses/comboreduct/table/table_io.cc",
      "line": 934,
      "type": "TODO",
      "content": "// TODO could be simplified, optimized, etc",
      "context": "            // It is sparse\n            is_sparse = is_sparse || string::npos != line.find(sparse_delim);\n            if (is_sparse) { // just get out\n                // TODO could be simplified, optimized, etc\n                in.seekg(beg);\n                in.clear();         // in case it has reached the eof\n                return in;\n",
      "description": "could be simplified, optimized, etc"
    },
    {
      "file": "moses/moses/comboreduct/table/table.h",
      "line": 692,
      "type": "TODO",
      "content": "// XXX TODO WARNING ERROR: builtin hardcoded shit!!!",
      "context": "        auto it = filter.cbegin();\n        for (unsigned i = 0; i < seq.size(); ++i) {\n            if (it != filter.cend() && (typename F::value_type)i == *it) {\n                // XXX TODO WARNING ERROR: builtin hardcoded shit!!!\n                res.push_back(seq.get_at<builtin>(i));\n                ++it;\n            } else {\n",
      "description": "WARNING ERROR: builtin hardcoded shit!!!"
    },
    {
      "file": "moses/moses/comboreduct/table/table.h",
      "line": 696,
      "type": "TODO",
      "content": "// XXX TODO WARNING ERROR: builtin hardcoded shit!!!",
      "context": "                res.push_back(seq.get_at<builtin>(i));\n                ++it;\n            } else {\n                // XXX TODO WARNING ERROR: builtin hardcoded shit!!!\n                res.push_back(id::null_vertex);\n            }\n        }\n",
      "description": "WARNING ERROR: builtin hardcoded shit!!!"
    },
    {
      "file": "moses/moses/comboreduct/table/table.h",
      "line": 1352,
      "type": "TODO",
      "content": "// XXX TODO, it would be easier if KLD took a sorted list",
      "context": "            }\n        }\n\n        // XXX TODO, it would be easier if KLD took a sorted list\n        // as the argument.\n        std::vector<contin_t> p, q;\n        for (auto pr : sorted_list) {\n",
      "description": ", it would be easier if KLD took a sorted list"
    },
    {
      "file": "moses/moses/comboreduct/table/table.h",
      "line": 1366,
      "type": "TODO",
      "content": "// XXX TODO remove this print, for better performance.",
      "context": "        // Also a problem, this is returning values greater than 1.0;\n        // I thought that IC was supposed to max out at 1.0 !?\n        contin_t ic = - KLD(p,q);\n        // XXX TODO remove this print, for better performance.\n        unsigned idx = *(fs.begin());\n        logger().debug() <<\"Contin MI for feat=\" << idx << \" ic=\" << ic;\n        return ic;\n",
      "description": "remove this print, for better performance."
    },
    {
      "file": "moses/moses/comboreduct/table/table.cc",
      "line": 420,
      "type": "TODO",
      "content": "// XXX TODO replace this by the util p_norm function.",
      "context": "    return rhs.get_label() == label;\n}\n\n// XXX TODO replace this by the util p_norm function.\ncontin_t OTable::abs_distance(const OTable& ot) const\n{\n    OC_ASSERT(ot.size() == size());\n",
      "description": "replace this by the util p_norm function."
    },
    {
      "file": "moses/moses/comboreduct/table/table.cc",
      "line": 445,
      "type": "TODO",
      "content": "// XXX TODO replace this by the util p_norm function.",
      "context": "    return res;\n}\n\n// XXX TODO replace this by the util p_norm function.\ncontin_t OTable::sum_squared_error(const OTable& ot) const\n{\n    OC_ASSERT(ot.size() == size());\n",
      "description": "replace this by the util p_norm function."
    },
    {
      "file": "moses/moses/comboreduct/table/table.cc",
      "line": 859,
      "type": "TODO",
      "content": "// XXX TODO replace this by the util p_norm function.",
      "context": "\n// -------------------------------------------------------\n\n// XXX TODO replace this by the util p_norm function.\ncomplete_truth_table::size_type\ncomplete_truth_table::hamming_distance(const complete_truth_table& other) const\n{\n",
      "description": "replace this by the util p_norm function."
    },
    {
      "file": "moses/moses/comboreduct/reduct/logical_rules.cc",
      "line": 102,
      "type": "TODO",
      "content": "// XXX TODO: I don't understand why this is not damaging contin_if  !??",
      "context": "    // Most nodes take simple lists; but not cond. Cond takes clauses,\n    // which are pairs. If we remove the condition, we must also remove\n    // the consequent.\n// XXX TODO: I don't understand why this is not damaging contin_if  !??\n// But .. umm, maybe build_knobs is not creating any kinds of contin_if's\n// that can be damaged... well, no matter, because thes if's will be\n// replaced by cond... \n",
      "description": "I don't understand why this is not damaging contin_if  !??"
    },
    {
      "file": "moses/moses/comboreduct/reduct/mixed_rules.cc",
      "line": 1228,
      "type": "TODO",
      "content": "//check if 0<-(y+pi) -> false //TODO",
      "context": "                }\n            }\n            else if(*copy_tr.begin()==id::logical_false) {\n                //check if 0<-(y+pi) -> false //TODO\n                combo_tree copy2_tr = tr.subtree(sib_it(it), tr.next_sibling(sib_it(it)));\n                //copy old assumptions, begin\n                sib_it bna = copy2_tr.begin(); //before new assumption\n",
      "description": ""
    },
    {
      "file": "moses/moses/comboreduct/reduct/contin_rules.cc",
      "line": 964,
      "type": "TODO",
      "content": "// TODO:  sin(*(-1 x)) -> -sin(x)",
      "context": "// or more generally\n// sin(sum x_i + sum c_j) -> sin(sum x_i + ((sum c_j)+pi)%2pi -pi\n//\n// TODO:  sin(*(-1 x)) -> -sin(x)\n// The above is frequently seen in real-life ...\nvoid reduce_sin::operator()(combo_tree& tr, combo_tree::iterator it) const\n{\n",
      "description": "sin(*(-1 x)) -> -sin(x)"
    },
    {
      "file": "moses/moses/comboreduct/main/action-reductor.cc",
      "line": 94,
      "type": "TODO",
      "content": "// TODO -- replace this by cond",
      "context": "    cout << \"output type \" << ba2->get_output_type_tree() << endl;\n\n#if 0\n    // TODO -- replace this by cond\n    cout << \"6----------------\" << endl;\n\n    cout << \"arity \" << (int)get_arity(id::boolean_if) << endl;\n",
      "description": "-- replace this by cond"
    },
    {
      "file": "moses/moses/comboreduct/main/eval-table.cc",
      "line": 147,
      "type": "FIXME",
      "content": "// XXX FIXME",
      "context": "    }\n\n    // HERE WE ARE ASSUMING THAT THE INPUT FILE HAS A HEADER!!!\n// XXX FIXME\n    vector<string> header = get_header(pa.input_table_file);\n\n    // Add to ignore_values (header - all_unique_variables - target feature)\n",
      "description": ""
    },
    {
      "file": "atomspace-rocks/opencog/persist/monospace/MonoIO.cc",
      "line": 1004,
      "type": "FIXME",
      "content": "// XXX FIXME. We would like to call",
      "context": "\nvoid MonoStorage::storeAtomSpace(const AtomSpace* table)\n{\n\t// XXX FIXME. We would like to call\n\t// Options::PrepareForBulkLoad() here, but its too late, this\n\t// can only be set when opening the DB. Should we maybe close\n\t// and reopen the DB? This would be ... conducive of weird bugs.\n",
      "description": ". We would like to call"
    },
    {
      "file": "atomspace-rocks/opencog/persist/rocks/RocksIO.cc",
      "line": 1371,
      "type": "FIXME",
      "content": "// XXX FIXME. We would like to call",
      "context": "\t                        and nullptr != getAtomSpace())\n\t\tconvertForFrames(HandleCast(getAtomSpace()));\n\n\t// XXX FIXME. We would like to call\n\t// Options::PrepareForBulkLoad() here, but its too late, this\n\t// can only be set when opening the DB. Should we maybe close\n\t// and reopen the DB? This would be ... conducive of weird bugs.\n",
      "description": ". We would like to call"
    },
    {
      "file": "components/integration/opencog/opencog/nlp/fuzzy/Fuzzy.cc",
      "line": 82,
      "type": "TODO",
      "content": "// TODO: Extend to find similar links as well",
      "context": "    {\n        if (h->is_node())\n        {\n            // TODO: Extend to find similar links as well\n            if (lp->get_type() == SIMILARITY_LINK)\n                sl.emplace_back(lp->get_handle());\n\n",
      "description": "Extend to find similar links as well"
    },
    {
      "file": "components/integration/opencog/opencog/nlp/fuzzy/FuzzyMatchBasic.cc",
      "line": 107,
      "type": "TODO",
      "content": "// TODO: May use Truth Value instead",
      "context": "\tdouble similarity = common_nodes.size();\n\n\t// Roughly estimate how \"rare\" each node is by using 1 / incoming set size\n\t// TODO: May use Truth Value instead\n\t// for (const Handle& common_node : common_nodes)\n\t// \tsimilarity += 1.0 / common_node->getIncomingSetSize();\n\n",
      "description": "May use Truth Value instead"
    },
    {
      "file": "components/integration/opencog/opencog/openpsi/OpenPsiSCM.cc",
      "line": 92,
      "type": "TODO",
      "content": "// TODO: Should this be a singleton? What could be the issues that need",
      "context": "  const Handle& goal, const TruthValuePtr stv, const Handle& category)\n{\n  AtomSpacePtr asp = SchemeSmob::ss_get_env_as(\"psi-rule\");\n  // TODO: Should this be a singleton? What could be the issues that need\n  // to be handled? How to handle multiple atomspace, maybe a singleton per\n  // atomspace?\n  Handle rule = openpsi_cache(asp.get()).add_rule(context, action, goal, stv);\n",
      "description": "Should this be a singleton? What could be the issues that need"
    },
    {
      "file": "components/integration/opencog/opencog/openpsi/OpenPsiSCM.cc",
      "line": 96,
      "type": "TODO",
      "content": "// TODO: Add to multiple categories using scheme rest list.",
      "context": "  // to be handled? How to handle multiple atomspace, maybe a singleton per\n  // atomspace?\n  Handle rule = openpsi_cache(asp.get()).add_rule(context, action, goal, stv);\n  // TODO: Add to multiple categories using scheme rest list.\n  openpsi_cache(asp.get()).add_to_category(rule, category);\n  return rule;\n}\n",
      "description": "Add to multiple categories using scheme rest list."
    },
    {
      "file": "components/integration/opencog/opencog/openpsi/OpenPsiRules.cc",
      "line": 69,
      "type": "TODO",
      "content": "// TODO: Test thoroughly, or develop an alternative. See discussion",
      "context": "      _psi_rules[rule] = std::make_tuple(context, action, goal, query);\n  } else {\n    // This is for backward compatability.\n    // TODO: Test thoroughly, or develop an alternative. See discussion\n    // @ https://github.com/opencog/opencog/pull/2899 for what the\n    // alternative might be.\n\n",
      "description": "Test thoroughly, or develop an alternative. See discussion"
    },
    {
      "file": "components/integration/opencog/opencog/openpsi/OpenPsiRules.cc",
      "line": 102,
      "type": "TODO",
      "content": "// TODO But why make the add_category public then?",
      "context": "{\n  _as->add_link(MEMBER_LINK, rule, category);\n  // Add the category just in case it hasn't been declared.\n  // TODO But why make the add_category public then?\n  add_category(category);\n  _category_index[category].insert(rule);\n\n",
      "description": "But why make the add_category public then?"
    },
    {
      "file": "components/integration/opencog/opencog/openpsi/OpenPsiSatisfier.cc",
      "line": 42,
      "type": "TODO",
      "content": "// TODO: Saperated component patterns aren't handled by this function",
      "context": "bool OpenPsiSatisfier::grounding(const HandleMap &var_soln,\n                                  const HandleMap &term_soln)\n{\n  // TODO: Saperated component patterns aren't handled by this function\n  // as PMCGroundings is used instead. Update to handle such cases.\n\n  // The psi-rule weight calculations could be done here.\n",
      "description": "Saperated component patterns aren't handled by this function"
    },
    {
      "file": "components/integration/opencog/opencog/openpsi/OpenPsiSatisfier.cc",
      "line": 56,
      "type": "TODO",
      "content": "// TODO: If we are here it means the suggested groundings doesn't have",
      "context": "      }\n    }\n\n    // TODO: If we are here it means the suggested groundings doesn't have\n    // VariableNodes, and can be cached. This doesn't account for terms\n    // that are under QuoteLink, or other similar type links. How should\n    // such cases be handled?\n",
      "description": "If we are here it means the suggested groundings doesn't have"
    },
    {
      "file": "components/integration/opencog/opencog/openpsi/OpenPsiSatisfier.cc",
      "line": 76,
      "type": "TODO",
      "content": "// TODO: This happens when InitiateSearchCB::no_search has groundings.",
      "context": "    _implicator -> _satisfiability_cache[_pattern_body] = var_soln;\n    return true;\n  } else {\n    // TODO: This happens when InitiateSearchCB::no_search has groundings.\n    // Cases for when this happens hasn't been tested yet. Explore the\n    // behavior and find a better solution. For now, log it and continue\n    // searching.\n",
      "description": "This happens when InitiateSearchCB::no_search has groundings."
    },
    {
      "file": "components/integration/opencog/opencog/openpsi/OpenPsiImplicator.cc",
      "line": 49,
      "type": "TODO",
      "content": "// TODO: Add cache per atomspace.",
      "context": "  Handle query_body = query->get_pattern().body;\n\n  // Always update cache to clear any previous result.\n  // TODO: Add cache per atomspace.\n  _satisfiability_cache.erase(query_body);\n  _pattern_seen.insert(query_body);\n\n",
      "description": "Add cache per atomspace."
    },
    {
      "file": "components/integration/opencog/opencog/openpsi/OpenPsiImplicator.h",
      "line": 44,
      "type": "TODO",
      "content": "// TODO Why would one need to reset during psi-loop?",
      "context": "  friend class OpenPsiSatisfier;\n\n  // Needed for resetting private cache.\n  // TODO Why would one need to reset during psi-loop?\n  friend class ::OpenPsiImplicatorUTest;\n\npublic:\n",
      "description": "Why would one need to reset during psi-loop?"
    },
    {
      "file": "components/integration/opencog/opencog/openpsi/OpenPsiRules.h",
      "line": 105,
      "type": "TODO",
      "content": "// TODO:add predicate to check for membership of category.",
      "context": "   * @param new_category The node reprsenting the new category.\n   * @return ConceptNode that represents the category.\n   */\n   // TODO:add predicate to check for membership of category.\n  Handle add_category(const Handle& new_category);\n\n  /**\n",
      "description": "add predicate to check for membership of category."
    },
    {
      "file": "components/integration/opencog/opencog/openpsi/OpenPsiRules.h",
      "line": 126,
      "type": "TODO",
      "content": "// TODO Should these entries be a member of Rules class?",
      "context": "   * where queryis a PatternLink that isn't added to the atomspace, and\n   * is used to check if the rule is satisfiable.\n   */\n  // TODO Should these entries be a member of Rules class?\n  typedef std::tuple<HandleSeq, Handle, Handle, PatternLinkPtr> PsiTuple;\n\n  /**\n",
      "description": "Should these entries be a member of Rules class?"
    },
    {
      "file": "components/integration/opencog/opencog/openpsi/OpenPsiRules.h",
      "line": 136,
      "type": "TODO",
      "content": "// TODO: Using names that are prefixed with \"OpenPsi: \" might be a bad idea,",
      "context": "   */\n  std::map<Handle, PsiTuple> _psi_rules;\n\n  // TODO: Using names that are prefixed with \"OpenPsi: \" might be a bad idea,\n  // because it might hinder interoperability with other components that\n  // expect an explicit ontological representation. For historic reasons we\n  // continue using such convention but should be replaced with graph that\n",
      "description": "Using names that are prefixed with \"OpenPsi: \" might be a bad idea,"
    },
    {
      "file": "components/language/learn/attic/run-ull-2019/SchemeEval.cc",
      "line": 1029,
      "type": "TODO",
      "content": "// TODO: it would be nice to pass exceptions on through, but",
      "context": "\t}\n\texpr = scm_cons(sfunc, expr);\n\n\t// TODO: it would be nice to pass exceptions on through, but\n\t// this currently breaks unit tests.\n\t// if (_in_eval)\n\t//    return scm_eval(expr, scm_interaction_environment());\n",
      "description": "it would be nice to pass exceptions on through, but"
    },
    {
      "file": "components/language/learn/attic/run-ull-2019/SchemeEval.cc",
      "line": 1182,
      "type": "FIXME",
      "content": "// XXX FIXME only a subset is needed.",
      "context": "\nvoid SchemeEval::init_scheme(void)\n{\n\t// XXX FIXME only a subset is needed.\n\tSchemeEval sch;\n}\n\n",
      "description": "only a subset is needed."
    },
    {
      "file": "components/language/lg-atomese/opencog/nlp/lg-dict/LGDictExpContainer.cc",
      "line": 239,
      "type": "FIXME",
      "content": "// XXX FIXME this does not smell right; optionals should get",
      "context": "\n    if (m_type == CONNECTOR_type)\n    {\n        // XXX FIXME this does not smell right; optionals should get\n        // blown up into pairs of disjuncts, one with and one without.\n        if (m_string == \"OPTIONAL\") return { optnl };\n\n",
      "description": "this does not smell right; optionals should get"
    },
    {
      "file": "components/language/lg-atomese/opencog/nlp/lg-dict/LGDictExpContainer.cc",
      "line": 268,
      "type": "FIXME",
      "content": "// XXX FIXME ... using an std::map would be more efficient.",
      "context": "    // remove repeated atoms from OR\n    if (m_type == OR_type)\n    {\n        // XXX FIXME ... using an std::map would be more efficient.\n        std::sort(outgoing.begin(), outgoing.end());\n        outgoing.erase(std::unique(outgoing.begin(),\n                                   outgoing.end()),\n",
      "description": "... using an std::map would be more efficient."
    },
    {
      "file": "components/language/lg-atomese/opencog/nlp/lg-dict/LGDictReader.cc",
      "line": 46,
      "type": "FIXME",
      "content": "// FIXME XXX -- Optionals are handled incorrectly here;",
      "context": "\n    std::vector<LGDictExpContainer> subcontainers;\n\n    // FIXME XXX -- Optionals are handled incorrectly here;\n    // they are denoted by a null Exp pointer in an OR_list!\n    // Ignoring all the nulls is just ... wrong.\n#if (LINK_MAJOR_VERSION == 5) &&  (LINK_MINOR_VERSION < 7)\n",
      "description": "XXX -- Optionals are handled incorrectly here;"
    },
    {
      "file": "components/language/lg-atomese/opencog/nlp/lg-dict/LGDictReader.cc",
      "line": 103,
      "type": "FIXME",
      "content": "// XXX FIXME -- if dn_head is null, then we should check regexes.",
      "context": "\n    HandleSeq outgoing;\n\n// XXX FIXME -- if dn_head is null, then we should check regexes.\n// Currently, LG does not do this automatically, but it almost surely\n// should. i.e. the LG public API needs to also handle regexes\n// automatically.\n",
      "description": "-- if dn_head is null, then we should check regexes."
    },
    {
      "file": "components/language/lg-atomese/opencog/nlp/lg-parse/LGParseLink.cc",
      "line": 229,
      "type": "FIXME",
      "content": "// XXX FIXME. This should be part of the LgDictNode but since",
      "context": "\t// Set up the dictionary config, if any.\n\t// This must happen before ldn->get_dictionary() because the\n\t// setup is stateful. This seems buggy, but is adequate for now.\n\t// XXX FIXME. This should be part of the LgDictNode but since\n\t// LgDictNode is a node, not a link, its hard to pass args.\n\t// We would need to wrap it with a StateLink, or maybe use the\n\t// new-fangled \"sensory API\". Sheesh.\n",
      "description": ". This should be part of the LgDictNode but since"
    },
    {
      "file": "components/language/lg-atomese/opencog/nlp/lg-parse/LGParseLink.cc",
      "line": 338,
      "type": "FIXME",
      "content": "// XXX FIXME -- We should fish parse options out of the atomspace.",
      "context": "\t\t\tparse_options_set_linkage_limit(opts, max_linkages);\n\t}\n\n\t// XXX FIXME -- We should fish parse options out of the atomspace.\n\t// Something like this, maybe:\n\t//     EvaluationLink\n\t//         PredicateNode \"LG ParseTime\"\n",
      "description": "-- We should fish parse options out of the atomspace."
    },
    {
      "file": "components/core/atomspace-rocks/opencog/persist/monospace/MonoIO.cc",
      "line": 920,
      "type": "TODO",
      "content": "// XXX TODO - maybe load links depth-order...",
      "context": "{\n\tCHECK_OPEN;\n\t// First, load all the nodes ... then the links.\n\t// XXX TODO - maybe load links depth-order...\n\tloadAtoms(table, \"n@\");\n\tloadAtoms(table, \"l@\");\n}\n",
      "description": "- maybe load links depth-order..."
    },
    {
      "file": "components/core/atomspace-rocks/opencog/persist/monospace/MonoIO.cc",
      "line": 944,
      "type": "FIXME",
      "content": "// XXX FIXME. We would like to call",
      "context": "\nvoid MonoStorage::storeAtomSpace(const AtomSpace* table)\n{\n\t// XXX FIXME. We would like to call\n\t// Options::PrepareForBulkLoad() here, but its too late, this\n\t// can only be set when opening the DB. Should we maybe close\n\t// and reopen the DB? This would be ... conducive of weird bugs.\n",
      "description": ". We would like to call"
    },
    {
      "file": "components/core/atomspace-rocks/opencog/persist/rocks/RocksDAG.cc",
      "line": 240,
      "type": "TODO",
      "content": "// XXX TODO: we should probably cache the results, instead of",
      "context": "void RocksStorage::makeOrder(Handle hasp,\n                             std::map<uint64_t, Handle>& order)\n{\n// XXX TODO: we should probably cache the results, instead of\n// recomputing every time!?\n\t// As long as there's a stack of Frames, just loop.\n\twhile (true)\n",
      "description": "we should probably cache the results, instead of"
    },
    {
      "file": "components/core/atomspace-rocks/opencog/persist/rocks/RocksIO.cc",
      "line": 1283,
      "type": "FIXME",
      "content": "// XXX FIXME. We would like to call",
      "context": "\t                        and nullptr != getAtomSpace())\n\t\tconvertForFrames(HandleCast(getAtomSpace()));\n\n\t// XXX FIXME. We would like to call\n\t// Options::PrepareForBulkLoad() here, but its too late, this\n\t// can only be set when opening the DB. Should we maybe close\n\t// and reopen the DB? This would be ... conducive of weird bugs.\n",
      "description": ". We would like to call"
    },
    {
      "file": "components/core/atomspace-rocks/opencog/persist/rocks/RocksPersistSCM.cc",
      "line": 82,
      "type": "FIXME",
      "content": "// XXX FIXME -- are open and close actually needed for anything?",
      "context": "    _storage = nullptr;\n}\n\n// XXX FIXME -- are open and close actually needed for anything?\nvoid RocksPersistSCM::do_open(const std::string& uri)\n{\n    if (_storage)\n",
      "description": "-- are open and close actually needed for anything?"
    },
    {
      "file": "components/core/atomspace-restful/lib/zmq/zhelpers.hpp",
      "line": 31,
      "type": "TODO",
      "content": "// todo: package updated zmq.hpp",
      "context": "\n#include <zmq.hpp>\n//#include <lib/zmq/zmq.hpp>\n// todo: package updated zmq.hpp\n\n#include <iostream>\n#include <iomanip>\n",
      "description": "package updated zmq.hpp"
    },
    {
      "file": "components/core/atomspace-restful/opencog/python/web/api/utilities.py",
      "line": 17,
      "type": "FIXME",
      "content": "# FIXME: Should this moved to the atomspace repo and be part",
      "context": "# https://github.com/opencog/opencog/pull/2012 and,\n# https://github.com/opencog/atomspace/pull/611\n# NOTE: This is similar to scheme `cog-node`.\n# FIXME: Should this moved to the atomspace repo and be part\n# of opencog.atomspace module?\ndef get_atoms_by_name(z_type, name, atomspace):\n    return filter(lambda x: x.name == name, atomspace.get_atoms_by_type(z_type))\n",
      "description": "Should this moved to the atomspace repo and be part"
    },
    {
      "file": "components/learning/moses/examples/example-progs/continmax.cc",
      "line": 67,
      "type": "TODO",
      "content": "// TODO: Expand documentation to match style of onemax/nmax examples.",
      "context": "// -- the number that is -log_2(epsilon) where epsilon is the smallest\n//    distinction between continuous variables what will be drawn.\n//\n// TODO: Expand documentation to match style of onemax/nmax examples.\n// Add parameter descriptions, usage examples, and expected output.\n\nint main(int argc, char** argv)\n",
      "description": "Expand documentation to match style of onemax/nmax examples."
    },
    {
      "file": "components/learning/moses/moses/moses/optimization/star-anneal.cc",
      "line": 42,
      "type": "TODO",
      "content": "// XXX TODO the annealing temperature control code should be ported over",
      "context": "// Star-shaped search  //\n/////////////////////////\n\n// XXX TODO the annealing temperature control code should be ported over\n// to the hill-climbing code, thus rendering the below obsolete.  The\n// hill-climbing code is much more sophisticated in every way: correct\n// definition of the temperature, termination conditions, exploration of\n",
      "description": "the annealing temperature control code should be ported over"
    },
    {
      "file": "components/learning/moses/moses/moses/optimization/particle-swarm.h",
      "line": 154,
      "type": "TODO",
      "content": "// TODO: pso description",
      "context": "// Particle Swarm //\n////////////////////\n\n// TODO: pso description\nstruct particle_swarm : optimizer_base\n{\n    particle_swarm(const optim_parameters& op = optim_parameters(),\n",
      "description": "pso description"
    },
    {
      "file": "components/learning/moses/moses/moses/optimization/particle-swarm.h",
      "line": 315,
      "type": "TODO",
      "content": "// TODO: Wind dispersion, but test without first",
      "context": "    void update_cont_particle(instance& temp, const instance& personal,\n            const instance& global, velocity::iterator vel, const field_set& fs);\n\n    // TODO: Wind dispersion, but test without first\n    // Make it later is easy.\n\npublic:\n",
      "description": "Wind dispersion, but test without first"
    },
    {
      "file": "components/learning/moses/moses/moses/optimization/hill-climbing.h",
      "line": 110,
      "type": "TODO",
      "content": "// XXX TODO make sure this value is appropriately updated.",
      "context": "\n    // Range of scores for which to keep instances.  This *should* be\n    // set to the value given by metapopulation::useful_score_range().\n    // XXX TODO make sure this value is appropriately updated.\n    //\n    // The range of scores is used to keep the size of the deme in check.\n    // The issue is that, for large feature sets, a large number of knobs\n",
      "description": "make sure this value is appropriately updated."
    },
    {
      "file": "components/learning/moses/moses/moses/optimization/particle-swarm.cc",
      "line": 197,
      "type": "TODO",
      "content": "// TODO: work in a better way to identify convergence.",
      "context": "            break;\n        }\n\n        // TODO: work in a better way to identify convergence.\n        not_improving = (has_improved) ? 0 : not_improving + 1;\n        if (not_improving > 3) {\n            logger().debug(\"Terminate Local Search: Convergence.\");\n",
      "description": "work in a better way to identify convergence."
    },
    {
      "file": "components/learning/moses/moses/moses/optimization/particle-swarm.cc",
      "line": 236,
      "type": "TODO",
      "content": "// TODO: Explanation",
      "context": "        \"complexity\";\n}\n\n// TODO: Explanation\n// There's no explanation for this, it's just a temporary solution.\n// Maybe use adaptative pso, something like LPSO (Lander).\nunsigned particle_swarm::calc_swarm_size(const field_set& fs) {\n",
      "description": "Explanation"
    },
    {
      "file": "components/learning/moses/moses/moses/metapopulation/metapopulation.h",
      "line": 535,
      "type": "TODO",
      "content": "// TODO: we may want to output the visited status as well",
      "context": "    // metapopulation. This function is used for fine logging to\n    // deeply probe the metapopulation.\n    //\n    // TODO: we may want to output the visited status as well\n    std::ostream& ostream_metapop(std::ostream&, int n = INT_MAX) const;\n\nprivate:\n",
      "description": "we may want to output the visited status as well"
    },
    {
      "file": "components/learning/moses/moses/moses/metapopulation/merging.cc",
      "line": 261,
      "type": "FIXME",
      "content": "// XXX FIXME: we should use a pointer set for scored_combo_tree_set",
      "context": "        logger().debug(\"Compute behavioral score of %d selected candidates\",\n                       candidates.size());\n\n        // XXX FIXME: we should use a pointer set for scored_combo_tree_set\n        // This would avoid some pointless copying here and a few other\n        // places.  This is easier said than done, because the stupid\n        // domination code is so snarky and icky.  Domination should die.\n",
      "description": "we should use a pointer set for scored_combo_tree_set"
    },
    {
      "file": "components/learning/moses/moses/moses/metapopulation/merging.cc",
      "line": 404,
      "type": "TODO",
      "content": "// TODO: Make population cap size-sensitive to exemplar complexity.",
      "context": "    // formula was arrived at via some ad-hoc experimentation.  A default\n    // value of _params.cap_coef=50 seems to work well.\n    //\n    // TODO: Make population cap size-sensitive to exemplar complexity.\n    // Large exemplars should result in smaller population sizes to maintain\n    // efficiency. Consider implementing adaptive sizing based on exemplar metrics.\n    //\n",
      "description": "Make population cap size-sensitive to exemplar complexity."
    },
    {
      "file": "components/learning/moses/moses/moses/metapopulation/merging.cc",
      "line": 552,
      "type": "FIXME",
      "content": "// XXX FIXME looks to me like it++ can often be collaed twice within this loop!",
      "context": "                    }\n                }\n\n// XXX FIXME looks to me like it++ can often be collaed twice within this loop!\n                prev_it = it++;\n            }\n\n",
      "description": "looks to me like it++ can often be collaed twice within this loop!"
    },
    {
      "file": "components/learning/moses/moses/moses/metapopulation/metapopulation.cc",
      "line": 222,
      "type": "FIXME",
      "content": "// XXX FIXME should probably not recompute every time ...",
      "context": "    if (not _params.do_boosting)\n        return _best_cscore;\n\n    // XXX FIXME should probably not recompute every time ...\n    // need to figure who is calling this method, and what they are expecting.\n    return _cscorer.get_cscore(_ensemble.get_ensemble());\n}\n",
      "description": "should probably not recompute every time ..."
    },
    {
      "file": "components/learning/moses/moses/moses/scoring/scoring_base.h",
      "line": 124,
      "type": "TODO",
      "content": "// XXX TODO should be a std::valarray not a vector.",
      "context": "\n    /// A vector of per-bscore weights, used to tote up the behavioral\n    /// score into a single number.\n    // XXX TODO should be a std::valarray not a vector.\n    virtual void update_weights(const std::vector<double>&);\n\n    /// Return the amount by which the bscore differs from a perfect\n",
      "description": "should be a std::valarray not a vector."
    },
    {
      "file": "components/learning/moses/moses/moses/scoring/time_dispersion.cc",
      "line": 43,
      "type": "TODO",
      "content": "// TODO multipler other than 1 is not supported yet",
      "context": "      _granularity(granularity), _multiplier(multiplier),\n      _pressure(time_dispersion_pressure), _exponent(time_dispersion_exponent)\n{\n    // TODO multipler other than 1 is not supported yet\n    OC_ASSERT(_multiplier == 1, \"Multiplier other than 1 is not supported yet\");\n\n    // Set of timestamp classes\n",
      "description": "multipler other than 1 is not supported yet"
    },
    {
      "file": "components/learning/moses/moses/moses/scoring/scoring_base.cc",
      "line": 108,
      "type": "FIXME",
      "content": "// XXX FIXME complexity_t should be a double not an int ...",
      "context": "        norm += w;\n    }\n\n    // XXX FIXME complexity_t should be a double not an int ...\n    return (complexity_t) floor (cpxy / norm + 0.5);\n}\n\n",
      "description": "complexity_t should be a double not an int ..."
    },
    {
      "file": "components/learning/moses/moses/moses/scoring/discriminating_bscore.cc",
      "line": 486,
      "type": "TODO",
      "content": "// XXX TODO -- should not return the penalties as part of the bscore,",
      "context": "                  float hardness)\n    : discriminating_bscore(ct, min_recall, max_recall, hardness)\n{\n    // XXX TODO -- should not return the penalties as part of the bscore,\n    // since this messes up boosting.\n    _size = ct.size() + 2;\n}\n",
      "description": "-- should not return the penalties as part of the bscore,"
    },
    {
      "file": "components/learning/moses/moses/moses/moses/local_moses.cc",
      "line": 180,
      "type": "TODO",
      "content": "// TODO use the option of the output",
      "context": "                   << \"\\t\" << ds.max;  // max distance\n\n                // diversity stats over all best n candidates of the metapopulation\n                // TODO use the option of the output\n                auto best_ds = mp.gather_diversity_stats(pa.max_cnd_output);\n                ss << \"\\t\" << best_ds.count // number of pairs of candidates\n                   << \"\\t\" << best_ds.mean  // average distance\n",
      "description": "use the option of the output"
    },
    {
      "file": "components/learning/moses/moses/moses/moses/partial.cc",
      "line": 96,
      "type": "TODO",
      "content": "// TODO: Improve generation tracking by getting actual number",
      "context": "\n        _moses_params.max_evals -= _num_evals;\n\n        // TODO: Improve generation tracking by getting actual number\n        // of generations run from MOSES and subtracting it here.\n        // Currently no easy API exists to retrieve this information.\n        _moses_params.max_gens -= _num_gens;\n",
      "description": "Improve generation tracking by getting actual number"
    },
    {
      "file": "components/learning/moses/moses/moses/moses/mpi_moses.cc",
      "line": 201,
      "type": "TODO",
      "content": "// XXX TODO -- trim the deme down, before sending, by using the worst acceptable score.",
      "context": "/// send_deme -- send the completed deme from the worker back to root\n///\n/// This sends a pretty big glob.\n// XXX TODO -- trim the deme down, before sending, by using the worst acceptable score.\nvoid moses_mpi_comm::send_deme(const metapopulation& mp, int n_evals)\n{\n    MPI::COMM_WORLD.Send(&n_evals, 1, MPI::INT, ROOT_NODE, MSG_NUM_EVALS);\n",
      "description": "-- trim the deme down, before sending, by using the worst acceptable score."
    },
    {
      "file": "components/learning/moses/moses/moses/moses/mpi_moses.cc",
      "line": 482,
      "type": "TODO",
      "content": "// TODO: Optimize statistics printing frequency to reduce output volume.",
      "context": "                thread_count--;\n                });\n\n// TODO: Optimize statistics printing frequency to reduce output volume.\n        // Consider printing detailed stats every N iterations instead of every iteration.\n        // Print stats in a way that makes them easy to graph.\n        // (columns of tab-seprated numbers)\n",
      "description": "Optimize statistics printing frequency to reduce output volume."
    },
    {
      "file": "components/learning/moses/moses/moses/moses/types.h",
      "line": 210,
      "type": "TODO",
      "content": "// TODO this should be a std::valarray not std::vector but I am too",
      "context": "/// in reference to a particular table of data.  Exactly which tree it\n/// is, and which table, is implicit.\n//\n// TODO this should be a std::valarray not std::vector but I am too\n// lazy to make the switch right now.\nstruct behavioral_score : public std::vector<score_t>\n{\n",
      "description": "this should be a std::valarray not std::vector but I am too"
    },
    {
      "file": "components/learning/moses/moses/moses/deme/deme_expander.cc",
      "line": 441,
      "type": "TODO",
      "content": "// TODO: DO NOT CHANGE THE MAX SCORE IF USER SET IT: BUT THAT",
      "context": "                // dynamically selected, it might be less that the global target;\n                // that is, the deme might not be able to reach the best score.)\n                //\n                // TODO: DO NOT CHANGE THE MAX SCORE IF USER SET IT: BUT THAT\n                // OPTION ISN'T GLOBAL WHAT TO DO?\n                //\n                // But why would we want to over-ride the best-possible score?\n",
      "description": "DO NOT CHANGE THE MAX SCORE IF USER SET IT: BUT THAT"
    },
    {
      "file": "components/learning/moses/moses/moses/deme/deme_expander.cc",
      "line": 457,
      "type": "TODO",
      "content": "// TODO: re-enable that once best_possible_bscore is fixed",
      "context": "                              \"terminate deme search. Except I think this \"\n                              \"is fixed now. It needs review and testing.\");\n\n                // TODO: re-enable that once best_possible_bscore is fixed\n                // I think its now fixed, but I'm not sure.  It needs to be\n                // reviewed and tested.\n#if THIS_IS_DISABLED_UNTIL_ABOVE_IS_FIXED\n",
      "description": "re-enable that once best_possible_bscore is fixed"
    },
    {
      "file": "components/learning/moses/moses/moses/deme/deme_expander.cc",
      "line": 502,
      "type": "FIXME",
      "content": "// XXX FIXME this is a bug .. the user may have specified that",
      "context": "    if (_params.fstor) {\n        // reset scorer to use all variables (important so that\n        // behavioral score is consistent across generations\n        // XXX FIXME this is a bug .. the user may have specified that\n        // certain incdexes should be ignored, and this just wipes\n        // those out...\n        _cscorer.ignore_cols(std::set<arity_t>());\n",
      "description": "this is a bug .. the user may have specified that"
    },
    {
      "file": "components/learning/moses/moses/moses/eda/replacement.h",
      "line": 62,
      "type": "TODO",
      "content": "// TODO: I think it might be a little more efficent to use the",
      "context": "// Replace the most similar individual, where similarity is determined by\n// the hamming distance.\n//\n// TODO: I think it might be a little more efficent to use the\n// hamming_distance as a sort comparison operator, and hand off the whole\n// thing to std:nth_element, and let that class figure out who is close or\n// not.  This avoids the use of doubly-nested loops, and multiple redundant\n",
      "description": "I think it might be a little more efficent to use the"
    },
    {
      "file": "components/learning/moses/moses/moses/representation/representation.cc",
      "line": 51,
      "type": "TODO",
      "content": "// XXX TODO: One might think that varying the stepsize, i.e. shrinking",
      "context": "// Stepsize should be roughly the standard-deviation of the expected\n// distribution of the contin variables.\n//\n// XXX TODO: One might think that varying the stepsize, i.e. shrinking\n// it, as the optimizers tune into a specific value, would be a good\n// thing (so that the optimizer could tune to a more precise value).\n// Unfortunately, a simple experiment in tuning (see below, surrounded\n",
      "description": "One might think that varying the stepsize, i.e. shrinking"
    },
    {
      "file": "components/learning/moses/moses/moses/representation/representation.cc",
      "line": 236,
      "type": "TODO",
      "content": "// XXX TODO need to add support for \"term algebra\" knobs",
      "context": "/// the instance supplied as the argument.\nvoid representation::transform(const instance& inst)\n{\n    // XXX TODO need to add support for \"term algebra\" knobs\n\n    contin_map_it ckb = contin.begin();\n    for (field_set::const_contin_iterator ci = _fields.begin_contin(inst);\n",
      "description": "need to add support for \"term algebra\" knobs"
    },
    {
      "file": "components/learning/moses/moses/moses/representation/build_knobs.cc",
      "line": 316,
      "type": "TODO",
      "content": "/// TODO: measure and compare the resulting performance.",
      "context": "/// can be rather incredibly costly, especially when the exemplars start\n/// getting large.  So the real question is: is the performance cost of\n/// this routine worth the eventual savings when scoring instances?\n/// TODO: measure and compare the resulting performance.\n//\n// Notes to self: hmm. in 5-parity problem, about 2/3 of knobs are\n// disallowed! viz of 6738 probes, 4007 knobs are completely disallowed.\n",
      "description": "measure and compare the resulting performance."
    },
    {
      "file": "components/learning/moses/moses/moses/representation/build_knobs.cc",
      "line": 489,
      "type": "TODO",
      "content": "// TODO: should bias the selection of these, so that",
      "context": "        }\n    }\n\n    // TODO: should bias the selection of these, so that\n    // larger subtrees are preferred .. !? why?\n\n    unsigned max_pairs = permitted_perms.size();\n",
      "description": "should bias the selection of these, so that"
    },
    {
      "file": "components/learning/moses/moses/moses/representation/build_knobs.cc",
      "line": 581,
      "type": "TODO",
      "content": "// TODO: Benchmark and clarify optimal breakeven point across different problem sizes.",
      "context": "    // The number of 30K is a wild guesstimate, based on recent\n    // measurements of relatively simple exemplars; its maybe even\n    // too low.  For large exemplars, it might be too big !?\n    // TODO: Benchmark and clarify optimal breakeven point across different problem sizes.\n#define BREAKEVEN 30000\n    size_t np = perms.size();\n    int nthr = 1 + np / BREAKEVEN;\n",
      "description": "Benchmark and clarify optimal breakeven point across different problem sizes."
    },
    {
      "file": "components/learning/moses/moses/moses/representation/build_knobs.cc",
      "line": 690,
      "type": "TODO",
      "content": "// XXX TODO: Is this really optimal?  The below adds an entire copy",
      "context": "        }\n    }\n\n    // XXX TODO: Is this really optimal?  The below adds an entire copy\n    // of the tree at it, which clearly increases the overall complexity.\n    // But is this really a wise thig to do? It seems gratuitous, and it's\n    // not obvious that knobs from this flipped tree will yeild benefits,\n",
      "description": "Is this really optimal?  The below adds an entire copy"
    },
    {
      "file": "components/learning/moses/moses/moses/representation/build_knobs.cc",
      "line": 1181,
      "type": "TODO",
      "content": "//TODO: should bias the selection of these (and possibly choose larger subtrees)",
      "context": "        perms.push_back(tr);\n\n    //and n random pairs out of the total  2 * choose(n,2) = n * (n - 1) of these\n    //TODO: should bias the selection of these (and possibly choose larger subtrees)\n    lazy_random_selector randpair(n * (n - 1));\n\n    dorepeat(n) {\n",
      "description": "should bias the selection of these (and possibly choose larger subtrees)"
    },
    {
      "file": "components/learning/moses/moses/moses/representation/build_knobs.cc",
      "line": 1283,
      "type": "TODO",
      "content": "// XXX TODO this below is clearly unfinished, broken, etc.",
      "context": "    }\n}\n\n// XXX TODO this below is clearly unfinished, broken, etc.\n// and can't possibly work ... \nvoid build_knobs::ann_canonize(pre_it it)\n{\n",
      "description": "this below is clearly unfinished, broken, etc."
    },
    {
      "file": "components/learning/moses/moses/moses/representation/build_knobs.cc",
      "line": 1342,
      "type": "FIXME",
      "content": "//FIXME: now just attaches to the first output",
      "context": "    cout << \"Created node: \" << new_node << endl;\n\n    //now attach the subtree to the hidden nodes\n    //FIXME: now just attaches to the first output\n    sib_it first_hidden = it.begin();\n\n    _exemplar.insert_subtree(first_hidden.begin(),new_node.begin());\n",
      "description": "now just attaches to the first output"
    },
    {
      "file": "components/learning/moses/moses/moses/main/problem-params.h",
      "line": 46,
      "type": "FIXME",
      "content": "// XXX FIXME TODO The structure below should be split into multiple",
      "context": "\nnamespace opencog { namespace moses {\n\n// XXX FIXME TODO The structure below should be split into multiple\n// parts, with each sub-part responsible for picking out the argv's\n// that it cares about. Unfortunately, this requires getting rid of\n// boost::program_options (because boost::program_options does not\n",
      "description": "TODO The structure below should be split into multiple"
    },
    {
      "file": "components/learning/moses/moses/moses/main/table-problems.cc",
      "line": 138,
      "type": "FIXME",
      "content": "// XXX FIXME -- the multiple tables should be merged into one.",
      "context": "    }\n    logger().info(\"Number of rows in tables = %d\", num_rows);\n\n    // XXX FIXME -- the multiple tables should be merged into one.\n    ctable = _ctables.front();\n    table = _tables.front();\n\n",
      "description": "-- the multiple tables should be merged into one."
    },
    {
      "file": "components/learning/moses/moses/moses/main/table-problems.cc",
      "line": 150,
      "type": "FIXME",
      "content": "// XXX FIXME .. check that they all have the same signature.",
      "context": "    arity = table.get_arity();\n\n    // Check that all input data files have the same arity\n    // XXX FIXME .. check that they all have the same signature.\n    if (_tables.size() > 1) {\n        for (size_t i = 1; i < _tables.size(); ++i) {\n            combo::arity_t test_arity = _tables[i].get_arity();\n",
      "description": ".. check that they all have the same signature."
    },
    {
      "file": "components/learning/moses/moses/moses/main/problem-params.cc",
      "line": 166,
      "type": "TODO",
      "content": "// XXX TODO: make this print correctly, instead of using brackets.",
      "context": "    using namespace std;\n\n    // Declare the supported options.\n    // XXX TODO: make this print correctly, instead of using brackets.\n    desc.add_options()\n\n        // General options\n",
      "description": "make this print correctly, instead of using brackets."
    },
    {
      "file": "components/learning/moses/moses/comboreduct/type_checker/type_tree.cc",
      "line": 599,
      "type": "TODO",
      "content": "// XXX TODO the code below was modified to allow arg lists of",
      "context": "            // then check that a1 inherits from T1, and that a2, a3\n            // and a4 inherit from T2.  T3 is the output type.\n\n            // XXX TODO the code below was modified to allow arg lists of\n            // mixed type, e.g. so that the cond primitive could be\n            // supported (as the current definition of cond alternates\n            // between boolean-valued predicates, and the result type).\n",
      "description": "the code below was modified to allow arg lists of"
    },
    {
      "file": "components/learning/moses/moses/comboreduct/type_checker/type_tree.h",
      "line": 235,
      "type": "TODO",
      "content": "// TODO : lambda",
      "context": "//\n// intersection of ill_formed and T is ill_formed\n//\n// TODO : lambda\n//\n// Of course the case if T1 inherit T2 then interection of T1 and T2\n// is T1 is also implemented. If the interection is ill_formed or\n",
      "description": "lambda"
    },
    {
      "file": "components/learning/moses/moses/comboreduct/interpreter/interpreter.cc",
      "line": 336,
      "type": "TODO",
      "content": "// XXX TODO: contin_if should go away.",
      "context": "            return (i == id::logical_true ? 1.0 : 0.0);\n        }\n\n        // XXX TODO: contin_if should go away.\n        case id::contin_if :\n        case id::cond : {\n            sib_it sib = it.begin();\n",
      "description": "contin_if should go away."
    },
    {
      "file": "components/learning/moses/moses/comboreduct/interpreter/eval.cc",
      "line": 530,
      "type": "TODO",
      "content": "// XXX TODO: contin_if should go away.",
      "context": "            return eval_throws_tree(bmap, exp_tr);\n        }\n\n        // XXX TODO: contin_if should go away.\n        case id::contin_if :\n        case id::cond : {\n            sib_it sib = it.begin();\n",
      "description": "contin_if should go away."
    },
    {
      "file": "components/learning/moses/moses/comboreduct/combo/descriptions.cc",
      "line": 45,
      "type": "TODO",
      "content": "// ToDo: would be nice to have a more Caml/Haskell style syntax here,",
      "context": "// with builtins as indicies, within the singleton class builtin_properties.\n// This array should not have any other usages.\n//\n// ToDo: would be nice to have a more Caml/Haskell style syntax here,\n// right?\nstatic const builtin_description bd[] =\n{\n",
      "description": "would be nice to have a more Caml/Haskell style syntax here,"
    },
    {
      "file": "components/learning/moses/moses/comboreduct/table/table_io.cc",
      "line": 955,
      "type": "TODO",
      "content": "// TODO could be simplified, optimized, etc",
      "context": "            // It is sparse\n            is_sparse = is_sparse || string::npos != line.find(sparse_delim);\n            if (is_sparse) { // just get out\n                // TODO could be simplified, optimized, etc\n                in.seekg(beg);\n                in.clear();         // in case it has reached the eof\n                return in;\n",
      "description": "could be simplified, optimized, etc"
    },
    {
      "file": "components/learning/moses/moses/comboreduct/table/table_io.cc",
      "line": 1078,
      "type": "TODO",
      "content": "// TODO: this could definitely be optimized",
      "context": "\n    if (is_sparse) {\n        // fallback on the old loader\n        // TODO: this could definitely be optimized\n        OC_ASSERT(timestamp_feature.empty(), \"Timestamp feature not implemented\");\n        return istreamTable_OLD(in, tab, target_feature, ignore_features);\n    } else {\n",
      "description": "this could definitely be optimized"
    },
    {
      "file": "components/learning/moses/moses/comboreduct/table/table.h",
      "line": 692,
      "type": "TODO",
      "content": "// XXX TODO WARNING ERROR: builtin hardcoded shit!!!",
      "context": "        auto it = filter.cbegin();\n        for (unsigned i = 0; i < seq.size(); ++i) {\n            if (it != filter.cend() && (typename F::value_type)i == *it) {\n                // XXX TODO WARNING ERROR: builtin hardcoded shit!!!\n                res.push_back(seq.get_at<builtin>(i));\n                ++it;\n            } else {\n",
      "description": "WARNING ERROR: builtin hardcoded shit!!!"
    },
    {
      "file": "components/learning/moses/moses/comboreduct/table/table.h",
      "line": 696,
      "type": "TODO",
      "content": "// XXX TODO WARNING ERROR: builtin hardcoded shit!!!",
      "context": "                res.push_back(seq.get_at<builtin>(i));\n                ++it;\n            } else {\n                // XXX TODO WARNING ERROR: builtin hardcoded shit!!!\n                res.push_back(id::null_vertex);\n            }\n        }\n",
      "description": "WARNING ERROR: builtin hardcoded shit!!!"
    },
    {
      "file": "components/learning/moses/moses/comboreduct/table/table.h",
      "line": 1352,
      "type": "TODO",
      "content": "// XXX TODO, it would be easier if KLD took a sorted list",
      "context": "            }\n        }\n\n        // XXX TODO, it would be easier if KLD took a sorted list\n        // as the argument.\n        std::vector<contin_t> p, q;\n        for (auto pr : sorted_list) {\n",
      "description": ", it would be easier if KLD took a sorted list"
    },
    {
      "file": "components/learning/moses/moses/comboreduct/table/table.h",
      "line": 1366,
      "type": "TODO",
      "content": "// XXX TODO remove this print, for better performance.",
      "context": "        // Also a problem, this is returning values greater than 1.0;\n        // I thought that IC was supposed to max out at 1.0 !?\n        contin_t ic = - KLD(p,q);\n        // XXX TODO remove this print, for better performance.\n        unsigned idx = *(fs.begin());\n        logger().debug() <<\"Contin MI for feat=\" << idx << \" ic=\" << ic;\n        return ic;\n",
      "description": "remove this print, for better performance."
    },
    {
      "file": "components/learning/moses/moses/comboreduct/table/table.cc",
      "line": 409,
      "type": "TODO",
      "content": "// XXX TODO replace this by the util p_norm function.",
      "context": "    return rhs.get_label() == label;\n}\n\n// XXX TODO replace this by the util p_norm function.\ncontin_t OTable::abs_distance(const OTable& ot) const\n{\n    OC_ASSERT(ot.size() == size());\n",
      "description": "replace this by the util p_norm function."
    },
    {
      "file": "components/learning/moses/moses/comboreduct/table/table.cc",
      "line": 434,
      "type": "TODO",
      "content": "// XXX TODO replace this by the util p_norm function.",
      "context": "    return res;\n}\n\n// XXX TODO replace this by the util p_norm function.\ncontin_t OTable::sum_squared_error(const OTable& ot) const\n{\n    OC_ASSERT(ot.size() == size());\n",
      "description": "replace this by the util p_norm function."
    },
    {
      "file": "components/learning/moses/moses/comboreduct/table/table.cc",
      "line": 848,
      "type": "TODO",
      "content": "// XXX TODO replace this by the util p_norm function.",
      "context": "\n// -------------------------------------------------------\n\n// XXX TODO replace this by the util p_norm function.\ncomplete_truth_table::size_type\ncomplete_truth_table::hamming_distance(const complete_truth_table& other) const\n{\n",
      "description": "replace this by the util p_norm function."
    },
    {
      "file": "components/learning/moses/moses/comboreduct/reduct/logical_rules.cc",
      "line": 100,
      "type": "TODO",
      "content": "// XXX TODO: I don't understand why this is not damaging contin_if  !??",
      "context": "    // Most nodes take simple lists; but not cond. Cond takes clauses,\n    // which are pairs. If we remove the condition, we must also remove\n    // the consequent.\n// XXX TODO: I don't understand why this is not damaging contin_if  !??\n// But .. umm, maybe build_knobs is not creating any kinds of contin_if's\n// that can be damaged... well, no matter, because thes if's will be\n// replaced by cond... \n",
      "description": "I don't understand why this is not damaging contin_if  !??"
    },
    {
      "file": "components/learning/moses/moses/comboreduct/reduct/mixed_rules.cc",
      "line": 1228,
      "type": "TODO",
      "content": "//check if 0<-(y+pi) -> false //TODO",
      "context": "                }\n            }\n            else if(*copy_tr.begin()==id::logical_false) {\n                //check if 0<-(y+pi) -> false //TODO\n                combo_tree copy2_tr = tr.subtree(sib_it(it), tr.next_sibling(sib_it(it)));\n                //copy old assumptions, begin\n                sib_it bna = copy2_tr.begin(); //before new assumption\n",
      "description": ""
    },
    {
      "file": "components/learning/moses/moses/comboreduct/reduct/contin_rules.cc",
      "line": 964,
      "type": "TODO",
      "content": "// TODO:  sin(*(-1 x)) -> -sin(x)",
      "context": "// or more generally\n// sin(sum x_i + sum c_j) -> sin(sum x_i + ((sum c_j)+pi)%2pi -pi\n//\n// TODO:  sin(*(-1 x)) -> -sin(x)\n// The above is frequently seen in real-life ...\nvoid reduce_sin::operator()(combo_tree& tr, combo_tree::iterator it) const\n{\n",
      "description": "sin(*(-1 x)) -> -sin(x)"
    },
    {
      "file": "components/learning/moses/moses/comboreduct/main/action-reductor.cc",
      "line": 94,
      "type": "TODO",
      "content": "// TODO -- replace this by cond",
      "context": "    cout << \"output type \" << ba2->get_output_type_tree() << endl;\n\n#if 0\n    // TODO -- replace this by cond\n    cout << \"6----------------\" << endl;\n\n    cout << \"arity \" << (int)get_arity(id::boolean_if) << endl;\n",
      "description": "-- replace this by cond"
    },
    {
      "file": "components/learning/moses/moses/comboreduct/main/eval-table.cc",
      "line": 147,
      "type": "FIXME",
      "content": "// XXX FIXME",
      "context": "    }\n\n    // HERE WE ARE ASSUMING THAT THE INPUT FILE HAS A HEADER!!!\n// XXX FIXME\n    vector<string> header = get_header(pa.input_table_file);\n\n    // Add to ignore_values (header - all_unique_variables - target feature)\n",
      "description": ""
    },
    {
      "file": "atomspace/opencog/guile/SchemeSmobAtom.cc",
      "line": 84,
      "type": "FIXME",
      "content": "// XXX FIXME. Work around the despicable, horrible guile UTF8 handling.",
      "context": "\n/* ============================================================== */\n\n// XXX FIXME. Work around the despicable, horrible guile UTF8 handling.\n// I am flabbergasted. The guile people are smart, but they could not have\n// possibly picked a crappier string handling design. Fuck me. See\n// https://stackoverflow.com/questions/79329532/c-c-encode-binary-into-utf8\n",
      "description": ". Work around the despicable, horrible guile UTF8 handling."
    },
    {
      "file": "atomspace/opencog/guile/modules/ExecSCM.cc",
      "line": 73,
      "type": "FIXME",
      "content": "// XXX FIXME: can we fix cython to not do this, already?",
      "context": "// NOTE HACK ALERT This needs to be static, in order for python to\n// work correctly.  The problem is that python keeps creating and\n// destroying this class, but it expects things to stick around.\n// XXX FIXME: can we fix cython to not do this, already?\n// Oh well. I guess that's OK, since the definition is meant to be\n// for the lifetime of the process, anyway.\nstd::vector<FunctionWrap*>* ExecSCM::_binders = nullptr;\n",
      "description": "can we fix cython to not do this, already?"
    },
    {
      "file": "atomspace/opencog/query/InitiateSearchMixin.cc",
      "line": 126,
      "type": "FIXME",
      "content": "// XXX FIXME; we should be using ptm->isVariable() instead !?",
      "context": "\tType t = h->get_type();\n\tif (_nameserver.isNode(t))\n\t{\n\t\t// XXX FIXME; we should be using ptm->isVariable() instead !?\n\t\tif (VARIABLE_NODE != t and GLOB_NODE != t and SIGN_NODE != t)\n\t\t{\n\t\t\twidth = h->getIncomingSetSize();\n",
      "description": "; we should be using ptm->isVariable() instead !?"
    },
    {
      "file": "atomspace/opencog/query/InitiateSearchMixin.cc",
      "line": 382,
      "type": "TODO",
      "content": "// TODO -- weed out duplicates!",
      "context": "\t}\n\telse\n\t{\n\t\t// TODO -- weed out duplicates!\n\t}\n\treturn true;\n}\n",
      "description": "-- weed out duplicates!"
    },
    {
      "file": "atomspace/opencog/query/NextSearchMixin.cc",
      "line": 167,
      "type": "TODO",
      "content": "// XXX TODO ... Rather than counting the number of variables, we",
      "context": "// can be done in a direct fashion; it resembles the concept of\n// \"unit propagation\" in the DPLL algorithm.\n//\n// XXX TODO ... Rather than counting the number of variables, we\n// should instead look for one with the smallest incoming set.\n// That is because the very next thing that we do will be to\n// iterate over the incoming set of \"pursue\" ... so it could be\n",
      "description": "... Rather than counting the number of variables, we"
    },
    {
      "file": "atomspace/opencog/query/SatisfyMixin.cc",
      "line": 583,
      "type": "FIXME",
      "content": "// XXX FIXME terrible hack.",
      "context": "\t\t// pure absent is found.\n\t\tif (is_pure_absent)\n\t\t{\n\t\t\t// XXX FIXME terrible hack.\n\t\t\tTermMatchMixin* intu =\n\t\t\t\tdynamic_cast<TermMatchMixin*>(this);\n\t\t\tif (intu->optionals_present()) return false;\n",
      "description": "terrible hack."
    },
    {
      "file": "atomspace/opencog/query/TermMatchMixin.cc",
      "line": 551,
      "type": "TODO",
      "content": "// XXX TODO as discussed on the mailing list, we should perhaps first",
      "context": "\t//       Arg1Atom\n\t//       Arg2Atom\n\t//\n\t// XXX TODO as discussed on the mailing list, we should perhaps first\n\t// see if the following can be found in the atomspace:\n\t//\n\t//   EvaluationLink\n",
      "description": "as discussed on the mailing list, we should perhaps first"
    },
    {
      "file": "atomspace/opencog/query/TermMatchMixin.cc",
      "line": 710,
      "type": "FIXME",
      "content": "// XXX FIXME: worse: this cannot possibly be right when",
      "context": "\t\t// possibilities?  And if they failed to do so, can we even do\n\t\t// anything about that here? Seems like we can't do anything...\n\t\t//\n\t\t// XXX FIXME: worse: this cannot possibly be right when\n\t\t// the ChoiceLink contains presentLinks.\n\t\tfor (const Handle& h : oset)\n\t\t{\n",
      "description": "worse: this cannot possibly be right when"
    },
    {
      "file": "atomspace/opencog/query/Recognizer.cc",
      "line": 126,
      "type": "TODO",
      "content": "// TODO: Change to something better if possible...",
      "context": "\t// mis-matched types are a dead-end.\n\tif (lpat->get_type() != lsoln->get_type()) return false;\n\n\t// TODO: Change to something better if possible...\n\t// What is happening here is to manually call the\n\t// fuzzy_match callback immediately if and only if\n\t// lsoln has one or more GlobNodes AND lpat and lsoln\n",
      "description": "Change to something better if possible..."
    },
    {
      "file": "atomspace/opencog/query/RewriteMixin.cc",
      "line": 162,
      "type": "FIXME",
      "content": "/// XXX FIXME now I see how it can be done. The groupings should",
      "context": "/// to dribble in. Perhaps the engine search could be modified in some\n/// clever way to find groupings in a single batch; but for now, I don't\n/// see how this could be done.\n/// XXX FIXME now I see how it can be done. The groupings should\n/// be converted to marginals, and handled the same way. So this\n/// needs a rewrite. Good thing that almost no one uses this ...\nbool RewriteMixin::propose_grouping(const GroundingMap &var_soln,\n",
      "description": "now I see how it can be done. The groupings should"
    },
    {
      "file": "atomspace/opencog/query/PatternMatchEngine.cc",
      "line": 284,
      "type": "FIXME",
      "content": "/// XXX FIXME: this is currently a weak stop-gap measure to handle",
      "context": "/// Compare the contents of a Present term in the pattern to the\n/// proposed grounding. The term `ptm` points at the Present term.\n///\n/// XXX FIXME: this is currently a weak stop-gap measure to handle\n/// the special case of Present terms embedded in Choice terms.\n/// Present terms that are NOT in a Choice are handled by the\n/// do_next_clause() system, which assumes that Present terms happen\n",
      "description": "this is currently a weak stop-gap measure to handle"
    },
    {
      "file": "atomspace/opencog/query/PatternMatchEngine.cc",
      "line": 1532,
      "type": "FIXME",
      "content": "// XXX FIXME - this is not very elegant. We should probably",
      "context": "\t// If this is some other rando variable that is not part of\n\t// search pattern, i.e. if is is a scoped variable, then\n\t// accept a match to any other alpha-equivalent variable.\n\t// XXX FIXME - this is not very elegant. We should probably\n\t// have a distinct `scoped_link_compare()` function to handle\n\t// this. Right now, the scope_match() callback uses a rather\n\t// screwy and indirect trick to check alpha conversion.\n",
      "description": "- this is not very elegant. We should probably"
    },
    {
      "file": "atomspace/opencog/query/PatternMatchEngine.cc",
      "line": 1982,
      "type": "FIXME",
      "content": "// XXX FIXME: Issue #3016 - Unification with unordered AndLinks",
      "context": "/// form; see `explore_sparse_branches()`.\n///\n//\n// XXX FIXME: Issue #3016 - Unification with unordered AndLinks\n// The current implementation of unordered link permutation exploration\n// in IdenticalLinks stops after finding the first valid permutation \n// instead of continuing to find all possible permutations. This is \n",
      "description": "Issue #3016 - Unification with unordered AndLinks"
    },
    {
      "file": "atomspace/opencog/query/PatternMatchEngine.cc",
      "line": 2049,
      "type": "FIXME",
      "content": "/// XXX FIXME: Right now, this code handles graphs that have only one",
      "context": "/// functional group, and the glob will end up holding the moiety that\n/// is not a part of the functional group.\n///\n/// XXX FIXME: Right now, this code handles graphs that have only one\n/// single sparse search.   Nested sparse searches are not supported;\n/// to implement those, its \"easy\": implement the same flow control as\n/// the unordered_explore steppers. I'm lzay, today, so I am not doing\n",
      "description": "Right now, this code handles graphs that have only one"
    },
    {
      "file": "atomspace/opencog/query/PatternMatchEngine.cc",
      "line": 2065,
      "type": "TODO",
      "content": "// XXX TODO FIXME. The ptm needs to be decomposed into connected",
      "context": "{\n\tlogmsg(\"Explore sparse: Start exploration\");\n\n\t// XXX TODO FIXME. The ptm needs to be decomposed into connected\n\t// components. Then only the connected components need to be walked\n\t// over.  That would be much more efficient.\n\tdo\n",
      "description": "FIXME. The ptm needs to be decomposed into connected"
    },
    {
      "file": "atomspace/opencog/query/PatternMatchEngine.cc",
      "line": 2223,
      "type": "FIXME",
      "content": "/// XXX FIXME -- do the above.",
      "context": "/// -- build a connectivity map, just like the one for clauses\n/// -- build a clause_variables struct, but just for this term\n/// -- search for the thinnest joint, just like `get_next_clause`\n/// XXX FIXME -- do the above.\n///\nbool PatternMatchEngine::next_untried_present(const PatternTermPtr& parent,\n                                              const PatternTermPtr& clause,\n",
      "description": "-- do the above."
    },
    {
      "file": "atomspace/opencog/query/PatternMatchEngine.cc",
      "line": 2445,
      "type": "TODO",
      "content": "// XXX TODO make sure that all variables in the clause have",
      "context": "\t\t                 << \"Parent has evaluatable but code path was expected to be unreachable. \"\n\t\t                 << \"Clause: \" << clause->getQuote()->to_string();\n\t\t// Continue with the evaluation anyway, but log the issue\n\t\t// XXX TODO make sure that all variables in the clause have\n\t\t// been grounded!  If they're not, something is badly wrong!\n\t\tlogmsg(\"Term inside evaluatable, move up to it's top:\",\n\t\t\t       clause->getQuote());\n",
      "description": "make sure that all variables in the clause have"
    },
    {
      "file": "atomspace/opencog/cython/PythonEval.cc",
      "line": 1427,
      "type": "FIXME",
      "content": "// XXX FIXME this does a lot of wasteful string copying.",
      "context": "\nvoid PythonEval::eval_expr(const std::string& partial_expr)\n{\n    // XXX FIXME this does a lot of wasteful string copying.\n    std::string expr = partial_expr;\n    size_t nl = expr.find_first_of(\"\\n\\r\");\n    while (std::string::npos != nl)\n",
      "description": "this does a lot of wasteful string copying."
    },
    {
      "file": "atomspace/opencog/atomspace/AtomSpace.cc",
      "line": 139,
      "type": "TODO",
      "content": "// TODO: this should probably be moved to a method on class Atom.",
      "context": "        }\n\n        // Check the values...\n        // TODO: this should probably be moved to a method on class Atom.\n        if (check_values)\n        {\n            HandleSet keys_first = atom_first->getKeys();\n",
      "description": "this should probably be moved to a method on class Atom."
    },
    {
      "file": "atomspace/opencog/atomspace/AtomSpace.cc",
      "line": 273,
      "type": "FIXME",
      "content": "// Fixme maybe later someday, if/when this is needed.",
      "context": "\t// having one AtomSpace be placed as a member into many others,\n\t// except that we don't have any viable mechanisms for such multiple\n\t// membership, and so I don't know how to treat this right now.\n\t// Fixme maybe later someday, if/when this is needed.\n\tif (not (nullptr == _atom_space or as == nullptr))\n\t\tthrow RuntimeException(TRACE_INFO,\n\t\t\t\"At this time, an AtomSpace can only be placed in one other\\n\"\n",
      "description": "maybe later someday, if/when this is needed."
    },
    {
      "file": "atomspace/opencog/atomspace/AtomSpace.h",
      "line": 524,
      "type": "FIXME",
      "content": "// XXX FIXME Users should call StorageNode::add_nocheck() instead.",
      "context": "\n    /* ----------------------------------------------------------- */\n    // Not for public use! Only StorageNodes get to call this!\n    // XXX FIXME Users should call StorageNode::add_nocheck() instead.\n    Handle storage_add_nocheck(const Handle& h) { return add(h); }\n};\n\n",
      "description": "Users should call StorageNode::add_nocheck() instead."
    },
    {
      "file": "atomspace/opencog/atoms/flow/FormulaPredicateLink.cc",
      "line": 41,
      "type": "FIXME",
      "content": "/// XXX FIXME - in the future, some user is going to want to include",
      "context": "/// not typed, and there are *two* bodies, each body returning one\n/// component of the final truth value...\n///\n/// XXX FIXME - in the future, some user is going to want to include\n/// variable declarations, and/or an explicit Lambda in the body, for\n/// some reason that I cannot imagine.  The code below will then fail.\n/// For now, ignore this possibility.\n",
      "description": "- in the future, some user is going to want to include"
    },
    {
      "file": "atomspace/opencog/atoms/flow/FilterLink.cc",
      "line": 615,
      "type": "TODO",
      "content": "// XXX TODO FIXME -- if vex is a stream, e.g. a QueueValue,",
      "context": "\t{\n\t\tvex = _outgoing[1]->execute(as, silent);\n\n\t\t// XXX TODO FIXME -- if vex is a stream, e.g. a QueueValue,\n\t\t// then we should construct another Queue as the return value,\n\t\t// and perform filtering on-demand.\n\t\tif (vex->is_type(LINK_VALUE))\n",
      "description": "FIXME -- if vex is a stream, e.g. a QueueValue,"
    },
    {
      "file": "atomspace/opencog/atoms/flow/CollectionOfLink.cc",
      "line": 70,
      "type": "TODO",
      "content": "// TODO: Handle executable _outgoing[0] by executing it first.",
      "context": "\n\t_have_typespec = true;\n\n\t// TODO: Handle executable _outgoing[0] by executing it first.\n\t// TODO: Support complex type signatures beyond simple TYPE_NODE.\n\t// Current implementation assumes simple type specification.\n\tif (not _outgoing[0]->is_type(TYPE_NODE))\n",
      "description": "Handle executable _outgoing[0] by executing it first."
    },
    {
      "file": "atomspace/opencog/atoms/flow/CollectionOfLink.cc",
      "line": 71,
      "type": "TODO",
      "content": "// TODO: Support complex type signatures beyond simple TYPE_NODE.",
      "context": "\t_have_typespec = true;\n\n\t// TODO: Handle executable _outgoing[0] by executing it first.\n\t// TODO: Support complex type signatures beyond simple TYPE_NODE.\n\t// Current implementation assumes simple type specification.\n\tif (not _outgoing[0]->is_type(TYPE_NODE))\n\t\tthrow InvalidParamException(TRACE_INFO,\n",
      "description": "Support complex type signatures beyond simple TYPE_NODE."
    },
    {
      "file": "atomspace/opencog/atoms/flow/ValueOfLink.cc",
      "line": 84,
      "type": "TODO",
      "content": "// XXX TODO FIXME ... if either of these are executable, then",
      "context": "\t// space; we can add the Atom there, and things will\n\t// trickle out properly in the end.\n\t//\n\t// XXX TODO FIXME ... if either of these are executable, then\n\t// they need to be executed, first, right? Yes, they do! We\n\t// can currently get away with not doing this for two reasons:\n\t// In all existing code, the first Atom is always an anchor,\n",
      "description": "FIXME ... if either of these are executable, then"
    },
    {
      "file": "atomspace/opencog/atoms/join/JoinLink.cc",
      "line": 550,
      "type": "TODO",
      "content": "/// TODO: it might be faster to use hash tables instead of rb-trees",
      "context": "/// think of any way of combining steps (2) and (3) that would avoid\n/// step (4) ... or even would reduce the work for stpe (4). Oh well.\n///\n/// TODO: it might be faster to use hash tables instead of rb-trees\n/// i.e. to use UnorderedHandleSet instead of HandleSet. XXX FIXME.\nHandleSet JoinLink::supremum(AtomSpace* as, bool silent,\n                             Traverse& trav) const\n",
      "description": "it might be faster to use hash tables instead of rb-trees"
    },
    {
      "file": "atomspace/opencog/atoms/join/JoinLink.cc",
      "line": 722,
      "type": "FIXME",
      "content": "// XXX FIXME this is really dumb, using a queue and then",
      "context": "\n\tHandleSet hs = container(as, jcb, silent);\n\n\t// XXX FIXME this is really dumb, using a queue and then\n\t// copying things into it. Whatever. Fix this.\n\tQueueValuePtr qvp(createQueueValue());\n\tfor (const Handle& h : hs)\n",
      "description": "this is really dumb, using a queue and then"
    },
    {
      "file": "atomspace/opencog/atoms/parallel/ExecuteThreadedLink.cc",
      "line": 59,
      "type": "TODO",
      "content": "/// XXX TODO: We could have a non-blocking version of this atom. We",
      "context": "/// Atoms in the set. If the NumberNode is present, then the number of\n/// threads is the smaller of the NumberNode and the seize of the Set.\n///\n/// XXX TODO: We could have a non-blocking version of this atom. We\n/// could just return the QueueValue immediately; the user could check\n/// to see if the queue is closed, to find out if the threads have\n/// finished.\n",
      "description": "We could have a non-blocking version of this atom. We"
    },
    {
      "file": "atomspace/opencog/atoms/pattern/PatternUtils.cc",
      "line": 55,
      "type": "FIXME",
      "content": "// XXX FIXME Are the below needed?",
      "context": "\t\t      or nameserver().isA(clause->getOutgoingAtom(0)->get_type(),\n\t\t                          EVALUATABLE_LINK)))\n\n\t\t// XXX FIXME Are the below needed?\n\t\tor contains_atomtype(clause, DEFINED_PREDICATE_NODE)\n\t\tor contains_atomtype(clause, DEFINED_SCHEMA_NODE)\n\t\tor is_black_box(clause);\n",
      "description": "Are the below needed?"
    },
    {
      "file": "atomspace/opencog/atoms/pattern/PatternLink.cc",
      "line": 146,
      "type": "FIXME",
      "content": "// XXX FIXME, more correct would be to loop over",
      "context": "\t{\n\t\t// The variables for that component are just the variables\n\t\t// that can be found in that component.\n\t\t// XXX FIXME, more correct would be to loop over\n\t\t// _pat.clause_variables and add those. Probably makes\n\t\t// no difference in most cases.\n\t\tFindAtoms fv(_variables.varset);\n",
      "description": ", more correct would be to loop over"
    },
    {
      "file": "atomspace/opencog/atoms/pattern/PatternLink.cc",
      "line": 165,
      "type": "FIXME",
      "content": "// XXX FIXME, this handles `absents`, `always` and `grouping`",
      "context": "\t\tunbundle_clauses(h);\n\n\t\t// Each component consists of the assorted parts.\n\t\t// XXX FIXME, this handles `absents`, `always` and `grouping`\n\t\t// incorrectly.\n\t\tHandleSeq clseq;\n\t\tfor (const PatternTermPtr& ptm: _pat.pmandatory)\n",
      "description": ", this handles `absents`, `always` and `grouping`"
    },
    {
      "file": "atomspace/opencog/atoms/pattern/PatternLink.cc",
      "line": 1039,
      "type": "FIXME",
      "content": "/// XXX FIXME: the code here assumes that the situation is indeed",
      "context": "/// If the ImplicationLink is suitably simple, it can be added\n/// as an ordinary clause, and searched for as if it was \"present\".\n///\n/// XXX FIXME: the code here assumes that the situation is indeed\n/// simple: more complex cases are not handled correctly.  Doing this\n/// correctly would require iterating again, and examining the\n/// contents of the left and right side of the IdenticalLink... ugh.\n",
      "description": "the code here assumes that the situation is indeed"
    },
    {
      "file": "atomspace/opencog/atoms/pattern/PatternLink.cc",
      "line": 1255,
      "type": "FIXME",
      "content": "// XXX FIXME -- this is wrong. What we really want is to",
      "context": "\t\t\t_pat.have_evaluatables = true;\n\t\t\tptm->addEvaluatable();\n\n\t\t\t// XXX FIXME -- this is wrong. What we really want is to\n\t\t\t// identify those clauses that bridge across multiple\n\t\t\t// components... not everything here does so. The\n\t\t\t// get_bridged_components() should be modified to\n",
      "description": "-- this is wrong. What we really want is to"
    },
    {
      "file": "atomspace/opencog/atoms/pattern/PatternLink.cc",
      "line": 1465,
      "type": "FIXME",
      "content": "// XXX FIXME: debug_log() above is more readable than the below.",
      "context": "\nDEFINE_LINK_FACTORY(PatternLink, PATTERN_LINK)\n\n// XXX FIXME: debug_log() above is more readable than the below.\nstd::string PatternLink::to_long_string(const std::string& indent) const\n{\n\tstd::string indent_p = indent + oc_to_string_indent;\n",
      "description": "debug_log() above is more readable than the below."
    },
    {
      "file": "atomspace/opencog/atoms/pattern/PatternTerm.h",
      "line": 80,
      "type": "TODO",
      "content": "// TODO: it would probably be more efficient to swap which of these",
      "context": "\tHandle _handle;\n\tHandle _quote;\n\n\t// TODO: it would probably be more efficient to swap which of these\n\t// two is weak, since I think _outgoing is requested far more often\n\t// than _parent, and having it run faster would be a performance win.\n\tPatternTermPtr _parent;\n",
      "description": "it would probably be more efficient to swap which of these"
    },
    {
      "file": "atomspace/opencog/atoms/execution/Instantiator.cc",
      "line": 289,
      "type": "TODO",
      "content": "// TODO: what about globs?",
      "context": "\t\t\treturn expr;\n\n\t\t// If it is a quoted or shadowed variable don't substitute.\n\t\t// TODO: what about globs?\n\t\tif (VARIABLE_NODE == t and not context_cp.is_free_variable(expr))\n\t\t\treturn expr;\n\n",
      "description": "what about globs?"
    },
    {
      "file": "atomspace/opencog/atoms/execution/Instantiator.cc",
      "line": 599,
      "type": "FIXME",
      "content": "// XXX FIXME Can we defer the addition to the atomspace to an even",
      "context": "\t// We do this here, instead of in walk_tree(), because adding\n\t// atoms to the atomspace is an expensive process.  We can save\n\t// some time by doing it just once, right here, in one big batch.\n\t// XXX FIXME Can we defer the addition to the atomspace to an even\n\t// later time??\n\tif (_as) return _as->add_atom(grounded);\n\treturn grounded;\n",
      "description": "Can we defer the addition to the atomspace to an even"
    },
    {
      "file": "atomspace/opencog/atoms/execution/Instantiator.cc",
      "line": 643,
      "type": "FIXME",
      "content": "// XXX FIXME, we need to get rid of this call entirely, and just",
      "context": "\tif (expr->is_type(NODE) and expr->is_executable())\n\t\treturn expr->execute(_as, silent);\n\n\t// XXX FIXME, we need to get rid of this call entirely, and just\n\t// return expr->execute(_as, silent) instead, like above.\n\t// However, assorted parts are still broken and don't work.\n\tValuePtr vp(instantiate(expr, GroundingMap(), silent));\n",
      "description": ", we need to get rid of this call entirely, and just"
    },
    {
      "file": "atomspace/opencog/atoms/base/Valuation.cc",
      "line": 50,
      "type": "TODO",
      "content": "// XXX TODO -- C++ smart pointers are not atomic; we really",
      "context": "\nvoid Valuation::setValue(const ValuePtr& v)\n{\n\t// XXX TODO -- C++ smart pointers are not atomic; we really\n\t// need to use a lock here, to avoid thread-races.\n\t_value = v;\n}\n",
      "description": "-- C++ smart pointers are not atomic; we really"
    },
    {
      "file": "atomspace/opencog/atoms/core/RewriteLink.h",
      "line": 224,
      "type": "TODO",
      "content": "// TODO: we probably want to",
      "context": "\t */\n\tHandle consume_quotations() const;\n\tstatic Handle consume_quotations(const Variables& variables, const Handle& h,\n\t                                 // TODO: we probably want to\n\t                                 // move quotation,\n\t                                 // needless_quotation,\n\t                                 // clause_root and more in\n",
      "description": "we probably want to"
    },
    {
      "file": "atomspace/opencog/atoms/core/Variables.cc",
      "line": 441,
      "type": "TODO",
      "content": "// XXX TODO type-checking could be lazy; if the function is not",
      "context": "\t\t\t\"Incorrect number of arguments specified, expecting %lu got %lu\",\n\t\t\tvarseq.size(), args.size());\n\n\t// XXX TODO type-checking could be lazy; if the function is not\n\t// actually using one of the args, it's type should not be checked.\n\t// Viz., one of the arguments might be undefined, and that's OK,\n\t// if that argument is never actually used.  Fixing this requires a\n",
      "description": "type-checking could be lazy; if the function is not"
    },
    {
      "file": "atomspace/opencog/atoms/core/RandomChoice.cc",
      "line": 110,
      "type": "FIXME",
      "content": "// XXX FIXME - fix this so it can also choose a single value",
      "context": "///           AtomZ\n///\n\n// XXX FIXME - fix this so it can also choose a single value\n// out of a vector of values.\nValuePtr RandomChoiceLink::execute(AtomSpace* as, bool silent)\n{\n",
      "description": "- fix this so it can also choose a single value"
    },
    {
      "file": "atomspace/opencog/atoms/core/RandomChoice.cc",
      "line": 143,
      "type": "TODO",
      "content": "// XXX TODO if execute() above returns FloatValue, use that!",
      "context": "\t\t\tif (hw->is_executable())\n\t\t\t\thw = HandleCast(hw->execute(as, silent));\n\n\t\t\t// XXX TODO if execute() above returns FloatValue, use that!\n\t\t\tNumberNodePtr nn(NumberNodeCast(hw));\n\t\t\tif (nullptr == nn) // goto uniform;\n\t\t\t\tthrow SyntaxException(TRACE_INFO,\n",
      "description": "if execute() above returns FloatValue, use that!"
    },
    {
      "file": "atomspace/opencog/atoms/core/RandomChoice.cc",
      "line": 179,
      "type": "FIXME",
      "content": "// XXX FIXME, also allow a FloatValue!!",
      "context": "\t\tstd::vector<double> weights;\n\t\tfor (Handle h : ofirst->getOutgoingSet())\n\t\t{\n\t\t\t// XXX FIXME, also allow a FloatValue!!\n\t\t\tif (h->is_executable())\n\t\t\t\th = HandleCast(h->execute(as, silent));\n\n",
      "description": ", also allow a FloatValue!!"
    },
    {
      "file": "atomspace/opencog/atoms/core/TypeNode.h",
      "line": 90,
      "type": "TODO",
      "content": "// XXX TODO ... Some types are defined. In this case,",
      "context": "\tstatic void validate(const std::string& str)\n\t{\n\t\tType t = nameserver().getType(str);\n\t\t// XXX TODO ... Some types are defined. In this case,\n\t\t// verify that the string occurs as a name inside\n\t\t// some DefineLink... if it does, then it's valid.\n\t\t// If it does not, then it's invalid.\n",
      "description": "... Some types are defined. In this case,"
    },
    {
      "file": "atomspace/opencog/atoms/core/RewriteLink.cc",
      "line": 297,
      "type": "TODO",
      "content": "// TODO: the following has no unit test!!! Yet it introduces a",
      "context": "\t// Base case\n\tif (h->is_node())\n\t{\n\t\t// TODO: the following has no unit test!!! Yet it introduces a\n\t\t// bug covered by RewriteLinkUTest::test_consume_quotations_4(),\n\t\t// thus this code is disabled till a unit test it created for it\n\t\t// and we understand what it fixes and how it fixes.\n",
      "description": "the following has no unit test!!! Yet it introduces a"
    },
    {
      "file": "atomspace/opencog/atoms/core/RewriteLink.cc",
      "line": 341,
      "type": "TODO",
      "content": "// TODO: generalize with when Unquote and Quote are apart",
      "context": "\t\t\t// A succession of (Unquote (Quote ..)) is an involution\n\t\t\t// and thus can be remove.\n\t\t\t//\n\t\t\t// TODO: generalize with when Unquote and Quote are apart\n\t\t\tif (child->get_type() == QUOTE_LINK)\n\t\t\t{\n\t\t\t\tquotation.update(child->get_type());\n",
      "description": "generalize with when Unquote and Quote are apart"
    },
    {
      "file": "atomspace/opencog/atoms/core/Checkers.cc",
      "line": 42,
      "type": "FIXME",
      "content": "// XXX FIXME Much of the onfusion below is due to a bug: if the",
      "context": "/// This only performs a very simple kind of type checking;\n/// it does not check deep types, nor does it check arity.\n\n// XXX FIXME Much of the onfusion below is due to a bug: if the\n// types script says something like\n// FOOBAR <- FUNCTION_LINK,BOOL_INPUT_LINK,NUMBER_INPUT_LINK\n// then the Foobar function will fail if given a boolean input:\n",
      "description": "Much of the onfusion below is due to a bug: if the"
    },
    {
      "file": "atomspace/opencog/atoms/core/Checkers.cc",
      "line": 95,
      "type": "FIXME",
      "content": "// XXX FIXME ... Perhaps IntersectionLink, UnionLink will",
      "context": "\t\t// want to forbid it in the future by maybe introducing a\n\t\t// specialized operator to explicitly map the higher order into\n\t\t// the lower order but as of today it is required.\n\t\t// XXX FIXME ... Perhaps IntersectionLink, UnionLink will\n\t\t// resolve this?\n\t\tif (h->is_type(SIMILARITY_LINK) or\n\t\t    h->is_type(MEMBER_LINK))\n",
      "description": "... Perhaps IntersectionLink, UnionLink will"
    },
    {
      "file": "atomspace/opencog/atoms/reduct/DecimateLink.cc",
      "line": 65,
      "type": "FIXME",
      "content": "// XXX FIXME ... both the NumberNode and the FloatValue variations",
      "context": "\t\treturn do_execute(vmask, vi);\n\t}\n\n\t// XXX FIXME ... both the NumberNode and the FloatValue variations\n\t// below make a copy of the mask.  Instead of making a copy, create\n\t// something more efficient/faster. It is, after all, a simple\n\t// test...\n",
      "description": "... both the NumberNode and the FloatValue variations"
    },
    {
      "file": "atomspace/opencog/atoms/reduct/AccumulateLink.cc",
      "line": 71,
      "type": "TODO",
      "content": "// XXX TODO -- we could also handle vectors of strings, by",
      "context": "\t\treturn createFloatValue(acc);\n\t}\n\n\t// XXX TODO -- we could also handle vectors of strings, by\n\t// concatenating them into one long string.  However, for this\n\t// to be generally useful, we'd want to insert whitespace in\n\t// between. But how? One way would be to pass another argument\n",
      "description": "-- we could also handle vectors of strings, by"
    },
    {
      "file": "atomspace/tests/cython/atomspace/test_atomspace.py",
      "line": 362,
      "type": "FIXME",
      "content": "# XXX FIXME is testing the name of the bottom type",
      "context": "    def test_get_type_name(self):\n        self.assertEqual(get_type_name(types.Node), \"Node\")\n        self.assertEqual(get_type_name(2231), \"\")\n        # XXX FIXME is testing the name of the bottom type\n        # a sane thing to do?\n        self.assertEqual(get_type_name(types.NO_TYPE), \"*** Bottom Type! ***\")\n",
      "description": "is testing the name of the bottom type"
    },
    {
      "file": "atomspace-storage/opencog/persist/sexcom/Commands.cc",
      "line": 165,
      "type": "FIXME",
      "content": "// FIXME read above comment.",
      "context": "// (cog-execute-cache! (GetLink ...) (Predicate \"key\") ...)\n// This is complicated, and subject to change...\n// XXX this should be nuked, and replaced by appropriate kind of proxy.\n// FIXME read above comment.\nstd::string Commands::cog_execute_cache(const std::string& cmd)\n{\n\tsize_t pos = 0;\n",
      "description": "read above comment."
    },
    {
      "file": "atomspace-storage/opencog/persist/api/cython/PersistCython.cc",
      "line": 31,
      "type": "FIXME",
      "content": "// XXX FIXME: except for the error messages, most of this code is",
      "context": "\nnamespace opencog {\n\n// XXX FIXME: except for the error messages, most of this code is\n// mostly a cut-n-pate of what's in PersistSCM.cc\n\n// =====================================================================\n",
      "description": "except for the error messages, most of this code is"
    },
    {
      "file": "atomspace-storage/opencog/persist/flow/StoreValueOfLink.cc",
      "line": 61,
      "type": "TODO",
      "content": "// XXX TODO FIXME ... if either of these are executable, then",
      "context": "{\n\tStorageNodePtr stnp = StorageNodeCast(_outgoing[2]);\n\n\t// XXX TODO FIXME ... if either of these are executable, then\n\t// they need to be executed, first, right? Because that's the\n\t// usual intent. Else they'd be wrapped in a DontExecLink, right?\n\t// I'm confused.\n",
      "description": "FIXME ... if either of these are executable, then"
    },
    {
      "file": "atomspace-storage/opencog/persist/flow/FetchValueOfLink.cc",
      "line": 63,
      "type": "TODO",
      "content": "// XXX TODO FIXME ... if either of _outgoing[0] or _outgoing[1]",
      "context": "{\n\tStorageNodePtr stnp = StorageNodeCast(_outgoing[2]);\n\n\t// XXX TODO FIXME ... if either of _outgoing[0] or _outgoing[1]\n\t// are executable, then they need to be executed, first, right?\n\t// Yes, they do. But, for just right now, we don't, to stay\n\t// compatible with ValueOfLink. See comments in that code.\n",
      "description": "FIXME ... if either of _outgoing[0] or _outgoing[1]"
    },
    {
      "file": "atomspace-storage/opencog/persist/csv/table_read.h",
      "line": 38,
      "type": "TODO",
      "content": "// TODO: Should this be a StringValue?",
      "context": "\nnamespace opencog {\n\n// TODO: Should this be a StringValue?\ntypedef std::vector<std::string> string_seq;\n\n/**\n",
      "description": "Should this be a StringValue?"
    },
    {
      "file": "atomspace-storage/opencog/persist/proxy/ProxyNode.cc",
      "line": 141,
      "type": "FIXME",
      "content": "// XXX FIXME. Using this ProxyParametersLink thing is a kind of",
      "context": "// Get our configuration from the DefineLink we live in.\n// Hmm, perhaps this should be a StateLink?\n//\n// XXX FIXME. Using this ProxyParametersLink thing is a kind of\n// cheesy hack, to pass parameters to the ProxyNode. It vaguely\n// resembles the structure of an ExecutionLink, but instead of\n// writing (Execution (Predicate \"foo\") (List (args...)))\n",
      "description": ". Using this ProxyParametersLink thing is a kind of"
    },
    {
      "file": "atomspace-storage/opencog/persist/proxy/WriteBufferProxy.cc",
      "line": 195,
      "type": "FIXME",
      "content": "// XXX FIXME. Buffering these naively, like this, voilates the",
      "context": "void WriteBufferProxy::updateValue(const Handle& atom, const Handle& key,\n                                   const ValuePtr& delta)\n{\n\t// XXX FIXME. Buffering these naively, like this, voilates the\n\t// intent of how this method should work. However, for the\n\t// RocksStorageNode, doing this is harmless. And the\n\t// CogStorageNode is just a pass-through. So there are no\n",
      "description": ". Buffering these naively, like this, voilates the"
    },
    {
      "file": "hypermind/hypermind.hpp",
      "line": 166,
      "type": "TODO",
      "content": "// TODO: Serialize message to wire format (e.g., Protocol Buffers, MessagePack)",
      "context": "    \n    bool send_network_message(Message* msg, const std::string& dest_address) {\n        // STUB: Network serialization not yet implemented\n        // TODO: Serialize message to wire format (e.g., Protocol Buffers, MessagePack)\n        // TODO: Send over actual network transport (ZeroMQ, gRPC, raw TCP/IP)\n        // TODO: Handle network errors and retries\n        // Reference: specs/integrations.zpp SendNetworkMessage operation\n",
      "description": "Serialize message to wire format (e.g., Protocol Buffers, MessagePack)"
    },
    {
      "file": "hypermind/hypermind.hpp",
      "line": 167,
      "type": "TODO",
      "content": "// TODO: Send over actual network transport (ZeroMQ, gRPC, raw TCP/IP)",
      "context": "    bool send_network_message(Message* msg, const std::string& dest_address) {\n        // STUB: Network serialization not yet implemented\n        // TODO: Serialize message to wire format (e.g., Protocol Buffers, MessagePack)\n        // TODO: Send over actual network transport (ZeroMQ, gRPC, raw TCP/IP)\n        // TODO: Handle network errors and retries\n        // Reference: specs/integrations.zpp SendNetworkMessage operation\n        \n",
      "description": "Send over actual network transport (ZeroMQ, gRPC, raw TCP/IP)"
    },
    {
      "file": "hypermind/hypermind.hpp",
      "line": 168,
      "type": "TODO",
      "content": "// TODO: Handle network errors and retries",
      "context": "        // STUB: Network serialization not yet implemented\n        // TODO: Serialize message to wire format (e.g., Protocol Buffers, MessagePack)\n        // TODO: Send over actual network transport (ZeroMQ, gRPC, raw TCP/IP)\n        // TODO: Handle network errors and retries\n        // Reference: specs/integrations.zpp SendNetworkMessage operation\n        \n        NetworkEnvelope envelope;\n",
      "description": "Handle network errors and retries"
    },
    {
      "file": "hypermind/hypermind.hpp",
      "line": 185,
      "type": "TODO",
      "content": "// TODO: Receive data from network transport",
      "context": "    \n    bool receive_network_message(NetworkEnvelope& envelope) {\n        // STUB: Network deserialization not yet implemented\n        // TODO: Receive data from network transport\n        // TODO: Deserialize from wire format to NetworkEnvelope\n        // TODO: Handle network errors and timeouts\n        // Reference: specs/integrations.zpp ReceiveNetworkMessage operation\n",
      "description": "Receive data from network transport"
    },
    {
      "file": "hypermind/hypermind.hpp",
      "line": 186,
      "type": "TODO",
      "content": "// TODO: Deserialize from wire format to NetworkEnvelope",
      "context": "    bool receive_network_message(NetworkEnvelope& envelope) {\n        // STUB: Network deserialization not yet implemented\n        // TODO: Receive data from network transport\n        // TODO: Deserialize from wire format to NetworkEnvelope\n        // TODO: Handle network errors and timeouts\n        // Reference: specs/integrations.zpp ReceiveNetworkMessage operation\n        \n",
      "description": "Deserialize from wire format to NetworkEnvelope"
    },
    {
      "file": "hypermind/hypermind.hpp",
      "line": 187,
      "type": "TODO",
      "content": "// TODO: Handle network errors and timeouts",
      "context": "        // STUB: Network deserialization not yet implemented\n        // TODO: Receive data from network transport\n        // TODO: Deserialize from wire format to NetworkEnvelope\n        // TODO: Handle network errors and timeouts\n        // Reference: specs/integrations.zpp ReceiveNetworkMessage operation\n        \n        // Validate checksum\n",
      "description": "Handle network errors and timeouts"
    },
    {
      "file": "hypermind/hypermind.hpp",
      "line": 358,
      "type": "TODO",
      "content": "// TODO: Manage GPU memory allocation and transfers",
      "context": "    void submit_gpu_operation(GPUOperation* op) {\n        // STUB: GPU integration not yet implemented\n        // TODO: Implement CUDA/OpenCL submission\n        // TODO: Manage GPU memory allocation and transfers\n        // TODO: Submit computation kernel to GPU stream\n        // Reference: specs/integrations.zpp SubmitGPUComputation operation\n        if (!op) {\n",
      "description": "Manage GPU memory allocation and transfers"
    },
    {
      "file": "hypermind/hypermind.hpp",
      "line": 359,
      "type": "TODO",
      "content": "// TODO: Submit computation kernel to GPU stream",
      "context": "        // STUB: GPU integration not yet implemented\n        // TODO: Implement CUDA/OpenCL submission\n        // TODO: Manage GPU memory allocation and transfers\n        // TODO: Submit computation kernel to GPU stream\n        // Reference: specs/integrations.zpp SubmitGPUComputation operation\n        if (!op) {\n            handle_error(IntegrationError{ERROR, \"Null GPU operation pointer\", \"GPU\", -1});\n",
      "description": "Submit computation kernel to GPU stream"
    },
    {
      "file": "hypermind/hypermind.hpp",
      "line": 371,
      "type": "TODO",
      "content": "// TODO: Use connection pooling for efficiency",
      "context": "    void submit_database_query(DatabaseQuery* query) {\n        // STUB: Database integration not yet implemented\n        // TODO: Implement PostgreSQL async query submission\n        // TODO: Use connection pooling for efficiency\n        // TODO: Submit to PostgreSQL pipe for async processing\n        // Reference: specs/integrations.zpp SubmitDatabaseOperation operation\n        if (!query) {\n",
      "description": "Use connection pooling for efficiency"
    },
    {
      "file": "hypermind/hypermind.hpp",
      "line": 372,
      "type": "TODO",
      "content": "// TODO: Submit to PostgreSQL pipe for async processing",
      "context": "        // STUB: Database integration not yet implemented\n        // TODO: Implement PostgreSQL async query submission\n        // TODO: Use connection pooling for efficiency\n        // TODO: Submit to PostgreSQL pipe for async processing\n        // Reference: specs/integrations.zpp SubmitDatabaseOperation operation\n        if (!query) {\n            handle_error(IntegrationError{ERROR, \"Null database query pointer\", \"Database\", -1});\n",
      "description": "Submit to PostgreSQL pipe for async processing"
    },
    {
      "file": "ure/opencog/ure/Rule.h",
      "line": 373,
      "type": "TODO",
      "content": "// TODO: subdivide in smaller and shared mutexes",
      "context": "\t// True if the rule has already been applied.\n\tbool _exhausted;\n\n\t// TODO: subdivide in smaller and shared mutexes\n\tmutable std::mutex _mutex;\n\n\t// Return a copy of the rule with the variables alpha-converted\n",
      "description": "subdivide in smaller and shared mutexes"
    },
    {
      "file": "ure/opencog/ure/Rule.cc",
      "line": 58,
      "type": "TODO",
      "content": "// TODO: could certainly be optimized by not systematically",
      "context": "\nvoid RuleSet::expand_meta_rules(AtomSpace& as)\n{\n\t// TODO: could certainly be optimized by not systematically\n\t// recollecting and re-instantiating meta-rules.\n\tRuleSet meta_rules;\n\tfor (RulePtr rule : *this) {\n",
      "description": "could certainly be optimized by not systematically"
    },
    {
      "file": "ure/opencog/ure/forwardchainer/FCStat.h",
      "line": 76,
      "type": "TODO",
      "content": "// TODO: subdivide in smaller and shared mutexes",
      "context": "\tstd::vector<InferenceRecord> _inf_rec;\n\tAtomSpace* _trace_as;\n\n\t// TODO: subdivide in smaller and shared mutexes\n\tmutable std::mutex _whole_mutex;\n};\n\n",
      "description": "subdivide in smaller and shared mutexes"
    },
    {
      "file": "ure/opencog/ure/forwardchainer/ForwardChainer.h",
      "line": 237,
      "type": "TODO",
      "content": "// TODO: subdivide in smaller and shared mutexes",
      "context": "\n\tbool _search_focus_set;\n\n\t// TODO: subdivide in smaller and shared mutexes\n\tmutable std::mutex _whole_mutex;\n\tmutable std::mutex _part_mutex;\n\n",
      "description": "subdivide in smaller and shared mutexes"
    },
    {
      "file": "ure/opencog/ure/forwardchainer/ForwardChainer.h",
      "line": 241,
      "type": "TODO",
      "content": "// TODO: use shared mutexes",
      "context": "\tmutable std::mutex _whole_mutex;\n\tmutable std::mutex _part_mutex;\n\n\t// TODO: use shared mutexes\n\tmutable std::mutex _rules_mutex;\n\n\t// Keep track of the number of threads to make sure\n",
      "description": "use shared mutexes"
    },
    {
      "file": "ure/opencog/ure/forwardchainer/ForwardChainer.cc",
      "line": 101,
      "type": "TODO",
      "content": "// TODO: For now the FC follows the old standard. We may move to",
      "context": "\n\t// Set rules.\n\t_rules = _config.get_rules();\n\t// TODO: For now the FC follows the old standard. We may move to\n\t// the new standard when all rules have been ported to the new one.\n\tfor (RulePtr rule : _rules)\n\t\trule->premises_as_clauses = true;\n",
      "description": "For now the FC follows the old standard. We may move to"
    },
    {
      "file": "ure/opencog/ure/forwardchainer/ForwardChainer.cc",
      "line": 165,
      "type": "TODO",
      "content": "// TODO: if creating/destroying threads is too expensive, use a thread",
      "context": "\twhile (not termination()) do_step(_iteration++);\n}\n\n// TODO: if creating/destroying threads is too expensive, use a thread\n// pool (see boost::asio::thread_pool).\nvoid ForwardChainer::do_steps_multithread()\n{\n",
      "description": "if creating/destroying threads is too expensive, use a thread"
    },
    {
      "file": "ure/opencog/ure/forwardchainer/ForwardChainer.cc",
      "line": 289,
      "type": "TODO",
      "content": "// TODO: This can be simplified but is let here until do_step is",
      "context": "\t\t// before being passed to the new source constructor, as this\n\t\t// one will take it into account.\n\t\t//\n\t\t// TODO: This can be simplified but is let here until do_step is\n\t\t// replaced by do_step_srpi.\n\t\tdouble weight = std::min(1.0, slc_sr.source->weight);\n\t\tdouble prob = success_plty / weight;\n",
      "description": "This can be simplified but is let here until do_step is"
    },
    {
      "file": "ure/opencog/ure/forwardchainer/ForwardChainer.cc",
      "line": 372,
      "type": "TODO",
      "content": "// TODO: refine mutex",
      "context": "\nSourcePtr ForwardChainer::select_source(const std::string& msgprfx)\n{\n\t// TODO: refine mutex\n\tstd::unique_lock<std::mutex> lock(_part_mutex);\n\n\tstd::vector<double> weights = _sources.get_weights();\n",
      "description": "refine mutex"
    },
    {
      "file": "ure/opencog/ure/forwardchainer/ForwardChainer.cc",
      "line": 409,
      "type": "TODO",
      "content": "// TODO: This has the effect of deallocating the rules, which",
      "context": "\t\tif (_config.get_retry_exhausted_sources()) {\n\t\t\ture_logger().debug() << msgprfx\n\t\t\t                     << \"Reset all exhausted flags to retry them\";\n\t\t\t// TODO: This has the effect of deallocating the rules, which\n\t\t\t// might cause a memory corruption if another thread is\n\t\t\t// attempting to apply that rule at the same time.\n\t\t\t_sources.reset_exhausted();\n",
      "description": "This has the effect of deallocating the rules, which"
    },
    {
      "file": "ure/opencog/ure/forwardchainer/ForwardChainer.cc",
      "line": 523,
      "type": "TODO",
      "content": "std::lock_guard<std::mutex> lock(_rules_mutex); // TODO: refine",
      "context": "\nRuleSet ForwardChainer::get_valid_rules(const Source& source)\n{\n\tstd::lock_guard<std::mutex> lock(_rules_mutex); // TODO: refine\n\n\t// Generate all valid rules\n\tRuleSet valid_rules;\n",
      "description": "refine"
    },
    {
      "file": "ure/opencog/ure/forwardchainer/SourceSet.h",
      "line": 54,
      "type": "TODO",
      "content": "// TODO: this class has thing in common with AndBIT, maybe their",
      "context": " *\n * 4. a flag call indicating if the source expansions have been exhausted.\n */\n// TODO: this class has thing in common with AndBIT, maybe their\n// common things could be placed in a parent class.\nclass Source : public boost::totally_ordered<Source>\n{\n",
      "description": "this class has thing in common with AndBIT, maybe their"
    },
    {
      "file": "ure/opencog/ure/forwardchainer/SourceSet.h",
      "line": 151,
      "type": "TODO",
      "content": "// TODO: subdivide in smaller and shared mutexes",
      "context": "\tRuleSet rules;\n\nprivate:\n\t// TODO: subdivide in smaller and shared mutexes\n\tmutable std::mutex _mutex;\n};\n\n",
      "description": "subdivide in smaller and shared mutexes"
    },
    {
      "file": "ure/opencog/ure/forwardchainer/SourceSet.h",
      "line": 165,
      "type": "TODO",
      "content": "// TODO: this class has things in common with BIT, maybe their common",
      "context": "/**\n * Population of sources to forwardly expand. Primary owner.\n */\n// TODO: this class has things in common with BIT, maybe their common\n// things could be placed in a parent class.\nclass SourceSet\n{\n",
      "description": "this class has things in common with BIT, maybe their common"
    },
    {
      "file": "ure/opencog/ure/forwardchainer/SourceSet.h",
      "line": 223,
      "type": "TODO",
      "content": "// TODO: subdivide in smaller and shared mutexes",
      "context": "private:\n\tconst UREConfig& _config;\n\n\t// TODO: subdivide in smaller and shared mutexes\n\tmutable std::mutex _mutex;\n};\n\n",
      "description": "subdivide in smaller and shared mutexes"
    },
    {
      "file": "ure/opencog/ure/backwardchainer/TraceRecorder.h",
      "line": 94,
      "type": "TODO",
      "content": "// TODO: the TV on the evaluation link should be more carefully",
      "context": "\t// is reported to the EvaluationLink, otherwise it is not\n\t// recorded.\n\t//\n\t// TODO: the TV on the evaluation link should be more carefully\n\t// thought. For instance maybe it was already proved to begin\n\t// with.\n\tvoid proof(const Handle& andbit_fcs, const Handle& target_result);\n",
      "description": "the TV on the evaluation link should be more carefully"
    },
    {
      "file": "ure/opencog/ure/backwardchainer/BIT.cc",
      "line": 121,
      "type": "TODO",
      "content": "set_leaf2bitnode();         // TODO: might differ till needed to optimize",
      "context": "AndBIT::AndBIT(const Handle& f, double cpx, const AtomSpace* qas)\n\t: fcs(f), complexity(cpx), exhausted(false), queried_as(qas)\n{\n\tset_leaf2bitnode();         // TODO: might differ till needed to optimize\n}\n\nAndBIT::~AndBIT() {}\n",
      "description": "might differ till needed to optimize"
    },
    {
      "file": "ure/opencog/ure/backwardchainer/BIT.cc",
      "line": 374,
      "type": "TODO",
      "content": "// TODO: is this merging necessary?",
      "context": "\tHandle nrewrite = expand_fcs_rewrite(nfcs_rewrite, rule.first);\n\n\t// Generate new vardecl\n\t// TODO: is this merging necessary?\n\tHandle merged_vardecl = merge_vardecl(nfcs_vardecl, rule_vardecl);\n\tHandle nvardecl = filter_vardecl(merged_vardecl, {npattern, nrewrite});\n\n",
      "description": "is this merging necessary?"
    },
    {
      "file": "ure/opencog/ure/backwardchainer/BIT.h",
      "line": 72,
      "type": "TODO",
      "content": "// TODO: Maybe this should be moved to BackwardChainer",
      "context": "\n\t// Estimate the probability of usefulness of expanding this\n\t// BIT-Node.\n\t// TODO: Maybe this should be moved to BackwardChainer\n\tdouble operator()() const;\n\n\tstd::string to_string(const std::string& indent=\"\") const;\n",
      "description": "Maybe this should be moved to BackwardChainer"
    },
    {
      "file": "ure/opencog/ure/backwardchainer/BackwardChainer.cc",
      "line": 288,
      "type": "TODO",
      "content": "// TODO: Maybe we could take advantage of the new read-only",
      "context": "\t// of concerns instead of the atoms themselves, and only modify\n\t// the atoms if there are existing results to copy back to _as.\n\t//\n\t// TODO: Maybe we could take advantage of the new read-only\n\t// capabilities of the AtomSpace.\n\tHandle hresult = HandleCast(fcs->execute(tmp_as.get()));\n\tHandleSeq results;\n",
      "description": "Maybe we could take advantage of the new read-only"
    },
    {
      "file": "ure/opencog/ure/backwardchainer/BackwardChainer.h",
      "line": 232,
      "type": "TODO",
      "content": "// TODO: perhaps move that under BIT",
      "context": "\t// Structure holding the Back Inference Tree\n\tBIT _bit;\n\n\t// TODO: perhaps move that under BIT\n\tAndBITFitness _andbit_fitness;\n\n\t// In charge of recording the inference traces\n",
      "description": "perhaps move that under BIT"
    },
    {
      "file": "ure/opencog/ure/backwardchainer/ControlPolicy.h",
      "line": 45,
      "type": "TODO",
      "content": "// TODO: maybe wrap that in a class, and use it in foward chainer",
      "context": "// selected rule fulfills the objective, which must be passed\n// to the BIT to calculate the and-BIT complexity.\n//\n// TODO: maybe wrap that in a class, and use it in foward chainer\ntypedef std::pair<RuleTypedSubstitutionPair, double> RuleSelection;\n\nclass ControlPolicy\n",
      "description": "maybe wrap that in a class, and use it in foward chainer"
    },
    {
      "file": "ure/opencog/ure/backwardchainer/Fitness.h",
      "line": 75,
      "type": "TODO",
      "content": "// TODO: we may want to move the arguments in its own class if it",
      "context": "\t\tTrace\n\t};\n\n\t// TODO: we may want to move the arguments in its own class if it\n\t// grows bigger.\n\tAndBITFitness(FitnessType ft=Uniform,\n\t              const std::set<ContentHash>& tr=std::set<ContentHash>());\n",
      "description": "we may want to move the arguments in its own class if it"
    },
    {
      "file": "ure/opencog/ure/backwardchainer/Fitness.h",
      "line": 92,
      "type": "TODO",
      "content": "// TODO: replace by class dedicated to hold the parameters",
      "context": "\tdouble operator()(const AndBIT& andbit) const;\n\nprivate:\n\t// TODO: replace by class dedicated to hold the parameters\n\tstd::set<ContentHash> _trace;\n};\n\n",
      "description": "replace by class dedicated to hold the parameters"
    },
    {
      "file": "cogzero/agentzero-core/src/ActionScheduler.cpp",
      "line": 861,
      "type": "TODO",
      "content": "// TODO: Replace with actual SpaceTimeIntegrator::Trajectory when linking is available",
      "context": "    }\n    \n    // Plan and validate trajectory using SpaceTimeIntegrator\n    // TODO: Replace with actual SpaceTimeIntegrator::Trajectory when linking is available\n    TrajectoryPlaceholder trajectory;\n    auto trajectory_end_time = start_time + std::chrono::minutes(5); // Assume 5-minute trajectory duration\n    \n",
      "description": "Replace with actual SpaceTimeIntegrator::Trajectory when linking is available"
    },
    {
      "file": "cogzero/agentzero-core/src/ActionScheduler.cpp",
      "line": 936,
      "type": "TODO",
      "content": "// TODO: Replace with actual SpaceTimeIntegrator call when linking is available",
      "context": "    }\n    \n    // Find optimal time window using SpaceTimeIntegrator\n    // TODO: Replace with actual SpaceTimeIntegrator call when linking is available\n    struct {\n        bool feasible = true;\n        std::chrono::steady_clock::time_point optimal_start_time;\n",
      "description": "Replace with actual SpaceTimeIntegrator call when linking is available"
    },
    {
      "file": "cogzero/agentzero-core/src/ActionScheduler.cpp",
      "line": 1005,
      "type": "TODO",
      "content": "// TODO: Replace with actual configuration when linking is available",
      "context": "        \n        if (_spacetime_integrator) {\n            // Configure the integrator\n            // TODO: Replace with actual configuration when linking is available\n            /*\n            SpaceTimeIntegrator::Configuration config;\n            config.spatial_resolution = spatial_resolution;\n",
      "description": "Replace with actual configuration when linking is available"
    },
    {
      "file": "atenspace/aten/src/THNN/generic/MultiMarginCriterion.c",
      "line": 5,
      "type": "TODO",
      "content": "// TODO: improve error messages",
      "context": "#define TH_GENERIC_FILE \"THNN/generic/MultiMarginCriterion.c\"\n#else\n\n// TODO: improve error messages\nvoid THNN_(MultiMarginCriterion_updateOutput)(\n          THNNState *state,\n          THTensor *input,\n",
      "description": "improve error messages"
    },
    {
      "file": "atenspace/aten/src/ATen/ScalarOps.h",
      "line": 10,
      "type": "FIXME",
      "content": "// FIXME: this should be (and was) Scalar::toTensor, but there is currently no way",
      "context": "// This is in the c10 namespace because we use ADL to find the functions in it.\nnamespace c10 {\n\n// FIXME: this should be (and was) Scalar::toTensor, but there is currently no way\n// to implement this without going through Derived Types (which are not part of core).\ninline at::Tensor scalar_to_tensor(Scalar s, const Device device = at::kCPU) {\n  // This is the fast track we have for CPU scalar tensors.\n",
      "description": "this should be (and was) Scalar::toTensor, but there is currently no way"
    },
    {
      "file": "atenspace/aten/src/ATen/Context.cpp",
      "line": 28,
      "type": "TODO",
      "content": "// TODO: This could be bad juju if someone calls globalContext() in the",
      "context": "    : thc_state(nullptr, [](THCState* p) { /* no-op */ }),\n      thh_state(nullptr, [](THHState* p) { /* no-op */ }) {}\n\n// TODO: This could be bad juju if someone calls globalContext() in the\n// destructor of an object with static lifetime.\nContext& globalContext() {\n  static Context globalContext_;\n",
      "description": "This could be bad juju if someone calls globalContext() in the"
    },
    {
      "file": "atenspace/aten/src/ATen/Utils.h",
      "line": 68,
      "type": "TODO",
      "content": "// TODO: This unwrapping code is ONLY used for TH bindings; once TH goes",
      "context": "  return expr;\n}\n\n// TODO: This unwrapping code is ONLY used for TH bindings; once TH goes\n// away, we can delete this function\nstatic inline TensorImpl* checked_dense_tensor_unwrap(const Tensor& expr, const char * name, int pos, const char * api, bool allowNull, DeviceType device_type, ScalarType scalar_type) {\n  if(allowNull && !expr.defined()) {\n",
      "description": "This unwrapping code is ONLY used for TH bindings; once TH goes"
    },
    {
      "file": "atenspace/aten/src/ATen/Utils.h",
      "line": 111,
      "type": "TODO",
      "content": "// TODO: is this necessary?  We used to treat nullptr-vs-not in IntList differently",
      "context": "template <size_t N>\nstd::array<int64_t, N> check_intlist(ArrayRef<int64_t> list, const char * name, int pos) {\n  if (list.empty()) {\n    // TODO: is this necessary?  We used to treat nullptr-vs-not in IntList differently\n    // with strides as a way of faking optional.\n    list = {};\n  }\n",
      "description": "is this necessary?  We used to treat nullptr-vs-not in IntList differently"
    },
    {
      "file": "atenspace/aten/src/ATen/SparseTensorUtils.h",
      "line": 42,
      "type": "TODO",
      "content": "// TODO: put this into the public API",
      "context": "      values.to(self._values().options(), non_blocking, /*copy=*/true));\n}\n\n// TODO: put this into the public API\ninline bool is_same_tensor(const Tensor& lhs, const Tensor& rhs) {\n  return lhs.unsafeGetTensorImpl() == rhs.unsafeGetTensorImpl();\n}\n",
      "description": "put this into the public API"
    },
    {
      "file": "atenspace/aten/src/ATen/SparseTensorUtils.h",
      "line": 53,
      "type": "TODO",
      "content": "// TODO: Expose this for real in ATen, some day?",
      "context": "\n// Give us a new values tensor, with the same dimensionality\n// as 'values' but with a new number of non-zero elements.\n// TODO: Expose this for real in ATen, some day?\n// NB: Doesn't preserve data.\ninline Tensor new_values_with_size_of(const Tensor& values, int64_t nnz) {\n  std::vector<int64_t> size = values.sizes().vec();\n",
      "description": "Expose this for real in ATen, some day?"
    },
    {
      "file": "atenspace/aten/src/ATen/Version.cpp",
      "line": 93,
      "type": "TODO",
      "content": "ss << \"PyTorch built with:\\n\"; // TODO add the version of PyTorch",
      "context": "\nstd::string show_config() {\n  std::ostringstream ss;\n  ss << \"PyTorch built with:\\n\"; // TODO add the version of PyTorch\n\n  // Reference:\n  // https://blog.kowalczyk.info/article/j/guide-to-predefined-macros-in-c-compilers-gcc-clang-msvc-etc..html\n",
      "description": "add the version of PyTorch"
    },
    {
      "file": "atenspace/aten/src/ATen/Version.cpp",
      "line": 129,
      "type": "TODO",
      "content": "// TODO: Actually record which one we actually picked",
      "context": "#endif\n\n#ifdef USE_LAPACK\n  // TODO: Actually record which one we actually picked\n  ss << \"  - LAPACK is enabled (usually provided by MKL)\\n\";\n#endif\n\n",
      "description": "Actually record which one we actually picked"
    },
    {
      "file": "atenspace/aten/src/ATen/Version.cpp",
      "line": 134,
      "type": "TODO",
      "content": "// TODO: No version; c.f. https://github.com/Maratyszcza/NNPACK/issues/165",
      "context": "#endif\n\n#if AT_NNPACK_ENABLED()\n  // TODO: No version; c.f. https://github.com/Maratyszcza/NNPACK/issues/165\n  ss << \"  - NNPACK is enabled\\n\";\n#endif\n\n",
      "description": "No version; c.f. https://github.com/Maratyszcza/NNPACK/issues/165"
    },
    {
      "file": "atenspace/aten/src/ATen/Version.cpp",
      "line": 150,
      "type": "TODO",
      "content": "// TODO: do HIP",
      "context": "  }\n  ss << \"\\n\";\n\n  // TODO: do HIP\n  // TODO: do XLA\n\n  return ss.str();\n",
      "description": "do HIP"
    },
    {
      "file": "atenspace/aten/src/ATen/Version.cpp",
      "line": 151,
      "type": "TODO",
      "content": "// TODO: do XLA",
      "context": "  ss << \"\\n\";\n\n  // TODO: do HIP\n  // TODO: do XLA\n\n  return ss.str();\n}\n",
      "description": "do XLA"
    },
    {
      "file": "atenspace/aten/src/ATen/common_with_cwrap.py",
      "line": 45,
      "type": "TODO",
      "content": "# TODO(zach): why does cwrap not propagate 'name'? I need it",
      "context": "    # Propagate defaults from declaration to options\n    for option in declaration['options']:\n        for k, v in declaration.items():\n            # TODO(zach): why does cwrap not propagate 'name'? I need it\n            # propagaged for ATen\n            if k != 'options':\n                option.setdefault(k, v)\n",
      "description": "(zach): why does cwrap not propagate 'name'? I need it"
    },
    {
      "file": "atenspace/aten/src/ATen/common_with_cwrap.py",
      "line": 50,
      "type": "TODO",
      "content": "# TODO(zach): added option to remove keyword handling for C++ which cannot",
      "context": "            if k != 'options':\n                option.setdefault(k, v)\n\n# TODO(zach): added option to remove keyword handling for C++ which cannot\n# support it.\n\n\n",
      "description": "(zach): added option to remove keyword handling for C++ which cannot"
    },
    {
      "file": "atenspace/aten/src/ATen/NamedTensorUtils.cpp",
      "line": 47,
      "type": "TODO",
      "content": "// TODO(zou3519): Can improve message by checking if names are alignable and suggesting workarounds",
      "context": "    DimnameList names,\n    DimnameList other_names,\n    const char* action) {\n  // TODO(zou3519): Can improve message by checking if names are alignable and suggesting workarounds\n  TORCH_CHECK(false,\n      \"Error when attempting to \", action, \" dims \", names, \" and dims \",\n      other_names, \": dim \", name, \" and dim \", other_name, \" are at the same position \"\n",
      "description": "(zou3519): Can improve message by checking if names are alignable and suggesting workarounds"
    },
    {
      "file": "atenspace/aten/src/ATen/NamedTensorUtils.cpp",
      "line": 63,
      "type": "TODO",
      "content": "// TODO(zou3519): Can improve message by checking if names are alignable and suggesting workarounds",
      "context": "    return;\n  }\n  auto it = std::find(other_names.begin(), other_names.end(), name);\n  // TODO(zou3519): Can improve message by checking if names are alignable and suggesting workarounds\n  TORCH_CHECK(it == other_names.end(),\n      \"Misaligned dims when attempting to \", action, \" dims \", names, \" and dims \",\n      other_names, \": dim \", name, \" appears in a different position from the right \"\n",
      "description": "(zou3519): Can improve message by checking if names are alignable and suggesting workarounds"
    },
    {
      "file": "atenspace/aten/src/ATen/TensorUtils.h",
      "line": 39,
      "type": "TODO",
      "content": "// TODO: Consider generalizing this into a call stack.",
      "context": "\n// A string describing which function did checks on its input\n// arguments.\n// TODO: Consider generalizing this into a call stack.\nusing CheckedFrom = const char*;\n\n// The undefined convention: singular operators assume their arguments\n",
      "description": "Consider generalizing this into a call stack."
    },
    {
      "file": "atenspace/aten/src/ATen/TensorUtils.h",
      "line": 112,
      "type": "FIXME",
      "content": "// FixMe: does TensorArg slow things down?",
      "context": "CAFFE2_API void checkDefined(CheckedFrom c, const TensorArg& t);\nCAFFE2_API void checkAllDefined(CheckedFrom c, at::ArrayRef<TensorArg> t);\n\n// FixMe: does TensorArg slow things down?\nCAFFE2_API void checkBackend(\n    CheckedFrom c,\n    at::ArrayRef<Tensor> t,\n",
      "description": "does TensorArg slow things down?"
    },
    {
      "file": "atenspace/aten/src/ATen/ParallelNative.cpp",
      "line": 246,
      "type": "TODO",
      "content": "// TODO: caffe2::ThreadPool doesn't support submitting tasks separately and",
      "context": "    func();\n  }\n#else\n  // TODO: caffe2::ThreadPool doesn't support submitting tasks separately and\n  // running in parallel. Should fix it when this API becomes popular.\n  func();\n#endif // C10_MOBILE\n",
      "description": "caffe2::ThreadPool doesn't support submitting tasks separately and"
    },
    {
      "file": "atenspace/aten/src/ATen/ParallelNative.cpp",
      "line": 269,
      "type": "TODO",
      "content": "// TODO: caffe2::ThreadPool doesn't support submitting tasks separately and",
      "context": "  }\n  return future;\n#else\n  // TODO: caffe2::ThreadPool doesn't support submitting tasks separately and\n  // running in parallel. Should fix it when this API becomes popular.\n  auto future = std::make_shared<c10::ivalue::Future>(NoneType::get());\n  func();\n",
      "description": "caffe2::ThreadPool doesn't support submitting tasks separately and"
    },
    {
      "file": "atenspace/aten/src/ATen/function_wrapper.py",
      "line": 1714,
      "type": "TODO",
      "content": "# TODO: check for move semantics...",
      "context": "                    else:\n                        types = [to_return_type(arg, option)['type']\n                                 for arg in arguments]\n                        # TODO: check for move semantics...\n                        names = [arg['name'] for arg in arguments]\n                        case_body.append(CodeTemplate(\"return std::tuple<${types}>(${names});\").substitute(\n                            types=types, names=names))\n",
      "description": "check for move semantics..."
    },
    {
      "file": "atenspace/aten/src/ATen/gen.py",
      "line": 149,
      "type": "TODO",
      "content": "densities = ['Dense', 'Sparse', 'Mkldnn']  # TODO: layout instead of densities?",
      "context": "    return backend\n\nbackends = ['CPU', 'CUDA']\ndensities = ['Dense', 'Sparse', 'Mkldnn']  # TODO: layout instead of densities?\n\nquantized_backends = ['QuantizedCPU']\n\n",
      "description": "layout instead of densities?"
    },
    {
      "file": "atenspace/aten/src/ATen/cudnn/Descriptors.h",
      "line": 15,
      "type": "TODO",
      "content": "// TODO: Add constructors for all of the descriptors",
      "context": "\nnamespace at { namespace native {\n\n// TODO: Add constructors for all of the descriptors\n\ninline int dataSize(cudnnDataType_t dataType)\n{\n",
      "description": "Add constructors for all of the descriptors"
    },
    {
      "file": "atenspace/aten/src/ATen/cudnn/Descriptors.h",
      "line": 70,
      "type": "TODO",
      "content": "// TODO: Figure out why const-correctness doesn't work here",
      "context": "class TORCH_CUDA_API Descriptor\n{\npublic:\n  // TODO: Figure out why const-correctness doesn't work here\n\n  // Use desc() to access the underlying descriptor pointer in\n  // a read-only fashion.  Most client code should use this.\n",
      "description": "Figure out why const-correctness doesn't work here"
    },
    {
      "file": "atenspace/aten/src/ATen/cudnn/Utils.h",
      "line": 12,
      "type": "TODO",
      "content": "// TODO: Should getCurrentStream be a method on Context?",
      "context": "namespace at { namespace native {\n\ninline void setCuDNNStreamToCurrent() {\n  // TODO: Should getCurrentStream be a method on Context?\n  AT_CUDNN_CHECK(cudnnSetStream(getCudnnHandle(), at::cuda::getCurrentCUDAStream()));\n}\n\n",
      "description": "Should getCurrentStream be a method on Context?"
    },
    {
      "file": "atenspace/aten/src/ATen/cuda/detail/CUDAHooks.h",
      "line": 6,
      "type": "TODO",
      "content": "// TODO: No need to have this whole header, we can just put it all in",
      "context": "#include <ATen/Generator.h>\n#include <c10/util/Optional.h>\n\n// TODO: No need to have this whole header, we can just put it all in\n// the cpp file\n\nnamespace at { namespace cuda { namespace detail {\n",
      "description": "No need to have this whole header, we can just put it all in"
    },
    {
      "file": "atenspace/aten/src/ATen/cuda/detail/CUDAHooks.cpp",
      "line": 246,
      "type": "TODO",
      "content": "// TODO: Make HIPIFY understand CUDART_VERSION macro",
      "context": "  printCudaStyleVersion(runtimeVersion);\n  oss << \"\\n\";\n\n  // TODO: Make HIPIFY understand CUDART_VERSION macro\n#ifndef __HIP_PLATFORM_HCC__\n  if (runtimeVersion != CUDART_VERSION) {\n    oss << \"  - Built with CUDA Runtime \";\n",
      "description": "Make HIPIFY understand CUDART_VERSION macro"
    },
    {
      "file": "atenspace/aten/src/ATen/cuda/detail/CUDAHooks.cpp",
      "line": 284,
      "type": "TODO",
      "content": "// TODO: Check if miopen has the functions above and unify",
      "context": "  }\n#endif\n#else\n  // TODO: Check if miopen has the functions above and unify\n  oss << \"  - MIOpen \" << MIOPEN_VERSION_MAJOR << \".\" << MIOPEN_VERSION_MINOR << \".\" << MIOPEN_VERSION_PATCH << \"\\n\";\n#endif\n\n",
      "description": "Check if miopen has the functions above and unify"
    },
    {
      "file": "atenspace/aten/src/ATen/core/jit_type.h",
      "line": 1329,
      "type": "TODO",
      "content": "// TODO: static_assert that a templated function exists, and throw a friendy",
      "context": "} // namespace detail\ntemplate <class T>\ninline TypePtr getTypePtr() {\n  // TODO: static_assert that a templated function exists, and throw a friendy\n  // error message if not\n  return detail::getTypePtr_<T>::call();\n}\n",
      "description": "static_assert that a templated function exists, and throw a friendy"
    },
    {
      "file": "atenspace/aten/src/ATen/core/jit_type.h",
      "line": 1571,
      "type": "TODO",
      "content": "// TODO: once modules support arbitrary ivalue attributes, we don't need this",
      "context": "\n  // Mapping of attribute names -> their type.\n  // NOTE: this does not contain methods, which are stored in the module\n  // TODO: once modules support arbitrary ivalue attributes, we don't need this\n  // anymore.\n  // TODO: This is better represented as an OrderedDict, but alas it is not yet\n  // available from c10\n",
      "description": "once modules support arbitrary ivalue attributes, we don't need this"
    },
    {
      "file": "atenspace/aten/src/ATen/core/jit_type.h",
      "line": 1573,
      "type": "TODO",
      "content": "// TODO: This is better represented as an OrderedDict, but alas it is not yet",
      "context": "  // NOTE: this does not contain methods, which are stored in the module\n  // TODO: once modules support arbitrary ivalue attributes, we don't need this\n  // anymore.\n  // TODO: This is better represented as an OrderedDict, but alas it is not yet\n  // available from c10\n  std::vector<std::string> attributeNames_;\n  std::vector<TypePtr> attributeTypes_;\n",
      "description": "This is better represented as an OrderedDict, but alas it is not yet"
    },
    {
      "file": "atenspace/aten/src/ATen/core/blob.h",
      "line": 69,
      "type": "TODO",
      "content": "// TODO(jerryzh): add a Get(DeviceType) function?",
      "context": "   * @brief Gets the const reference of the stored object. The code checks if\n   * the stored object is of the desired type.\n   */\n  // TODO(jerryzh): add a Get(DeviceType) function?\n  template <class T>\n  const T& Get() const {\n    AT_ASSERTM(\n",
      "description": "(jerryzh): add a Get(DeviceType) function?"
    },
    {
      "file": "atenspace/aten/src/ATen/core/blob.h",
      "line": 78,
      "type": "TODO",
      "content": "// TODO: after we add Get<Tensor>(DeviceType)",
      "context": "        meta_.name(),\n        \" while caller expects \",\n        TypeMeta::TypeName<T>());\n    // TODO: after we add Get<Tensor>(DeviceType)\n    // and changed all the callsites, we can add\n    // a static assert here to enforce T != Tensor\n    return *static_cast<const T*>(pointer_);\n",
      "description": "after we add Get<Tensor>(DeviceType)"
    },
    {
      "file": "atenspace/aten/src/ATen/core/blob.h",
      "line": 108,
      "type": "TODO",
      "content": "// TODO Re-enable logging",
      "context": "    if (IsType<T>()) {\n      return static_cast<T*>(pointer_);\n    } else {\n      // TODO Re-enable logging\n      // VLOG(1) << \"Create new mutable object \" << TypeMeta::TypeName<T>();\n      return Reset<T>(new T());\n    }\n",
      "description": "Re-enable logging"
    },
    {
      "file": "atenspace/aten/src/ATen/core/blob.h",
      "line": 158,
      "type": "TODO",
      "content": "// TODO Remove ShareExternal() and have Blob always own its content",
      "context": "        TypeMeta::Make<typename std::remove_const<T>::type>()));\n  }\n\n  // TODO Remove ShareExternal() and have Blob always own its content\n  void* ShareExternal(void* allocated, const TypeMeta& meta) {\n    free_();\n    meta_ = meta;\n",
      "description": "Remove ShareExternal() and have Blob always own its content"
    },
    {
      "file": "atenspace/aten/src/ATen/core/LegacyTypeDispatch.cpp",
      "line": 5,
      "type": "TODO",
      "content": "// TODO: This could be bad juju if someone calls globalContext() in the",
      "context": "\nnamespace at {\n\n// TODO: This could be bad juju if someone calls globalContext() in the\n// destructor of an object with static lifetime.\nLegacyTypeDispatch & globalLegacyTypeDispatch() {\n  static LegacyTypeDispatch singleton;\n",
      "description": "This could be bad juju if someone calls globalContext() in the"
    },
    {
      "file": "atenspace/aten/src/ATen/core/ivalue.cpp",
      "line": 173,
      "type": "TODO",
      "content": "// TODO we should attempt to call __str__ if the object defines it.",
      "context": "    case IValue::Tag::GenericDict:\n      return printDict(out, v.toGenericDict());\n    case IValue::Tag::Object:\n      // TODO we should attempt to call __str__ if the object defines it.\n      auto obj = v.toObject();\n      // print this out the way python would do it\n      return out << \"<\" << obj->name() << \" object at \" << obj.get() << \">\";\n",
      "description": "we should attempt to call __str__ if the object defines it."
    },
    {
      "file": "atenspace/aten/src/ATen/core/List.h",
      "line": 423,
      "type": "TODO",
      "content": "// TODO Test use_count",
      "context": "   * Returns the number of Lists currently pointing to this same list.\n   * If this is the only instance pointing to this list, returns 1.\n   */\n  // TODO Test use_count\n  size_t use_count() const;\n\n  TypePtr elementType() const;\n",
      "description": "Test use_count"
    },
    {
      "file": "atenspace/aten/src/ATen/core/interned_strings.h",
      "line": 314,
      "type": "TODO",
      "content": "// TODO: eliminate me",
      "context": "  static Symbol user(const std::string & s);\n  static Symbol caffe2(const std::string & s);\n  static Symbol dimname(const std::string & s);\n  // TODO: eliminate me\n  static Symbol scope(const std::string & s);\n\n  bool is_attr() const;\n",
      "description": "eliminate me"
    },
    {
      "file": "atenspace/aten/src/ATen/core/LegacyTypeDispatch.h",
      "line": 9,
      "type": "TODO",
      "content": "// TODO: Clean up what remains here",
      "context": "//\n// This has been deprecated in favor of ATenDispatch, and in the future,\n// c10 dispatcher.\n// TODO: Clean up what remains here\n\n#include <c10/core/Backend.h>\n#include <c10/core/ScalarType.h>\n",
      "description": "Clean up what remains here"
    },
    {
      "file": "atenspace/aten/src/ATen/core/LegacyTypeDispatch.h",
      "line": 24,
      "type": "TODO",
      "content": "// TODO: Avoid use of legacyExtractTypeId here.  The key",
      "context": "class CAFFE2_API LegacyTypeDispatch {\n public:\n  void initForTensorTypeSet(TensorTypeSet ts) {\n    // TODO: Avoid use of legacyExtractTypeId here.  The key\n    // problem is that you may get a TensorTypeSet with\n    // VariableTensorId set; should you initialize the \"underlying\"\n    // type in that case?  Hard to say.\n",
      "description": "Avoid use of legacyExtractTypeId here.  The key"
    },
    {
      "file": "atenspace/aten/src/ATen/core/LegacyTypeDispatch.h",
      "line": 74,
      "type": "TODO",
      "content": "// TODO: Since `torch::NoGradGuard` serves almost the same purpose in libtorch,",
      "context": "// when enabled will cause `legacyTensorType()` and `getType()` to always return\n// non-Variable type, even if the tensor being called on is a variable.\n//\n// TODO: Since `torch::NoGradGuard` serves almost the same purpose in libtorch,\n// we should merge these two thread-local guards.  However, NoGradGuard does\n// something subtly different: it turns off gradient recording, but DOES NOT\n// skip VariableType implementation (as we still might need to profile or\n",
      "description": "Since `torch::NoGradGuard` serves almost the same purpose in libtorch,"
    },
    {
      "file": "atenspace/aten/src/ATen/core/LegacyTypeDispatch.h",
      "line": 83,
      "type": "TODO",
      "content": "// TODO: Eliminate this parameter entirely",
      "context": "\nstruct CAFFE2_API AutoNonVariableTypeMode {\n  // NB: The enabled parameter must ALWAYS be black, as Henry Ford used to say.\n  // TODO: Eliminate this parameter entirely\n  AutoNonVariableTypeMode(bool enabled = true) :\n    guard_(TensorTypeId::VariableTensorId) {\n\n",
      "description": "Eliminate this parameter entirely"
    },
    {
      "file": "atenspace/aten/src/ATen/core/DeprecatedTypePropertiesRegistry.cpp",
      "line": 26,
      "type": "TODO",
      "content": "// TODO: This could be bad juju if someone calls globalContext() in the",
      "context": "  return *registry[static_cast<int>(p)][static_cast<int>(s)];\n}\n\n// TODO: This could be bad juju if someone calls globalContext() in the\n// destructor of an object with static lifetime.\nDeprecatedTypePropertiesRegistry & globalDeprecatedTypePropertiesRegistry() {\n  static DeprecatedTypePropertiesRegistry singleton;\n",
      "description": "This could be bad juju if someone calls globalContext() in the"
    },
    {
      "file": "atenspace/aten/src/ATen/core/stack.h",
      "line": 5,
      "type": "TODO",
      "content": "// TODO move this to c10 namespace",
      "context": "\n#include <ATen/core/ivalue.h>\n\n// TODO move this to c10 namespace\n\nnamespace torch {\nnamespace jit {\n",
      "description": "move this to c10 namespace"
    },
    {
      "file": "atenspace/aten/src/ATen/core/interned_strings.cpp",
      "line": 99,
      "type": "TODO",
      "content": "// TODO: Make this actually return something that's \"user friendly\".",
      "context": "}\n\nconst char * Symbol::toDisplayString() const {\n  // TODO: Make this actually return something that's \"user friendly\".\n  // The trouble is that, for this to be usable in printf-style assert\n  // statements, this has to return a const char* (whose lifetime is\n  // global), so we can't actually assemble a string on the fly.\n",
      "description": "Make this actually return something that's \"user friendly\"."
    },
    {
      "file": "atenspace/aten/src/ATen/core/ivalue.h",
      "line": 157,
      "type": "TODO",
      "content": "// TODO (after Tensor merge) If we pass in a Blob holding a Tensor, extract",
      "context": "\n  IValue(intrusive_ptr<caffe2::Blob> blob)\n  : tag(Tag::Blob), is_intrusive_ptr(true) {\n    // TODO (after Tensor merge) If we pass in a Blob holding a Tensor, extract\n    // and store it as a Tensor instead.\n    payload.as_intrusive_ptr = blob.release();\n  }\n",
      "description": "(after Tensor merge) If we pass in a Blob holding a Tensor, extract"
    },
    {
      "file": "atenspace/aten/src/ATen/core/List_inl.h",
      "line": 210,
      "type": "TODO",
      "content": "// TODO Use list_element_from?",
      "context": "template<class T>\ntemplate<class... Args>\ntypename List<T>::iterator List<T>::emplace(iterator pos, Args&&... value) const {\n  // TODO Use list_element_from?\n  return iterator { impl_->list.emplace(pos.iterator_, std::forward<Args>(value)...) };\n}\n\n",
      "description": "Use list_element_from?"
    },
    {
      "file": "atenspace/aten/src/ATen/core/List_inl.h",
      "line": 236,
      "type": "TODO",
      "content": "// TODO Use list_element_from?",
      "context": "template<class T>\ntemplate<class... Args>\nvoid List<T>::emplace_back(Args&&... args) const {\n  // TODO Use list_element_from?\n  impl_->list.emplace_back(std::forward<Args>(args)...);\n}\n\n",
      "description": "Use list_element_from?"
    },
    {
      "file": "atenspace/aten/src/ATen/core/dispatch/Dispatcher.h",
      "line": 177,
      "type": "TODO",
      "content": "// TODO This should be a nested lambda instead of a separate function call, but that triggers an internal",
      "context": "\n  // note: this doesn't need the mutex because write operations on the list keep iterators intact.\n  return op.operatorIterator_->op.readDispatchTable([&] (const DispatchTable& dispatchTable) -> Return {\n    // TODO This should be a nested lambda instead of a separate function call, but that triggers an internal\n    // compiler error on GCC5. Change this once we don't need gcc 5 anymore.\n    return doCallUnboxed<Return, Args...>(dispatchTable, backendFallbackKernels_, std::forward<Args>(args)...);\n  });\n",
      "description": "This should be a nested lambda instead of a separate function call, but that triggers an internal"
    },
    {
      "file": "atenspace/aten/src/ATen/core/dispatch/Dispatcher.h",
      "line": 199,
      "type": "TODO",
      "content": "// TODO This should be a nested lambda instead of a separate function call, but that triggers an internal",
      "context": "\n  // note: this doesn't need the mutex because write operations on the list keep iterators intact.\n  return op.operatorIterator_->op.readDispatchTable([&] (const DispatchTable& dispatchTable) -> Return {\n    // TODO This should be a nested lambda instead of a separate function call, but that triggers an internal\n    // compiler error on GCC5. Change this once we don't need gcc 5 anymore.\n    return doCallUnboxedOnly<Return, Args...>(dispatchTable, backendFallbackKernels_, std::forward<Args>(args)...);\n  });\n",
      "description": "This should be a nested lambda instead of a separate function call, but that triggers an internal"
    },
    {
      "file": "atenspace/aten/src/ATen/core/dispatch/DispatchTable.h",
      "line": 52,
      "type": "TODO",
      "content": "// TODO Stop generating those kernels and re-enable this assertion here.",
      "context": "    // autograd kernels for operators without tensor arguments even though\n    // they are never called. These, however, register kernels for\n    // VariableTensorId.\n    // TODO Stop generating those kernels and re-enable this assertion here.\n    auto emplaced = kernels_.emplace(dispatchKey, kernel);\n    if (!emplaced.second) {\n      // Element already existed. Overwrite it.\n",
      "description": "Stop generating those kernels and re-enable this assertion here."
    },
    {
      "file": "atenspace/aten/src/ATen/core/dispatch/DispatchKeyExtractor.h",
      "line": 21,
      "type": "TODO",
      "content": "// TODO: I'm not sure if this should live in this header or not; the operant",
      "context": "//\n// NB: I didn't make this take a Tensor to avoid header include shenanigans.\n//\n// TODO: I'm not sure if this should live in this header or not; the operant\n// question is whether or not we have access to all the relevant TLS at this\n// point.\nstatic inline TensorTypeId dispatchTypeId(TensorTypeSet ts) {\n",
      "description": "I'm not sure if this should live in this header or not; the operant"
    },
    {
      "file": "atenspace/aten/src/ATen/core/dispatch/DispatchKeyExtractor.h",
      "line": 72,
      "type": "TODO",
      "content": "// TODO Unboxed dispatch supports TensorOptions (i.e. ScalarType/Device/Layout) arguments",
      "context": "  }\n\n  c10::optional<TensorTypeId> getDispatchKeyBoxed(const Stack* stack) const {\n    // TODO Unboxed dispatch supports TensorOptions (i.e. ScalarType/Device/Layout) arguments\n    //      but boxed doesn't yet. These should be aligned and do the same thing.\n\n    TensorTypeSet ts;\n",
      "description": "Unboxed dispatch supports TensorOptions (i.e. ScalarType/Device/Layout) arguments"
    },
    {
      "file": "atenspace/aten/src/ATen/core/dispatch/DispatchKeyExtractor.h",
      "line": 91,
      "type": "TODO",
      "content": "// TODO: Don't use legacy extractor; blocked on c10 understanding variable",
      "context": "      return c10::nullopt;\n    }\n\n    // TODO: Don't use legacy extractor; blocked on c10 understanding variable\n    return c10::legacyExtractTypeId(ts);\n  }\n\n",
      "description": "Don't use legacy extractor; blocked on c10 understanding variable"
    },
    {
      "file": "atenspace/aten/src/ATen/core/dispatch/DispatchKeyExtractor.h",
      "line": 117,
      "type": "TODO",
      "content": "// TODO: a potential optimization is to store a bitfield of arg locations,",
      "context": "  // again and again for each dispatcher lookup.\n  // num_args_ is allowed to be zero; that just means you must do the\n  // fallthrough\n  // TODO: a potential optimization is to store a bitfield of arg locations,\n  size_t num_args_;\n};\n\n",
      "description": "a potential optimization is to store a bitfield of arg locations,"
    },
    {
      "file": "atenspace/aten/src/ATen/core/op_registration/op_registration_test.cpp",
      "line": 114,
      "type": "TODO",
      "content": "// TODO Rewrite (since this is now allowed) and reenable",
      "context": "  EXPECT_TRUE(called);\n}\n\n// TODO Rewrite (since this is now allowed) and reenable\n// TEST(OperatorRegistrationTest, givenOpWithCatchallKernel_whenRegisteringDispatchedKernel_thenFails) {\n//   bool called = false;\n//   auto registrar = c10::RegisterOperators().op(\"_test::dummy(Tensor dummy) -> ()\", c10::RegisterOperators::options().catchAllKernel<MockKernel>(&called));\n",
      "description": "Rewrite (since this is now allowed) and reenable"
    },
    {
      "file": "atenspace/aten/src/ATen/core/op_registration/op_registration_test.cpp",
      "line": 147,
      "type": "TODO",
      "content": "// TODO Rewrite (since this is now allowed) and reenable",
      "context": "  EXPECT_TRUE(called);\n}\n\n// TODO Rewrite (since this is now allowed) and reenable\n// TEST(OperatorRegistrationTest, givenOpWithDispatchedKernel_whenRegisteringCatchallKernel_thenFails) {\n//   bool called = false;\n//   auto registrar = c10::RegisterOperators().op(\"_test::dummy(Tensor dummy) -> ()\", c10::RegisterOperators::options().kernel<MockKernel>(c10::TensorTypeId::CPUTensorId, &called));\n",
      "description": "Rewrite (since this is now allowed) and reenable"
    },
    {
      "file": "atenspace/aten/src/ATen/core/op_registration/op_registration_test.cpp",
      "line": 642,
      "type": "TODO",
      "content": "// TODO Reenable these",
      "context": "  called_autograd = true;\n}\n\n// TODO Reenable these\n// TEST(OperatorRegistrationTest, whenRegisteringAutogradKernel_thenCanCallAutogradKernel) {\n//   auto registrar = c10::RegisterOperators().op(\"_test::dummy(Tensor dummy) -> ()\", c10::RegisterOperators::options()\n//     .impl_unboxedOnlyKernel<decltype(autograd_kernel), &autograd_kernel>(TensorTypeId::VariableTensorId));\n",
      "description": "Reenable these"
    },
    {
      "file": "atenspace/aten/src/ATen/core/op_registration/op_registration_test.cpp",
      "line": 772,
      "type": "TODO",
      "content": "// TODO Test Scalar",
      "context": "};\n\nTEST(OperatorRegistrationTest, testAvailableArgTypes) {\n  // TODO Test Scalar\n\n  // primitive types\n  testArgTypes<double>::test(\n",
      "description": "Test Scalar"
    },
    {
      "file": "atenspace/aten/src/ATen/core/op_registration/op_registration.h",
      "line": 227,
      "type": "TODO",
      "content": "// TODO Do schema inference without relying on WrapKernelFunction",
      "context": "      return std::move(*this).kernel(\n        std::move(dispatch_key),\n        KernelFunction::makeFromUnboxedFunction<FuncType, kernel_func>(),\n        // TODO Do schema inference without relying on WrapKernelFunction\n        detail::FunctionSchemaInferer<typename detail::WrapKernelFunction<FuncType, kernel_func>::type>()()\n      );\n    }\n",
      "description": "Do schema inference without relying on WrapKernelFunction"
    },
    {
      "file": "atenspace/aten/src/ATen/core/op_registration/op_registration.h",
      "line": 255,
      "type": "TODO",
      "content": "// TODO Do schema inference without relying on WrapKernelFunction",
      "context": "      return std::move(*this).kernel(\n        c10::nullopt,\n        KernelFunction::makeFromUnboxedFunction<FuncType, kernel_func>(),\n        // TODO Do schema inference without relying on WrapKernelFunction\n        detail::FunctionSchemaInferer<typename detail::WrapKernelFunction<FuncType, kernel_func>::type>()()\n      );\n    }\n",
      "description": "Do schema inference without relying on WrapKernelFunction"
    },
    {
      "file": "atenspace/aten/src/ATen/core/op_registration/op_registration.h",
      "line": 269,
      "type": "TODO",
      "content": "// TODO Do schema inference without relying on WrapKernelFunction",
      "context": "      return std::move(*this).kernel(\n        std::move(dispatch_key),\n        KernelFunction::makeFromUnboxedRuntimeFunction(kernel_func),\n        // TODO Do schema inference without relying on WrapKernelFunction\n        detail::FunctionSchemaInferer<detail::WrapRuntimeKernelFunctor<guts::decay_t<FuncType>>>()()\n      );\n    }\n",
      "description": "Do schema inference without relying on WrapKernelFunction"
    },
    {
      "file": "atenspace/aten/src/ATen/core/op_registration/op_registration.h",
      "line": 283,
      "type": "TODO",
      "content": "// TODO Do schema inference without relying on WrapKernelFunction",
      "context": "      return std::move(*this).kernel(\n        c10::nullopt,\n        KernelFunction::makeFromUnboxedRuntimeFunction(kernel_func),\n        // TODO Do schema inference without relying on WrapKernelFunction\n        detail::FunctionSchemaInferer<detail::WrapRuntimeKernelFunctor<guts::decay_t<FuncType>>>()()\n      );\n    }\n",
      "description": "Do schema inference without relying on WrapKernelFunction"
    },
    {
      "file": "atenspace/aten/src/ATen/core/op_registration/op_registration.h",
      "line": 288,
      "type": "TODO",
      "content": "// TODO Remove impl_unboxedOnlyKernel once all of aten can generate boxed kernels",
      "context": "      );\n    }\n\n    // TODO Remove impl_unboxedOnlyKernel once all of aten can generate boxed kernels\n    template<class FuncType, FuncType* kernel_func>\n    // enable_if: only enable it if FuncType is actually a function\n    guts::enable_if_t<guts::is_function_type<FuncType>::value, Options&&> impl_unboxedOnlyKernel(TensorTypeId dispatch_key) && {\n",
      "description": "Remove impl_unboxedOnlyKernel once all of aten can generate boxed kernels"
    },
    {
      "file": "atenspace/aten/src/ATen/core/op_registration/op_registration.h",
      "line": 302,
      "type": "TODO",
      "content": "// TODO Remove impl_unboxedOnlyCatchAllKernel once all of aten can generate boxed kernels",
      "context": "      );\n    }\n\n    // TODO Remove impl_unboxedOnlyCatchAllKernel once all of aten can generate boxed kernels\n    template<class FuncType, FuncType* kernel_func>\n    // enable_if: only enable it if FuncType is actually a function\n    guts::enable_if_t<guts::is_function_type<FuncType>::value, Options&&> impl_unboxedOnlyCatchAllKernel() && {\n",
      "description": "Remove impl_unboxedOnlyCatchAllKernel once all of aten can generate boxed kernels"
    },
    {
      "file": "atenspace/aten/src/ATen/core/op_registration/op_registration.h",
      "line": 351,
      "type": "TODO",
      "content": "// TODO Do schema inference without relying on WrapRuntimeKernelFunctor",
      "context": "      return std::move(*this).kernel(\n        std::move(dispatch_key),\n        KernelFunction::makeFromUnboxedLambda(std::forward<Lambda>(functor)),\n        // TODO Do schema inference without relying on WrapRuntimeKernelFunctor\n        detail::FunctionSchemaInferer<detail::WrapRuntimeKernelFunctor<guts::decay_t<Lambda>>>()()\n      );\n    }\n",
      "description": "Do schema inference without relying on WrapRuntimeKernelFunctor"
    },
    {
      "file": "atenspace/aten/src/ATen/core/op_registration/op_registration.h",
      "line": 391,
      "type": "TODO",
      "content": "// TODO Do schema inference without relying on WrapRuntimeKernelFunctor",
      "context": "      return std::move(*this).kernel(\n        c10::nullopt,\n        KernelFunction::makeFromUnboxedLambda(std::forward<Lambda>(lambda)),\n        // TODO Do schema inference without relying on WrapRuntimeKernelFunctor\n        detail::FunctionSchemaInferer<detail::WrapRuntimeKernelFunctor<guts::decay_t<Lambda>>>()()\n      );\n    }\n",
      "description": "Do schema inference without relying on WrapRuntimeKernelFunctor"
    },
    {
      "file": "atenspace/aten/src/ATen/core/op_registration/op_registration.h",
      "line": 518,
      "type": "TODO",
      "content": "// TODO Do schema inference without relying on WrapRuntimeKernelFunctor",
      "context": "     return std::move(*this).op(std::move(options).schema(schemaOrName).kernel(\n       c10::nullopt,\n       KernelFunction::makeFromUnboxedRuntimeFunction<AllowLegacyTypes>(func),\n       // TODO Do schema inference without relying on WrapRuntimeKernelFunctor\n       detail::FunctionSchemaInferer<detail::WrapRuntimeKernelFunctor<guts::decay_t<FuncType>>>()()\n     ));\n   }\n",
      "description": "Do schema inference without relying on WrapRuntimeKernelFunctor"
    },
    {
      "file": "atenspace/aten/src/ATen/core/op_registration/op_registration.h",
      "line": 548,
      "type": "TODO",
      "content": "// TODO Do schema inference without relying on WrapRuntimeKernelFunctor",
      "context": "      return std::move(*this).op(std::move(options).schema(schemaOrName).kernel(\n        c10::nullopt,\n        KernelFunction::makeFromUnboxedLambda<AllowLegacyTypes>(std::forward<Lambda>(lambda)),\n        // TODO Do schema inference without relying on WrapRuntimeKernelFunctor\n        detail::FunctionSchemaInferer<detail::WrapRuntimeKernelFunctor<guts::decay_t<Lambda>>>()()\n      ));\n    }\n",
      "description": "Do schema inference without relying on WrapRuntimeKernelFunctor"
    },
    {
      "file": "atenspace/aten/src/ATen/core/op_registration/op_registration.h",
      "line": 564,
      "type": "TODO",
      "content": "// TODO Do schema inference without relying on WrapRuntimeKernelFunctor",
      "context": "      return std::move(*this).op(std::move(options).schema(schemaOrName).kernel(\n        c10::nullopt,\n        KernelFunction::makeFromUnboxedLambda<AllowLegacyTypes>(std::forward<Lambda>(lambda)),\n        // TODO Do schema inference without relying on WrapRuntimeKernelFunctor\n        detail::FunctionSchemaInferer<detail::WrapRuntimeKernelFunctor<guts::decay_t<Lambda>>>()()\n      ));\n    }\n",
      "description": "Do schema inference without relying on WrapRuntimeKernelFunctor"
    },
    {
      "file": "atenspace/aten/src/ATen/core/boxing/kernel_functor.h",
      "line": 8,
      "type": "TODO",
      "content": "using Stack = torch::jit::Stack; // TODO Instead of this, move torch::jit::Stack to the c10 namespace.",
      "context": "\nnamespace c10 {\n\nusing Stack = torch::jit::Stack; // TODO Instead of this, move torch::jit::Stack to the c10 namespace.\n\n/**\n * Inherit from OperatorKernel to implement a c10 kernel.\n",
      "description": "Instead of this, move torch::jit::Stack to the c10 namespace."
    },
    {
      "file": "atenspace/aten/src/ATen/core/boxing/kernel_functor.h",
      "line": 83,
      "type": "TODO",
      "content": "// TODO static_assert(AllowDeprecatedTypes, \"You tried to register a kernel with an unsupported input type: std::vector<T>. Please use List<T> instead.\");",
      "context": "  struct assert_is_valid_input_type<std::vector<T>, AllowDeprecatedTypes>\n  : assert_is_valid_input_type<T, AllowDeprecatedTypes> {\n    static_assert(!std::is_same<T, at::Scalar>::value, \"You tried to register a kernel with an unsupported input type: std::vector<Scalar>. Please use List<int64_t>, List<double> or Tensor instead.\");\n    // TODO static_assert(AllowDeprecatedTypes, \"You tried to register a kernel with an unsupported input type: std::vector<T>. Please use List<T> instead.\");\n  };\n\n  // The following specialisations of assert_is_valid_input_type are technically not\n",
      "description": "static_assert(AllowDeprecatedTypes, \"You tried to register a kernel with an unsupported input type: std::vector<T>. Please use List<T> instead.\");"
    },
    {
      "file": "atenspace/aten/src/ATen/core/boxing/kernel_functor.h",
      "line": 149,
      "type": "TODO",
      "content": "// TODO static_assert(AllowDeprecatedTypes, \"You tried to register a kernel with an unsupported output type: std::vector<T>. Please use List<T> instead.\");",
      "context": "  struct assert_is_valid_output_type<std::vector<T>, AllowDeprecatedTypes>\n  : assert_is_valid_output_type<T, AllowDeprecatedTypes> {\n    static_assert(!std::is_same<T, at::Scalar>::value, \"You tried to register a kernel with an unsupported output type: std::vector<Scalar>. Please use List<int64_t>, List<double> or Tensor instead.\");\n    // TODO static_assert(AllowDeprecatedTypes, \"You tried to register a kernel with an unsupported output type: std::vector<T>. Please use List<T> instead.\");\n  };\n\n  // The following specialisations of assert_is_valid_output_type are technically not\n",
      "description": "static_assert(AllowDeprecatedTypes, \"You tried to register a kernel with an unsupported output type: std::vector<T>. Please use List<T> instead.\");"
    },
    {
      "file": "atenspace/aten/src/ATen/core/boxing/KernelFunction.h",
      "line": 33,
      "type": "TODO",
      "content": "// TODO We want to introduce the invariant that all kernels must be callable in a boxed way, then this should only check boxed_kernel_func_.",
      "context": "  {}\n\n  bool isValid() const {\n    // TODO We want to introduce the invariant that all kernels must be callable in a boxed way, then this should only check boxed_kernel_func_.\n    return boxed_kernel_func_ != nullptr || unboxed_kernel_func_ != nullptr;\n  }\n\n",
      "description": "We want to introduce the invariant that all kernels must be callable in a boxed way, then this should only check boxed_kernel_func_."
    },
    {
      "file": "atenspace/aten/src/ATen/core/boxing/KernelFunction.h",
      "line": 60,
      "type": "TODO",
      "content": "// TODO We want to introduce the invariant that all kernels must be callable in a boxed way, then this case should be impossible.",
      "context": "      if (unboxed_kernel_func_ == nullptr) {\n        TORCH_INTERNAL_ASSERT(false, \"Tried to call KernelFunction::callBoxed() on an uninitialized KernelFunction.\");\n      } else {\n        // TODO We want to introduce the invariant that all kernels must be callable in a boxed way, then this case should be impossible.\n        TORCH_INTERNAL_ASSERT(false, \"Tried to call KernelFunction::callBoxed() on a KernelFunction that can only be called with KernelFunction::callUnboxed().\");\n      }\n    }\n",
      "description": "We want to introduce the invariant that all kernels must be callable in a boxed way, then this case should be impossible."
    },
    {
      "file": "atenspace/aten/src/ATen/core/boxing/KernelFunction.h",
      "line": 90,
      "type": "TODO",
      "content": "// TODO Remove this function once all kernels support a boxed variant",
      "context": "    // forwarding, which would require Args to be deduced, but instead we\n    // want callers to explicitly specify the Args.\n\n    // TODO Remove this function once all kernels support a boxed variant\n\n    if (C10_LIKELY(unboxed_kernel_func_ != nullptr)) {\n      using ActualSignature = Return (OperatorKernel*, Args...);\n",
      "description": "Remove this function once all kernels support a boxed variant"
    },
    {
      "file": "atenspace/aten/src/ATen/core/boxing/KernelFunction.h",
      "line": 230,
      "type": "TODO",
      "content": "// TODO We want to get rid of kernels that have only an unboxed function pointer.",
      "context": "   */\n  template<class KernelFunctor>\n  static KernelFunction makeFromUnboxedOnlyFunctor(std::unique_ptr<OperatorKernel> kernelFunctor) {\n    // TODO We want to get rid of kernels that have only an unboxed function pointer.\n    //      All kernels should have a boxed pointer.\n\n    static_assert(guts::is_functor<KernelFunctor>::value, \"Tried to call KernelFunction::makeFromUnboxedFunctor<KernelFunctor> but the argument is not a functor.\");\n",
      "description": "We want to get rid of kernels that have only an unboxed function pointer."
    },
    {
      "file": "atenspace/aten/src/ATen/core/boxing/KernelFunction.h",
      "line": 284,
      "type": "TODO",
      "content": "// TODO We want to get rid of kernels that have only an unboxed function pointer.",
      "context": "   */\n  template<class FuncType, FuncType* func>\n  static KernelFunction makeFromUnboxedOnlyFunction() {\n    // TODO We want to get rid of kernels that have only an unboxed function pointer.\n    //      All kernels should have a boxed pointer.\n\n    static_assert(guts::is_function_type<FuncType>::value, \"Tried to call KernelFunction::makeFromUnboxedOnlyFunction with invalid template parameters. They must be <FuncType, *func_ptr>.\");\n",
      "description": "We want to get rid of kernels that have only an unboxed function pointer."
    },
    {
      "file": "atenspace/aten/src/ATen/core/boxing/KernelFunction.h",
      "line": 386,
      "type": "TODO",
      "content": "// TODO Reuse stack vector instead of allocating?",
      "context": "template<class Return, class... Args>\nstruct boxAndCallBoxedFunc final {\n  static Return call(KernelFunction::BoxedKernelFunction* boxed_kernel_func, OperatorKernel* functor, Args... args) {\n    // TODO Reuse stack vector instead of allocating?\n    std::vector<IValue> stack {std::forward<Args>(args)...};\n\n    (*boxed_kernel_func)(functor, &stack);\n",
      "description": "Reuse stack vector instead of allocating?"
    },
    {
      "file": "atenspace/aten/src/ATen/core/boxing/KernelFunction.h",
      "line": 398,
      "type": "TODO",
      "content": "// TODO Reuse stack vector instead of allocating?",
      "context": "template<class... Args>\nstruct boxAndCallBoxedFunc<void, Args...> final {\n  static void call(KernelFunction::BoxedKernelFunction* boxed_kernel_func, OperatorKernel* functor, Args... args) {\n    // TODO Reuse stack vector instead of allocating?\n    std::vector<IValue> stack {std::forward<Args>(args)...};\n\n    (*boxed_kernel_func)(functor, &stack);\n",
      "description": "Reuse stack vector instead of allocating?"
    },
    {
      "file": "atenspace/aten/src/ATen/core/boxing/KernelFunction_test.cpp",
      "line": 413,
      "type": "TODO",
      "content": "// TODO Also test different variants of calling unboxed with wrong signatures",
      "context": "\n}\n\n// TODO Also test different variants of calling unboxed with wrong signatures\n",
      "description": "Also test different variants of calling unboxed with wrong signatures"
    },
    {
      "file": "atenspace/aten/src/ATen/detail/CUDAHooksInterface.h",
      "line": 53,
      "type": "TODO",
      "content": "// TODO: Consider putting the stub definitions in another class, so that one",
      "context": "// (2) should filter out many ostensible use-cases, since many times a CUDA\n// function provided by ATen is only really ever used by actual CUDA code.\n//\n// TODO: Consider putting the stub definitions in another class, so that one\n// never forgets to implement each virtual function in the real implementation\n// in CUDAHooks.  This probably doesn't buy us much though.\nstruct CAFFE2_API CUDAHooksInterface {\n",
      "description": "Consider putting the stub definitions in another class, so that one"
    },
    {
      "file": "atenspace/aten/src/ATen/cpu/vec256/functional.h",
      "line": 6,
      "type": "TODO",
      "content": "// TODO: Make this more efficient",
      "context": "\nnamespace at { namespace vec256 {\n\n// TODO: Make this more efficient\ntemplate <typename scalar_t, typename Op>\ninline scalar_t vec_reduce_all(\n    const Op& vec_fun,\n",
      "description": "Make this more efficient"
    },
    {
      "file": "atenspace/aten/src/ATen/cpu/vec256/vec256.h",
      "line": 151,
      "type": "TODO",
      "content": "#else  // TODO Remove else case once switch to C++14 is finished",
      "context": "#if __cpp_binary_literals >= 201304L\n  auto a_swapped = _mm256_permute2f128_pd(a, b, 0b0100000);\n  auto b_swapped = _mm256_permute2f128_pd(a, b, 0b0110001);\n#else  // TODO Remove else case once switch to C++14 is finished\n  static constexpr int swap_ctrl_a = 0 | (2 << 4);  // 0, 2.   4 bits apart\n  static constexpr int swap_ctrl_b = 1 | (3 << 4);  // 1, 3.   4 bits apart\n  auto a_swapped = _mm256_permute2f128_pd(a, b, swap_ctrl_a);\n",
      "description": "Remove else case once switch to C++14 is finished"
    },
    {
      "file": "atenspace/aten/src/ATen/cpu/vec256/vec256.h",
      "line": 164,
      "type": "TODO",
      "content": "#else  // TODO Remove else case once switch to C++14 is finished",
      "context": "#if __cpp_binary_literals >= 201304L\n  return std::make_pair(_mm256_permute4x64_pd(a_swapped, 0b11011000),\n                        _mm256_permute4x64_pd(b_swapped, 0b11011000));\n#else  // TODO Remove else case once switch to C++14 is finished\n  static constexpr int group_ctrl = 0 | (2 << 2) | (1 << 4) | (3 << 6);  // 0, 2, 1, 3\n  return std::make_pair(_mm256_permute4x64_pd(a_swapped, group_ctrl),\n                        _mm256_permute4x64_pd(b_swapped, group_ctrl));\n",
      "description": "Remove else case once switch to C++14 is finished"
    },
    {
      "file": "atenspace/aten/src/ATen/cpu/vec256/vec256.h",
      "line": 181,
      "type": "TODO",
      "content": "// TODO: can we support caching this?",
      "context": "  // swap lanes:\n  //   a_swapped = {a0, a1, a2, a3, b0, b1, b2, b3}\n  //   b_swapped = {a4, a5, a6, a7, b4, b5, b6, b7}\n  // TODO: can we support caching this?\n#if __cpp_binary_literals >= 201304L\n  auto a_swapped = _mm256_permute2f128_ps(a, b, 0b0100000);\n  auto b_swapped = _mm256_permute2f128_ps(a, b, 0b0110001);\n",
      "description": "can we support caching this?"
    },
    {
      "file": "atenspace/aten/src/ATen/cpu/vec256/vec256.h",
      "line": 185,
      "type": "TODO",
      "content": "#else  // TODO Remove else case once switch to C++14 is finished",
      "context": "#if __cpp_binary_literals >= 201304L\n  auto a_swapped = _mm256_permute2f128_ps(a, b, 0b0100000);\n  auto b_swapped = _mm256_permute2f128_ps(a, b, 0b0110001);\n#else  // TODO Remove else case once switch to C++14 is finished\n  static constexpr int swap_ctrl_a = 0 | (2 << 4);  // 0, 2.   4 bits apart\n  static constexpr int swap_ctrl_b = 1 | (3 << 4);  // 1, 3.   4 bits apart\n  auto a_swapped = _mm256_permute2f128_ps(a, b, swap_ctrl_a);\n",
      "description": "Remove else case once switch to C++14 is finished"
    },
    {
      "file": "atenspace/aten/src/ATen/cpu/vec256/vec256.h",
      "line": 215,
      "type": "TODO",
      "content": "#else  // TODO Remove else case once switch to C++14 is finished",
      "context": "#if __cpp_binary_literals >= 201304L\n  auto a_grouped = _mm256_permute4x64_pd(a, 0b11011000);\n  auto b_grouped = _mm256_permute4x64_pd(b, 0b11011000);\n#else  // TODO Remove else case once switch to C++14 is finished\n  static constexpr int group_ctrl = 0 | (2 << 2) | (1 << 4) | (3 << 6);  // 0, 2, 1, 3\n  auto a_grouped = _mm256_permute4x64_pd(a, group_ctrl);\n  auto b_grouped = _mm256_permute4x64_pd(b, group_ctrl);\n",
      "description": "Remove else case once switch to C++14 is finished"
    },
    {
      "file": "atenspace/aten/src/ATen/cpu/vec256/vec256.h",
      "line": 227,
      "type": "TODO",
      "content": "#else  // TODO Remove else case once switch to C++14 is finished",
      "context": "#if __cpp_binary_literals >= 201304L\n  return std::make_pair(_mm256_permute2f128_pd(a_grouped, b_grouped, 0b0100000),\n                        _mm256_permute2f128_pd(a_grouped, b_grouped, 0b0110001));\n#else  // TODO Remove else case once switch to C++14 is finished\n  static constexpr int swap_ctrl_a = 0 | (2 << 4);  // 0, 2.   4 bits apart\n  static constexpr int swap_ctrl_b = 1 | (3 << 4);  // 1, 3.   4 bits apart\n  return std::make_pair(_mm256_permute2f128_pd(a_grouped, b_grouped, swap_ctrl_a),\n",
      "description": "Remove else case once switch to C++14 is finished"
    },
    {
      "file": "atenspace/aten/src/ATen/cpu/vec256/vec256.h",
      "line": 245,
      "type": "TODO",
      "content": "// TODO: can we support caching this?",
      "context": "  // group cols crossing lanes:\n  //   a_grouped = {a0, a1, a2, a3, b0, b1, b2, b3}\n  //   b_grouped = {a4, a5, a6, a7, b4, b5, b6, b7}\n  // TODO: can we support caching this?\n  const __m256i group_ctrl = _mm256_setr_epi32(0, 2, 4, 6, 1, 3, 5, 7);\n  auto a_grouped = _mm256_permutevar8x32_ps(a, group_ctrl);\n  auto b_grouped = _mm256_permutevar8x32_ps(b, group_ctrl);\n",
      "description": "can we support caching this?"
    },
    {
      "file": "atenspace/aten/src/ATen/cpu/vec256/vec256.h",
      "line": 256,
      "type": "TODO",
      "content": "#else  // TODO Remove else case once switch to C++14 is finished",
      "context": "#if __cpp_binary_literals >= 201304L\n  return std::make_pair(_mm256_permute2f128_ps(a_grouped, b_grouped, 0b0100000),\n                        _mm256_permute2f128_ps(a_grouped, b_grouped, 0b0110001));\n#else  // TODO Remove else case once switch to C++14 is finished\n  static constexpr int swap_ctrl_a = 0 | (2 << 4);  // 0, 2.   4 bits apart\n  static constexpr int swap_ctrl_b = 1 | (3 << 4);  // 1, 3.   4 bits apart\n  return std::make_pair(_mm256_permute2f128_ps(a_grouped, b_grouped, swap_ctrl_a),\n",
      "description": "Remove else case once switch to C++14 is finished"
    },
    {
      "file": "atenspace/aten/src/ATen/test/basic.cpp",
      "line": 74,
      "type": "TODO",
      "content": "// TODO:0-dim Tensor d(3.f);",
      "context": "  Tensor a = rand({3, 4}, type);\n  Tensor b = rand({3, 4}, type);\n  Tensor c = add(a, add(a, b));\n  // TODO:0-dim Tensor d(3.f);\n  Scalar d = 3.f;\n  ASSERT_TRUE(add(c, d).allclose(a + a + b + d));\n}\n",
      "description": "0-dim Tensor d(3.f);"
    },
    {
      "file": "atenspace/aten/src/ATen/test/basic.cpp",
      "line": 87,
      "type": "TODO",
      "content": "// TODO TEST PERF?",
      "context": "    add_out(r, r, d);\n  }\n  auto end = std::chrono::high_resolution_clock::now();\n  // TODO TEST PERF?\n  std::cout << std::dec << \"   \"\n            << std::chrono::duration_cast<std::chrono::milliseconds>(\n                   end - begin)\n",
      "description": "TEST PERF?"
    },
    {
      "file": "atenspace/aten/src/ATen/test/basic.cpp",
      "line": 104,
      "type": "TODO",
      "content": "// TODO TEST PERF?",
      "context": "    r = add(r, d);\n  }\n  auto end = std::chrono::high_resolution_clock::now();\n  // TODO TEST PERF?\n  std::cout << std::dec << \"   \"\n            << std::chrono::duration_cast<std::chrono::milliseconds>(\n                   end - begin)\n",
      "description": "TEST PERF?"
    },
    {
      "file": "atenspace/aten/src/ATen/test/basic.cpp",
      "line": 140,
      "type": "TODO",
      "content": "// TODO 0-dim squeeze",
      "context": "  ASSERT_EQ_RESOLVED(b.dim(), 1);\n  a = rand({1}, type);\n  b = squeeze(a);\n  // TODO 0-dim squeeze\n  ASSERT_TRUE(a[0].equal(b));\n}\n\n",
      "description": "0-dim squeeze"
    },
    {
      "file": "atenspace/aten/src/ATen/test/atest.cpp",
      "line": 69,
      "type": "TODO",
      "content": "// TODO(ezyang): maybe do a more precise exception type.",
      "context": "  ASSERT_EQ(f.sizes()[1], 2);\n  ASSERT_EQ(f.sizes()[2], 3);\n\n  // TODO(ezyang): maybe do a more precise exception type.\n  ASSERT_THROW(f.resize_({3, 4, 5}), std::exception);\n  {\n    int isgone = 0;\n",
      "description": "(ezyang): maybe do a more precise exception type."
    },
    {
      "file": "atenspace/aten/src/ATen/native/Copy.cpp",
      "line": 90,
      "type": "TODO",
      "content": "// TODO: this should be handled during dispatch, but that's missing...",
      "context": "namespace native {\n\nstatic Tensor & copy_impl(Tensor & self, const Tensor & src, bool non_blocking) {\n  // TODO: this should be handled during dispatch, but that's missing...\n  TORCH_CHECK(self.defined(), \"self is undefined\");\n  TORCH_CHECK(src.defined(), \"src is undefined\");\n\n",
      "description": "this should be handled during dispatch, but that's missing..."
    },
    {
      "file": "atenspace/aten/src/ATen/native/Copy.cpp",
      "line": 142,
      "type": "TODO",
      "content": "// TODO: if we need to, we can also enable this path for quantized tensor",
      "context": "    device_type = kCUDA;\n  }\n\n  // TODO: if we need to, we can also enable this path for quantized tensor\n  if (device_type == kCPU && copy_transpose_valid(self, src) && !self.is_quantized()) {\n    copy_same_type_transpose_(self, src);\n    return self;\n",
      "description": "if we need to, we can also enable this path for quantized tensor"
    },
    {
      "file": "atenspace/aten/src/ATen/native/NaiveDilatedConvolution.cpp",
      "line": 360,
      "type": "TODO",
      "content": "scalar_t scale = 1; // TODO: expose as argument?",
      "context": "            pad_size,\n            dilation_size,\n            columns.data_ptr<scalar_t>());\n        scalar_t scale = 1; // TODO: expose as argument?\n        /*\n          Compute:\n\n",
      "description": "expose as argument?"
    },
    {
      "file": "atenspace/aten/src/ATen/native/Distributions.cpp",
      "line": 125,
      "type": "TODO",
      "content": "// TODO: Fix resize_as_. See pytorch/pytorch#11665.",
      "context": "Tensor& bernoulli_out(Tensor& result, const Tensor& self, Generator* gen) {\n  // result.resize_as_(self) requires self to have same dtype as result, so we\n  // use resize_ instead.\n  // TODO: Fix resize_as_. See pytorch/pytorch#11665.\n  result.resize_(self.sizes()).bernoulli_(self, gen);\n#ifdef BUILD_NAMEDTENSOR\n  namedinference::propagate_names(result, self);\n",
      "description": "Fix resize_as_. See pytorch/pytorch#11665."
    },
    {
      "file": "atenspace/aten/src/ATen/native/Resize.h",
      "line": 43,
      "type": "FIXME",
      "content": "// FIXME: Don't rely on storage_size being negative because this",
      "context": "    self->set_sizes_and_strides(size, *stride);\n    // NB: storage size can be different from numel.\n    for (size_t dim = 0; dim < size.size(); ++dim) {\n      // FIXME: Don't rely on storage_size being negative because this\n      // may not be true for some edge cases.\n      if (size[dim] == 0) {\n        storage_size = 0;\n",
      "description": "Don't rely on storage_size being negative because this"
    },
    {
      "file": "atenspace/aten/src/ATen/native/MaxUnpooling.cpp",
      "line": 552,
      "type": "TODO",
      "content": "// TODO (from THNN): check gradOutput shape",
      "context": "  max_unpooling3d_shape_check(\n      self, grad_output_, indices_, output_size, stride, padding);\n\n  // TODO (from THNN): check gradOutput shape\n  /* get contiguous gradOutput */\n  auto grad_output = grad_output_.contiguous();\n  auto indices = indices_.contiguous();\n",
      "description": "(from THNN): check gradOutput shape"
    },
    {
      "file": "atenspace/aten/src/ATen/native/RNN.cpp",
      "line": 389,
      "type": "TODO",
      "content": "// TODO: can use inplace ops?",
      "context": "  }\n};\n\n// TODO: can use inplace ops?\ntemplate <typename cell_params>\nstruct LSTMCell : Cell<std::tuple<Tensor, Tensor>, cell_params> {\n  using hidden_type = std::tuple<Tensor, Tensor>;\n",
      "description": "can use inplace ops?"
    },
    {
      "file": "atenspace/aten/src/ATen/native/Integration.cpp",
      "line": 17,
      "type": "TODO",
      "content": "// TODO: if we extend TensorIterator to accept 3 inputs,",
      "context": "//\n// \\sum_{i=1}^{n-1}  dx_i * (y_i + y_{i+1}) / 2\n//\n// TODO: if we extend TensorIterator to accept 3 inputs,\n// we can probably make this a bit more performant.\nTensor do_trapz(const Tensor& y, const Tensor& dx, int64_t dim) {\n    Tensor left = y.slice(dim, 0, -1);\n",
      "description": "if we extend TensorIterator to accept 3 inputs,"
    },
    {
      "file": "atenspace/aten/src/ATen/native/Normalization.cpp",
      "line": 521,
      "type": "TODO",
      "content": "// TODO: _batch_norm_impl_index_backward is only used in JIT. cudnn NHWC",
      "context": "  if (impl_index == 0) {\n    return at::native_batch_norm_backward(grad_output, input, weight, running_mean, running_var, save_mean, save_var_transform, train, epsilon, output_mask);\n  } else if (impl_index == 1) {\n    // TODO: _batch_norm_impl_index_backward is only used in JIT. cudnn NHWC\n    // format conversion is done inside cudnn_batch_norm_backward instead\n    return at::cudnn_batch_norm_backward(input, grad_output, weight, running_mean, running_var, save_mean, save_var_transform, epsilon, reservedSpace);\n  } else if (impl_index == 2) {\n",
      "description": "_batch_norm_impl_index_backward is only used in JIT. cudnn NHWC"
    },
    {
      "file": "atenspace/aten/src/ATen/native/TensorCompare.cpp",
      "line": 54,
      "type": "TODO",
      "content": "// TODO: use bitwise operator overloads once we add them",
      "context": "}\n\nTensor isclose(const Tensor& self, const Tensor& other, double rtol, double atol, bool equal_nan) {\n  // TODO: use bitwise operator overloads once we add them\n\n  TORCH_CHECK(self.scalar_type() == other.scalar_type(), self.scalar_type(), \" did not match \", other.scalar_type())\n\n",
      "description": "use bitwise operator overloads once we add them"
    },
    {
      "file": "atenspace/aten/src/ATen/native/TensorCompare.cpp",
      "line": 194,
      "type": "TODO",
      "content": "// TODO: qscheme",
      "context": "  if (self.is_quantized()) {\n    Tensor max = at::empty({0}, self.options().dtype(toUnderlying(self.scalar_type())));\n    at::native::max_out(max, max_indices, self.int_repr(), dim, keepdim);\n    // TODO: qscheme\n    return std::tuple<Tensor, Tensor>(at::_make_per_tensor_quantized_tensor(max, self.q_scale(), self.q_zero_point()), max_indices);\n  } else {\n    Tensor  max = at::empty({0}, self.options());\n",
      "description": "qscheme"
    },
    {
      "file": "atenspace/aten/src/ATen/native/TensorShape.cpp",
      "line": 683,
      "type": "TODO",
      "content": "// TODO: support negative strides",
      "context": "  dim = maybe_wrap_dim(dim, ndim);\n  auto sizes = self.sizes().vec();\n  auto strides = self.strides().vec();\n  // TODO: support negative strides\n  TORCH_CHECK(step > 0, \"slice step must be positive\");\n  if (start < 0) {\n    start += sizes[dim];\n",
      "description": "support negative strides"
    },
    {
      "file": "atenspace/aten/src/ATen/native/Embedding.cpp",
      "line": 19,
      "type": "TODO",
      "content": "// TODO: use tensor.index() after improving perf",
      "context": "  auto indices_arg = TensorArg(indices, \"indices\", 1);\n  checkScalarType(\"embedding\", indices_arg, kLong);\n\n  // TODO: use tensor.index() after improving perf\n  if (indices.dim() == 1) {\n    return weight.index_select(0, indices);\n  }\n",
      "description": "use tensor.index() after improving perf"
    },
    {
      "file": "atenspace/aten/src/ATen/native/TensorFactories.cpp",
      "line": 179,
      "type": "TODO",
      "content": "// TODO: remove when we have Type support in the IR",
      "context": "// Temporary type cast operators. These are needed to trace type-casts now since\n// Type's are not supported in the IR. Instead, we call down to these\n// specialized operators for each datatype.\n// TODO: remove when we have Type support in the IR\n\n#define DEFINE_CAST_OP(_1, n)                                    \\\n  Tensor _cast_##n(const Tensor& self, bool non_blocking) {      \\\n",
      "description": "remove when we have Type support in the IR"
    },
    {
      "file": "atenspace/aten/src/ATen/native/TensorFactories.cpp",
      "line": 219,
      "type": "TODO",
      "content": "// TODO: To support all features of MemoryFormat::Preserve we need to add",
      "context": "    auto memory_format =\n        optional_memory_format.value_or(MemoryFormat::Contiguous);\n\n    // TODO: To support all features of MemoryFormat::Preserve we need to add\n    // _empty_affine_quantized_strided function and use it similarly to\n    // Tensor clone(const Tensor& src, c10::optional<c10::MemoryFormat> optional_memory_format)\n    // if (self.is_non_overlapping_and_dense()) -> _empty_affine_quantized_strided\n",
      "description": "To support all features of MemoryFormat::Preserve we need to add"
    },
    {
      "file": "atenspace/aten/src/ATen/native/DispatchStub.h",
      "line": 35,
      "type": "TODO",
      "content": "// TODO: CPU instruction set selection should be folded into whatever",
      "context": "// To call:\n//   stub(kCPU, tensor);\n//\n// TODO: CPU instruction set selection should be folded into whatever\n// the main dispatch mechanism is.\n\n// ignore warnings about DispatchStub::DEFAULT, AVX, AVX2 defined elsewhere\n",
      "description": "CPU instruction set selection should be folded into whatever"
    },
    {
      "file": "atenspace/aten/src/ATen/native/DispatchStub.h",
      "line": 134,
      "type": "TODO",
      "content": "// TODO: make this point at hip_dispatch_ptr",
      "context": "template <typename FnPtr, typename T>\nstruct RegisterHIPDispatch {\n  RegisterHIPDispatch(DispatchStub<FnPtr, T>& stub, FnPtr value) {\n    // TODO: make this point at hip_dispatch_ptr\n    stub.cuda_dispatch_ptr = value;\n  }\n};\n",
      "description": "make this point at hip_dispatch_ptr"
    },
    {
      "file": "atenspace/aten/src/ATen/native/DispatchStub.h",
      "line": 186,
      "type": "TODO",
      "content": "// TODO: cut this over to HIP dispatch once we stop pretending that CUDA",
      "context": "#if defined(__CUDACC__)\n#define REGISTER_DISPATCH(name, fn) REGISTER_CUDA_DISPATCH(name, fn)\n#elif defined(__HIPCC__)\n// TODO: cut this over to HIP dispatch once we stop pretending that CUDA\n// is HIP in the PyTorch HIPify build.\n#define REGISTER_DISPATCH(name, fn) REGISTER_CUDA_DISPATCH(name, fn)\n// #define REGISTER_DISPATCH(name, fn) REGISTER_HIP_DISPATCH(name, fn)\n",
      "description": "cut this over to HIP dispatch once we stop pretending that CUDA"
    },
    {
      "file": "atenspace/aten/src/ATen/native/AdaptiveAveragePooling.cpp",
      "line": 329,
      "type": "TODO",
      "content": "// TODO: fastpath for Channels_last should be explored later;",
      "context": "      return at::mkldnn_adaptive_avg_pool2d(input, output_size);\n    }\n\n    // TODO: fastpath for Channels_last should be explored later;\n    if (input.suggest_memory_format() == at::MemoryFormat::Contiguous && !input.is_quantized() && output_size[0] == 1 && output_size[1] == 1) {\n      // in this case, adaptive pooling is just computing mean over hw\n      // dimensions, which can be done more efficiently\n",
      "description": "fastpath for Channels_last should be explored later;"
    },
    {
      "file": "atenspace/aten/src/ATen/native/Convolution.cpp",
      "line": 918,
      "type": "TODO",
      "content": "// TODO: figure out if we can narrow gO and save some compute,",
      "context": "\n        // narrow gI to only relevant portion\n        // we do it this way because negative output_padding is not supported\n        // TODO: figure out if we can narrow gO and save some compute,\n        // rather than narrowing the computed gI\n        auto gI_size = gI.sizes();\n        auto i_size = input.sizes();\n",
      "description": "figure out if we can narrow gO and save some compute,"
    },
    {
      "file": "atenspace/aten/src/ATen/native/Convolution.cpp",
      "line": 938,
      "type": "TODO",
      "content": "// TODO: figure out why this needs to be computed...",
      "context": "        auto gOt = gO.transpose(0, 1);\n\n        // calculate output_padding\n        // TODO: figure out why this needs to be computed...\n        auto kernel_size = weight.sizes().slice(2);\n        auto input_shape = input.sizes().slice(2);\n        auto grad_output_shape = gO.sizes().slice(2);\n",
      "description": "figure out why this needs to be computed..."
    },
    {
      "file": "atenspace/aten/src/ATen/native/Sorting.cpp",
      "line": 105,
      "type": "FIXME",
      "content": "// FIXME: This seems bogus, I only do this because it was the old behaviour.",
      "context": "    int64_t dim_,\n    bool keepdim) {\n  int64_t dim = maybe_wrap_dim(dim_, self.dim(), /*wrap_scalar=*/true);\n  // FIXME: This seems bogus, I only do this because it was the old behaviour.\n  //        The reductions are fine, as long as the axis being reduced along\n  //        isn't of 0 elements (and the output has elements).\n  TORCH_CHECK(\n",
      "description": "This seems bogus, I only do this because it was the old behaviour."
    },
    {
      "file": "atenspace/aten/src/ATen/native/Resize.cpp",
      "line": 38,
      "type": "TODO",
      "content": "// TODO(VitalyFedyunin): Move it to HTML docs.",
      "context": "Tensor& resize_as_sparse_(Tensor& self, const Tensor& src);\n\n\n// TODO(VitalyFedyunin): Move it to HTML docs.\n//\n// Strides of the output tensor of `resize_as_` operator is defined by input\n// tensor strides and the value of memory_format argument.\n",
      "description": "(VitalyFedyunin): Move it to HTML docs."
    },
    {
      "file": "atenspace/aten/src/ATen/native/QuantizedLinear.cpp",
      "line": 353,
      "type": "TODO",
      "content": "// TODO(mingzhe09088):",
      "context": "  float* weight_contig_ptr = weight_contig.data_ptr<float>();\n  HandleWeightsSaturation(K * N, weight_contig_ptr);\n\n  // TODO(mingzhe09088):\n  // Consider using a functor here in PackedGemmMatrixFP16\n  // Comments from (XQ): Not entirely sure this make_unique is safe. make_unique\n  // is created with regular \"new\", and freed through TypeMetaData::deleteFn in\n",
      "description": "(mingzhe09088):"
    },
    {
      "file": "atenspace/aten/src/ATen/native/TensorIterator.cpp",
      "line": 117,
      "type": "TODO",
      "content": "//TODO refactor so that no tensor copies are done",
      "context": "  if (all_same_type) {\n    return std::make_tuple(device, common_type, true);\n  }\n  //TODO refactor so that no tensor copies are done\n  std::vector<Tensor> tensors;\n  std::transform(std::begin(operands), std::end(operands), std::back_inserter(tensors),\n                  [](const OperandInfo& op) { return op.tensor; });\n",
      "description": "refactor so that no tensor copies are done"
    },
    {
      "file": "atenspace/aten/src/ATen/native/TensorIterator.cpp",
      "line": 586,
      "type": "TODO",
      "content": "// TODO: check for casting once it's supported",
      "context": "}\n\nbool TensorIterator::is_trivial_1d() const {\n  // TODO: check for casting once it's supported\n  return ndim() == 1;\n}\n\n",
      "description": "check for casting once it's supported"
    },
    {
      "file": "atenspace/aten/src/ATen/native/TensorIterator.cpp",
      "line": 703,
      "type": "FIXME",
      "content": "// FIXME: workaround for bug: https://github.com/pytorch/pytorch/issues/20342",
      "context": "TensorIterator TensorIterator::nullary_op(Tensor& out) {\n  auto iter = TensorIterator();\n  iter.add_output(out);\n  // FIXME: workaround for bug: https://github.com/pytorch/pytorch/issues/20342\n  iter.resize_outputs_ = false;\n  iter.build();\n  return iter;\n",
      "description": "workaround for bug: https://github.com/pytorch/pytorch/issues/20342"
    },
    {
      "file": "atenspace/aten/src/ATen/native/TensorIterator.cpp",
      "line": 717,
      "type": "TODO",
      "content": "// TODO: This is only really necessary for arg{min,max}",
      "context": "  iter.promote_gpu_output_dtypes_ = true;\n  iter.resize_outputs_ = false;\n  iter.is_reduction_ = true;\n  // TODO: This is only really necessary for arg{min,max}\n  iter.compute_common_dtype_only_for_inputs();\n  iter.build();\n  return iter;\n",
      "description": "This is only really necessary for arg{min,max}"
    },
    {
      "file": "atenspace/aten/src/ATen/native/TensorIterator.cpp",
      "line": 813,
      "type": "TODO",
      "content": "// TODO: issue warning",
      "context": "    if (tensor.defined() && !tensor.sizes().equals(shape_)) {\n      if (resize_outputs_ && !operands_[i].is_read_write) {\n        // Preserve legacy resizing behavior of out=... arguments\n        // TODO: issue warning\n        tensor.resize_(shape_);\n        if (requires_channels_last_output_ && tensor.dim() == 4) {\n          // Temporary stick to 4d tensor, will update with arbitrary batched later on\n",
      "description": "issue warning"
    },
    {
      "file": "atenspace/aten/src/ATen/native/TensorIterator.cpp",
      "line": 914,
      "type": "TODO",
      "content": "//TODO enable fast handling for reductions",
      "context": "void TensorIterator::fast_set_up() {\n  //this function is called if all the inputs are contiguous to avoid needless reordering of dimensions\n  //and tracking output strides\n  //TODO enable fast handling for reductions\n  //TODO enable fast handling for channels_last\n\n  //allocate contiguous tensor for output\n",
      "description": "enable fast handling for reductions"
    },
    {
      "file": "atenspace/aten/src/ATen/native/TensorIterator.cpp",
      "line": 915,
      "type": "TODO",
      "content": "//TODO enable fast handling for channels_last",
      "context": "  //this function is called if all the inputs are contiguous to avoid needless reordering of dimensions\n  //and tracking output strides\n  //TODO enable fast handling for reductions\n  //TODO enable fast handling for channels_last\n\n  //allocate contiguous tensor for output\n  for (int i = 0; i < num_outputs_; i++){\n",
      "description": "enable fast handling for channels_last"
    },
    {
      "file": "atenspace/aten/src/ATen/native/UpSample.h",
      "line": 157,
      "type": "TODO",
      "content": "// TODO: Our current linear mode impls use unbound indices",
      "context": "    // for negative indices as they use 2 pixels to interpolate.\n    // For example, [-1, 0], they both use pixel 0 value so it\n    // doesn't affect if we bound the src_idx to 0 or not.\n    // TODO: Our current linear mode impls use unbound indices\n    // where we should and then remove this cubic flag.\n    // This matters in cubic mode, as we might need [-1, 0, 1, 2]\n    // to interpolate and the weights can be affected.\n",
      "description": "Our current linear mode impls use unbound indices"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/Conv.cpp",
      "line": 122,
      "type": "TODO",
      "content": "// TODO: Go through all the checking code again and make sure",
      "context": "\nnamespace at { namespace native {\n\n// TODO: Go through all the checking code again and make sure\n// we haven't missed anything.\n\n// ---------------------------------------------------------------------\n",
      "description": "Go through all the checking code again and make sure"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/Conv.cpp",
      "line": 197,
      "type": "TODO",
      "content": "// TODO: Move this into the standard library, with a better name?",
      "context": "  return weight_size;\n}\n\n// TODO: Move this into the standard library, with a better name?\nTensor narrowGroup(const Tensor& t, int dim, int group_idx, int64_t groups) {\n  auto group_size = t.size(dim) / groups;\n  return t.narrow(dim, group_idx * group_size, group_size);\n",
      "description": "Move this into the standard library, with a better name?"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/Conv.cpp",
      "line": 269,
      "type": "TODO",
      "content": "// TODO: check that output->size() matches output_sizes",
      "context": "  // Weight\n  checkSameDim(c, input, weight);\n\n  // TODO: check that output->size() matches output_sizes\n  // TODO: check that weight matches output->sizes()\n  checkSameDim(c, input, output);\n}\n",
      "description": "check that output->size() matches output_sizes"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/Conv.cpp",
      "line": 270,
      "type": "TODO",
      "content": "// TODO: check that weight matches output->sizes()",
      "context": "  checkSameDim(c, input, weight);\n\n  // TODO: check that output->size() matches output_sizes\n  // TODO: check that weight matches output->sizes()\n  checkSameDim(c, input, output);\n}\n\n",
      "description": "check that weight matches output->sizes()"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/Conv.cpp",
      "line": 293,
      "type": "TODO",
      "content": "// TODO: Use TensorGeometry here instead of the entire Tensor, which we",
      "context": "\n// NB: This can't be a constructor, because then ConvolutionParams\n// would not be a POD anymore.\n// TODO: Use TensorGeometry here instead of the entire Tensor, which we\n// don't actually need.  (OTOH: We can always pass in\n// grad_input/grad_output, so this is not very pressing)\nvoid setConvolutionParams(\n",
      "description": "Use TensorGeometry here instead of the entire Tensor, which we"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/Conv.cpp",
      "line": 344,
      "type": "TODO",
      "content": "// TODO: Use something less heavy duty than a big honking mutex",
      "context": "//\n// ---------------------------------------------------------------------\n\n// TODO: Use something less heavy duty than a big honking mutex\ntemplate <typename T>\nstruct BenchmarkCache {\n  std::mutex mutex;\n",
      "description": "Use something less heavy duty than a big honking mutex"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/Conv.cpp",
      "line": 370,
      "type": "TODO",
      "content": "// TODO: Stop manually allocating CUDA memory; allocate an ATen byte",
      "context": "BenchmarkCache<cudnnConvolutionBwdDataAlgoPerf_t> bwd_data_algos;\nBenchmarkCache<cudnnConvolutionBwdFilterAlgoPerf_t> bwd_filter_algos;\n\n// TODO: Stop manually allocating CUDA memory; allocate an ATen byte\n// tensor instead.\nstruct Workspace {\n  Workspace(size_t size) : size(size), data(NULL) {\n",
      "description": "Stop manually allocating CUDA memory; allocate an ATen byte"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/Conv.cpp",
      "line": 466,
      "type": "TODO",
      "content": "// TODO: Shouldn't all returned results be successful?",
      "context": "  if (args.params.deterministic) {\n    // iterate over perf results of all algorithms and find the best deterministic algo\n    for (int i = 0; i < n_algo; i++) {\n      // TODO: Shouldn't all returned results be successful?\n      // Double check documentation for cudnnFindConvolutionForwardAlgorithmEx\n      if (perfResults[i].status == CUDNN_STATUS_SUCCESS &&\n          perfResults[i].determinism == CUDNN_DETERMINISTIC) {\n",
      "description": "Shouldn't all returned results be successful?"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/Conv.cpp",
      "line": 838,
      "type": "TODO",
      "content": "// TODO: Consider renaming zero-indexed arguments to \"self\"",
      "context": "//  - Things that happen in TensorArg\n//    - Check arguments (type, GPU, shape)\n//\n// TODO: Consider renaming zero-indexed arguments to \"self\"\n\n\n\n",
      "description": "Consider renaming zero-indexed arguments to \"self\""
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/Conv.cpp",
      "line": 873,
      "type": "TODO",
      "content": "// TODO: when we do legacy group convolution support, we'll repeatedly",
      "context": "  args.odesc.set(output);\n  args.cdesc.set(dataType, input.dim() - 2, args.params.padding, args.params.stride, args.params.dilation, args.params.groups);\n\n  // TODO: when we do legacy group convolution support, we'll repeatedly\n  // reinitialize the workspace for each convolution we do.  This is\n  // wasteful; we'd rather reuse the workspace.  OTOH, legacy group\n  // convolution support is already pretty slow, so this might not\n",
      "description": "when we do legacy group convolution support, we'll repeatedly"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/RNN.cpp",
      "line": 291,
      "type": "TODO",
      "content": "// TODO: Actually, would this make ASAN's job harder catching",
      "context": "          mini_batch = input_sizes[1];\n        }\n        input_size = input_sizes[2];\n        // TODO: Actually, would this make ASAN's job harder catching\n        // an uninitialized access?\n        batch_sizes_sum = -1; // something bogus in case we access it\n      }\n",
      "description": "Actually, would this make ASAN's job harder catching"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/RNN.cpp",
      "line": 297,
      "type": "TODO",
      "content": "// TODO: check x for consistency with input_size?",
      "context": "      }\n    }\n\n    // TODO: check x for consistency with input_size?\n    std::vector<TensorDescriptor> descriptors(Tensor x) const {\n      auto is_input_packed = batch_sizes.size() != 0;\n      if (is_input_packed) {\n",
      "description": "check x for consistency with input_size?"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/RNN.cpp",
      "line": 340,
      "type": "TODO",
      "content": "// TODO: This is annoying, having to put the cudnnTensorDescriptor_t",
      "context": "      }\n    }\n\n    // TODO: This is annoying, having to put the cudnnTensorDescriptor_t\n    // in a contiguous array...\n    std::vector<cudnnTensorDescriptor_t> get_descs(const std::vector<TensorDescriptor>& descs) {\n      std::vector<cudnnTensorDescriptor_t> r;\n",
      "description": "This is annoying, having to put the cudnnTensorDescriptor_t"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/RNN.cpp",
      "line": 438,
      "type": "TODO",
      "content": "// TODO: The use of CPU tensor here is a bit goofy in C++,",
      "context": "          cudnnTensorFormat_t format;\n          int nb_dims;\n          constexpr int min_dim = 3;\n          // TODO: The use of CPU tensor here is a bit goofy in C++,\n          // some sort of alloca would be good enough except that it is\n          // kind of convenient to be able to prod() on it.\n          Tensor filter_dim_a = at::empty(min_dim, at::initialTensorOptions().dtype(kInt));\n",
      "description": "The use of CPU tensor here is a bit goofy in C++,"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/RNN.cpp",
      "line": 701,
      "type": "TODO",
      "content": "// TODO: Set device to input",
      "context": "  fn.dropout.set(fn_train, fn_dropout, fn_dropout_state);\n  fn.tensors.set(input.sizes(), fn_batch_sizes, batch_first);\n\n  // TODO: Set device to input\n\n  if (fn.rnn.mode != CUDNN_LSTM) {\n    TORCH_CHECK(!cx.defined(),\n",
      "description": "Set device to input"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/RNN.cpp",
      "line": 708,
      "type": "TODO",
      "content": "// TODO: can batch_first be a wrapper around this function?",
      "context": "             \"rnn: illegal defined cx for non-LSTM RNN\");\n  }\n\n  // TODO: can batch_first be a wrapper around this function?\n  auto is_input_packed = fn.tensors.batch_sizes.size() != 0;\n  if (batch_first && !is_input_packed) {\n    input = input.transpose(0, 1);\n",
      "description": "can batch_first be a wrapper around this function?"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/RNN.cpp",
      "line": 843,
      "type": "TODO",
      "content": "// TODO: Set device to input",
      "context": "  fn.dropout.set(fn_train, fn_dropout, fn_dropout_state);\n  fn.tensors.set(input.sizes(), fn_batch_sizes, batch_first);\n\n  // TODO: Set device to input\n  auto handle = getCudnnHandle();\n\n  if (fn.rnn.mode != CUDNN_LSTM) {\n",
      "description": "Set device to input"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/RNN.cpp",
      "line": 871,
      "type": "TODO",
      "content": "auto dx = at::empty(input.sizes(), input.options()); // TODO: more compact way of saying this",
      "context": "  auto dy = grad_output.contiguous();\n  auto y = output;\n  auto w = weight_buf;\n  auto dx = at::empty(input.sizes(), input.options()); // TODO: more compact way of saying this\n  auto dhy = grad_hy.contiguous().view(hidden_size);\n  auto dcy = grad_cy.defined() ? grad_cy.contiguous().view(hidden_size) : Tensor();\n  auto dhx = at::empty(hidden_size, hx.options());\n",
      "description": "more compact way of saying this"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/RNN.cpp",
      "line": 915,
      "type": "TODO",
      "content": "// TODO: put this in the correct device???",
      "context": "        x_descs_arr.data(),\n        &workspace_size\n        ));\n  // TODO: put this in the correct device???\n  Tensor workspace = at::empty(workspace_size, input.options().dtype(kByte));\n  setCuDNNStreamToCurrent();\n  AT_CUDNN_CHECK(cudnnRNNBackwardData(\n",
      "description": "put this in the correct device???"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/RNN.cpp",
      "line": 946,
      "type": "TODO",
      "content": "// TODO: I think tensor geometry sufficient for weight_buf/weight",
      "context": "// NB: This MUST BE CALLED AFTER _cudnn_rnn_backward_input.\n// We'll give a user friendly combined function...\nstd::vector<Tensor> _cudnn_rnn_backward_weight(\n    // TODO: I think tensor geometry sufficient for weight_buf/weight\n    const Tensor& input_r, TensorList weight_arr, int64_t weight_stride0,\n    const Tensor& weight_buf, const Tensor& hx, const Tensor& cx,\n    const Tensor& output_r,\n",
      "description": "I think tensor geometry sufficient for weight_buf/weight"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/RNN.cpp",
      "line": 991,
      "type": "TODO",
      "content": "// TODO: the above were the only checks in rnn.py, but it doesn't seem",
      "context": "  TORCH_CHECK(!hx.defined() || hx.sizes().equals(hidden_size),\n           \"Expected hidden size \", IntArrayRef{hidden_size}, \", got \", hx.sizes());\n\n  // TODO: the above were the only checks in rnn.py, but it doesn't seem\n  // like these checks are enough\n\n  TORCH_CHECK(hx.is_contiguous(),\n",
      "description": "the above were the only checks in rnn.py, but it doesn't seem"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/RNN.cpp",
      "line": 1081,
      "type": "TODO",
      "content": "// TODO: I am not sure if we actually need the 'dropout' and 'train' parameters",
      "context": "  return std::tuple<Tensor, Tensor, Tensor, std::vector<Tensor>>{dx, dhx, dcx, dw};\n}\n\n// TODO: I am not sure if we actually need the 'dropout' and 'train' parameters\n// to initialize just the state tensor\nTensor _cudnn_init_dropout_state(double dropout, bool train, int64_t dropout_seed, const TensorOptions& options) {\n  auto handle = getCudnnHandle();\n",
      "description": "I am not sure if we actually need the 'dropout' and 'train' parameters"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/BatchNorm.cpp",
      "line": 76,
      "type": "TODO",
      "content": "// TODO: is weight required to be contiguous?",
      "context": "    checkAllSameType(c, {input, weight});\n  }\n  checkAllSameType(c, {weight, bias, running_mean, running_var});\n  // TODO: is weight required to be contiguous?\n  checkAllContiguous(c, {weight, bias, running_mean, running_var});\n  // TODO: TensorArg check should start handle memory format\n  TORCH_CHECK(input->is_contiguous(input->suggest_memory_format()));\n",
      "description": "is weight required to be contiguous?"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/BatchNorm.cpp",
      "line": 78,
      "type": "TODO",
      "content": "// TODO: TensorArg check should start handle memory format",
      "context": "  checkAllSameType(c, {weight, bias, running_mean, running_var});\n  // TODO: is weight required to be contiguous?\n  checkAllContiguous(c, {weight, bias, running_mean, running_var});\n  // TODO: TensorArg check should start handle memory format\n  TORCH_CHECK(input->is_contiguous(input->suggest_memory_format()));\n\n  checkDimRange(c, input, 2, 6 /* exclusive */);\n",
      "description": "TensorArg check should start handle memory format"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/BatchNorm.cpp",
      "line": 99,
      "type": "TODO",
      "content": "// TODO: The new CUDNN_BATCHNORM_SPATIAL_PERSISTENT mode was",
      "context": "    mode = CUDNN_BATCHNORM_SPATIAL;\n#endif // CUDNN_VERSION >= 7400\n  } else {\n    // TODO: The new CUDNN_BATCHNORM_SPATIAL_PERSISTENT mode was\n    // introduced in CuDNN 7 for performance optimization, but it results in\n    // accuracy losses in convolution models such as ResNeXt-101 and\n    // video R(2+1)D. We will fall back to the normal CUDNN_BATCHNORM_SPATIAL\n",
      "description": "The new CUDNN_BATCHNORM_SPATIAL_PERSISTENT mode was"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/BatchNorm.cpp",
      "line": 227,
      "type": "TODO",
      "content": "// TODO: Is it worth it to have a contiguous call or maybe we should go with",
      "context": "    const Tensor& save_mean_t, const Tensor& save_var_t,\n    double epsilon, const Tensor& reserveSpace)\n{\n  // TODO: Is it worth it to have a contiguous call or maybe we should go with\n  // whatever format is given here.\n  TensorArg input{ input_t, \"input\", 1 },\n            grad_output{ grad_output_t.contiguous(input_t.suggest_memory_format()), \"grad_output\", 2 },\n",
      "description": "Is it worth it to have a contiguous call or maybe we should go with"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/BatchNorm.cpp",
      "line": 247,
      "type": "TODO",
      "content": "// TODO: is weight required to be contiguous?",
      "context": "  }\n  checkAllSameType(c, {input, grad_output});\n  checkAllSameType(c, {weight, save_mean, save_var});\n  // TODO: is weight required to be contiguous?\n  checkAllContiguous(c, {save_mean, save_var});\n  // TODO: TensorArg check should start handle memory format\n  TORCH_CHECK(input->is_contiguous(input->suggest_memory_format()));\n",
      "description": "is weight required to be contiguous?"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/BatchNorm.cpp",
      "line": 249,
      "type": "TODO",
      "content": "// TODO: TensorArg check should start handle memory format",
      "context": "  checkAllSameType(c, {weight, save_mean, save_var});\n  // TODO: is weight required to be contiguous?\n  checkAllContiguous(c, {save_mean, save_var});\n  // TODO: TensorArg check should start handle memory format\n  TORCH_CHECK(input->is_contiguous(input->suggest_memory_format()));\n  TORCH_CHECK(grad_output->is_contiguous(grad_output->suggest_memory_format()));\n  checkDimRange(c, input, 2, 6 /* exclusive */);\n",
      "description": "TensorArg check should start handle memory format"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/BatchNorm.cpp",
      "line": 269,
      "type": "TODO",
      "content": "// TODO: The new CUDNN_BATCHNORM_SPATIAL_PERSISTENT mode was",
      "context": "    mode = CUDNN_BATCHNORM_SPATIAL;\n#endif // CUDNN_VERSION >= 7400\n  } else {\n    // TODO: The new CUDNN_BATCHNORM_SPATIAL_PERSISTENT mode was\n    // introduced in CuDNN 7 for performance optimization, but it results in\n    // accuracy losses in convolution models such as ResNeXt-101 and\n    // video R(2+1)D. We will fall back to the normal CUDNN_BATCHNORM_SPATIAL\n",
      "description": "The new CUDNN_BATCHNORM_SPATIAL_PERSISTENT mode was"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/GridSampler.cpp",
      "line": 34,
      "type": "TODO",
      "content": "// TODO: descriptor checking",
      "context": "\n#include <ATen/TensorUtils.h>\n\n// TODO: descriptor checking\n\n\nnamespace at { namespace native {\n",
      "description": "descriptor checking"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cudnn/GridSampler.cpp",
      "line": 58,
      "type": "TODO",
      "content": "// TODO: Maybe more user friendly to report where the expected size",
      "context": "  // if grid has values >1 or <-1, those values are ignored\n  checkContiguous(c, grid);\n  checkDim(c, grid, 4);\n  // TODO: Maybe more user friendly to report where the expected size\n  // came from\n  checkSize(c, grid, 0, input->size(0));\n  checkSize(c, grid, 3, 2);\n",
      "description": "Maybe more user friendly to report where the expected size"
    },
    {
      "file": "atenspace/aten/src/ATen/native/mkldnn/Normalization.cpp",
      "line": 51,
      "type": "TODO",
      "content": "// TODO: support training",
      "context": "  ideep::tensor y;\n\n  if (train) {\n    // TODO: support training\n    AT_ERROR(\"mkldnn_batch_norm: mkldnn training is not supported in yet.\");\n\n    // ideep::tensor saved_mean;\n",
      "description": "support training"
    },
    {
      "file": "atenspace/aten/src/ATen/native/mkldnn/TensorFactories.cpp",
      "line": 12,
      "type": "TODO",
      "content": "// TODO: support int64_t dims in ideep::tensor to avoid extra conversion",
      "context": "     !optional_memory_format.has_value(),\n     \"'memory_format' argument is incompatible with mkldnn tensor\");\n  // NOTE: int32_t dims from ideep::tensor but sizes needs int64_t\n  // TODO: support int64_t dims in ideep::tensor to avoid extra conversion\n  ideep::tensor::dims dst_dims (sizes.begin(), sizes.end());\n  ideep::tensor it;\n  it.resize<AllocForMKLDNN>(dst_dims, ideep::tensor::data_type::f32);\n",
      "description": "support int64_t dims in ideep::tensor to avoid extra conversion"
    },
    {
      "file": "atenspace/aten/src/ATen/native/mkldnn/MKLDNNCommon.cpp",
      "line": 45,
      "type": "TODO",
      "content": "// TODO: support int64_t dims in ideep::tensor to avoid extra conversion",
      "context": "\nTensor new_with_itensor_mkldnn(ideep::tensor&& it, const TensorOptions& options) {\n  // NOTE: int32_t dims from ideep::tensor but sizes needs int64_t\n  // TODO: support int64_t dims in ideep::tensor to avoid extra conversion\n  auto dims = it.get_dims();\n  IDeepTensorWrapperPtr handle = c10::make_intrusive<IDeepTensorWrapper>(std::move(it));\n  return detail::make_tensor<MKLDNNTensorImpl>(\n",
      "description": "support int64_t dims in ideep::tensor to avoid extra conversion"
    },
    {
      "file": "atenspace/aten/src/ATen/native/mkldnn/MKLDNNConversions.cpp",
      "line": 31,
      "type": "TODO",
      "content": "// TODO: consider to convert non-contiguous tensor to `ideep::tensor` directly.",
      "context": "             \"dense_to_mkldnn expects float tensor input\");\n  AT_ASSERTM(cpu_tensor.dim() <= 5,\n             \"Can't convert cpu tensor with the number of dimensions > 5\");\n  // TODO: consider to convert non-contiguous tensor to `ideep::tensor` directly.\n  auto cpu_tensor_cont = cpu_tensor.contiguous();\n  Tensor mkldnn_tensor = empty_mkldnn(cpu_tensor_cont.sizes(), cpu_tensor_cont.options());\n  ideep::tensor& dtensor = itensor_from_mkldnn(mkldnn_tensor);\n",
      "description": "consider to convert non-contiguous tensor to `ideep::tensor` directly."
    },
    {
      "file": "atenspace/aten/src/ATen/native/miopen/Conv_miopen.cpp",
      "line": 5,
      "type": "TODO",
      "content": "// TODO: Remove the condition on AT_ROCM_ENABLED entirely,",
      "context": "#include <ATen/NativeFunctions.h>\n#include <ATen/Config.h>\n\n// TODO: Remove the condition on AT_ROCM_ENABLED entirely,\n// don't build this file as part of CPU build.\n#include <ATen/cuda/CUDAConfig.h>\n\n",
      "description": "Remove the condition on AT_ROCM_ENABLED entirely,"
    },
    {
      "file": "atenspace/aten/src/ATen/native/miopen/BatchNorm_miopen.cpp",
      "line": 5,
      "type": "TODO",
      "content": "// TODO: Remove the condition on AT_ROCM_ENABLED entirely,",
      "context": "#include <ATen/NativeFunctions.h>\n#include <ATen/Config.h>\n\n// TODO: Remove the condition on AT_ROCM_ENABLED entirely,\n// don't build this file as part of CPU build.\n#include <ATen/cuda/CUDAConfig.h>\n\n",
      "description": "Remove the condition on AT_ROCM_ENABLED entirely,"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cuda/CuFFTPlanCache.h",
      "line": 145,
      "type": "TODO",
      "content": "// TODO: Figure out why windows fails to compile",
      "context": "    // Checks if input strides can be viewed as embedded.\n    // See NOTE [ cuFFT Embedded Strides ].\n    //\n    // TODO: Figure out why windows fails to compile\n    //         c10::optional<std::vector<long long int>> inembed_opt =\n    //         c10::nullopt;\n    //       Then move the following to a helper function.\n",
      "description": "Figure out why windows fails to compile"
    },
    {
      "file": "atenspace/aten/src/ATen/native/sparse/SparseTensor.cpp",
      "line": 302,
      "type": "TODO",
      "content": "// TODO: it seems like sparse_dim == 0 could be supported even if self.dim() > 0,",
      "context": "\nSparseTensor dense_to_sparse(const Tensor& self, int64_t sparse_dim){\n  int64_t dims = self.dim();\n  // TODO: it seems like sparse_dim == 0 could be supported even if self.dim() > 0,\n  // but this would take some work and doesn't seem particularly useful.\n  TORCH_CHECK(sparse_dim > 0 || self.dim() == 0, \"sparse_dim must be >0 if dimensionality > 0\");\n  TORCH_CHECK(sparse_dim <= dims,\n",
      "description": "it seems like sparse_dim == 0 could be supported even if self.dim() > 0,"
    },
    {
      "file": "atenspace/aten/src/ATen/native/sparse/SparseTensor.cpp",
      "line": 381,
      "type": "TODO",
      "content": "// TODO: is there a more idiomatic way to do this?",
      "context": "\n  SparseTensor dst = new_sparse(self.options());\n  get_sparse_impl(dst)->resize_(sparse_dim, dense_dim, self.sizes());\n  // TODO: is there a more idiomatic way to do this?\n  LongTensor newIndices = at::empty(indices.sizes(), indices.options());\n  Tensor newValues = at::empty(values.sizes(), values.options());\n  alias_into_sparse(dst, newIndices, newValues);\n",
      "description": "is there a more idiomatic way to do this?"
    },
    {
      "file": "atenspace/aten/src/ATen/native/sparse/SparseTensor.cpp",
      "line": 501,
      "type": "TODO",
      "content": "// TODO: Re-audit this; it used to be an indexSelect directly into r_values",
      "context": "    }\n\n    Tensor t_view = t.view(view_size);\n    // TODO: Re-audit this; it used to be an indexSelect directly into r_values\n    at::index_select_out(r_values, t_view, 0, indices);\n  } else {\n    AT_DISPATCH_ALL_TYPES(r_values.scalar_type(), \"sparse_mask\", [&] {\n",
      "description": "Re-audit this; it used to be an indexSelect directly into r_values"
    },
    {
      "file": "atenspace/aten/src/ATen/native/sparse/SparseTensorMath.cpp",
      "line": 25,
      "type": "TODO",
      "content": "// TODO: eliminate this conditional when zero-size dims supported correctly",
      "context": "  LongTensor _to_csr(const int64_t* indices, int64_t dim, int64_t nnz) {\n    LongTensor csr = native::zeros({dim + 1}, kLong);\n\n    // TODO: eliminate this conditional when zero-size dims supported correctly\n    if (nnz > 0) {\n      auto csr_accessor = csr.accessor<int64_t, 1>();\n      // Convert the sparse matrix to CSR format\n",
      "description": "eliminate this conditional when zero-size dims supported correctly"
    },
    {
      "file": "atenspace/aten/src/ATen/native/sparse/SparseTensorMath.cpp",
      "line": 96,
      "type": "TODO",
      "content": "// TODO: add in-place variant",
      "context": "// log1p(SparseTensor)\n// --------------------------------------------------------------------\n\n// TODO: add in-place variant\n\nSparseTensor& log1p_out_sparse(SparseTensor& r, const SparseTensor& t) {\n  AT_ASSERT(r.is_sparse());\n",
      "description": "add in-place variant"
    },
    {
      "file": "atenspace/aten/src/ATen/native/sparse/SparseTensorMath.cpp",
      "line": 123,
      "type": "TODO",
      "content": "// TODO: add in-place variant",
      "context": "// pow(SparseTensor, Scalar)\n// --------------------------------------------------------------------\n\n// TODO: add in-place variant\n\nSparseTensor& pow_out_sparse_scalar(SparseTensor& r, const SparseTensor& t_, Scalar value) {\n  AT_ASSERT(r.is_sparse());\n",
      "description": "add in-place variant"
    },
    {
      "file": "atenspace/aten/src/ATen/native/sparse/SparseTensorMath.cpp",
      "line": 206,
      "type": "TODO",
      "content": "// TODO: Why?! Can't we just flip the order here...",
      "context": "// --------------------------------------------------------------------\n\nTensor add_sparse(const Tensor& self, const Tensor& other, Scalar alpha) {\n  // TODO: Why?! Can't we just flip the order here...\n  TORCH_CHECK(!(self.is_sparse() && !other.is_sparse()),\n              \"add(sparse, dense) is not supported. Use add(dense, sparse) instead.\");\n  Tensor result = at::empty({0}, self.options());\n",
      "description": "Why?! Can't we just flip the order here..."
    },
    {
      "file": "atenspace/aten/src/ATen/native/sparse/SparseTensorMath.cpp",
      "line": 237,
      "type": "TODO",
      "content": "// TODO: This test seems a bit goofy",
      "context": "  if (!t.is_sparse()) {\n    return add_out_dense_sparse_cpu(r, t, src, value);\n  }\n  // TODO: This test seems a bit goofy\n  TORCH_CHECK(src.is_sparse(), \"add(sparse, dense) is not supported. Use add(dense, sparse) instead.\");\n  AT_ASSERT(!t.is_cuda());  // the dispatch argument\n  TORCH_CHECK(!r.is_cuda(), \"add: expected 'out' to be CPU tensor, but got CUDA tensor\");\n",
      "description": "This test seems a bit goofy"
    },
    {
      "file": "atenspace/aten/src/ATen/native/sparse/SparseTensorMath.cpp",
      "line": 330,
      "type": "TODO",
      "content": "// TODO: I think it may be possible to track inside the loop and",
      "context": "    );\n\n    get_sparse_impl(r)->set_nnz_and_narrow(r_i);\n    // TODO: I think it may be possible to track inside the loop and\n    // detect when we are uncoalesced (e.g., by observing that an\n    // index goes backwards) which may be more precise than using the\n    // coalesced flag here.  But this is easy.\n",
      "description": "I think it may be possible to track inside the loop and"
    },
    {
      "file": "atenspace/aten/src/ATen/native/sparse/SparseTensorMath.cpp",
      "line": 590,
      "type": "TODO",
      "content": "// TODO: This error message seems awfully opaque",
      "context": "    Scalar beta,\n    Scalar alpha\n) {\n  // TODO: This error message seems awfully opaque\n  AT_ASSERT(!t.is_cuda());\n  TORCH_CHECK(!r.is_cuda(), \"addmm: expected 'out' to be CPU tensor, but got CUDA tensor\");\n  TORCH_CHECK(!sparse_.is_cuda(), \"addmm: expected 'mat1' to be a CPU tensor, but got a CUDA tensor\");\n",
      "description": "This error message seems awfully opaque"
    },
    {
      "file": "atenspace/aten/src/ATen/native/sparse/SparseTensorMath.cpp",
      "line": 721,
      "type": "TODO",
      "content": "// TODO: Make this a real argument",
      "context": "// --------------------------------------------------------------------\n\nSparseTensor& hspmm_out_sparse_cpu(SparseTensor& r, const SparseTensor& sparse_, const Tensor& dense) {\n  // TODO: Make this a real argument\n  Scalar alpha = 1;\n\n  AT_ASSERT(!sparse_.is_cuda()); // dispatch argument\n",
      "description": "Make this a real argument"
    },
    {
      "file": "atenspace/aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cpp",
      "line": 50,
      "type": "TODO",
      "content": "// TODO: Re-audit this; it used to be an indexSelect directly into r_values",
      "context": "  }\n\n  Tensor t_view = t.view(view_size);\n  // TODO: Re-audit this; it used to be an indexSelect directly into r_values\n  at::index_select_out(r_values, t_view, 0, indices);\n\n  return r;\n",
      "description": "Re-audit this; it used to be an indexSelect directly into r_values"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp",
      "line": 41,
      "type": "TODO",
      "content": "// TODO This is an inefficient way to compite sign, and can be much faster",
      "context": "  // implementation of the general backward pass when p is less than two, so\n  // there's a struct with only a backward pass for this case.\n\n  // TODO This is an inefficient way to compite sign, and can be much faster\n  // using native SSE instructions that should be added to Vec256.\n  static inline Vec sign(Vec val) {\n    return vec256::minimum(vec256::maximum(Vec(0), val.ceil()), Vec(1)) +\n",
      "description": "This is an inefficient way to compite sign, and can be much faster"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp",
      "line": 113,
      "type": "TODO",
      "content": "// TODO This can probably use fused add multiply to get better perf",
      "context": "  // Two norm\n  template<typename data_t>\n  struct tdist_calc {\n    // TODO This can probably use fused add multiply to get better perf\n    static inline data_t map(const data_t& diff, const data_t& p) { return diff * diff; }\n    static inline data_t red(const data_t& agg, const data_t& up) { return agg + up; }\n    static inline scalar_t finish(const scalar_t agg, const scalar_t p) { return std::sqrt(agg); }\n",
      "description": "This can probably use fused add multiply to get better perf"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp",
      "line": 135,
      "type": "TODO",
      "content": "// TODO This backward pass uses a very complext expression to compute (diff",
      "context": "    static inline data_t map(const data_t& diff, const data_t& p) { return diff; }\n    static inline data_t red(const data_t& agg, const data_t& up) { return max(agg, up); }\n    static inline scalar_t finish(const scalar_t agg, const scalar_t p) { return agg; }\n    // TODO This backward pass uses a very complext expression to compute (diff\n    // == dist) that could be much faster if using SSE instructions.\n    static inline Vec backward(const Vec& diff, const scalar_t grad, const scalar_t dist, const Vec& p) { return Vec(grad) * sign(diff) * (Vec(1) - vec256::minimum(Vec(1), (diff.abs() - Vec(dist)).abs().ceil())); }\n  };\n",
      "description": "This backward pass uses a very complext expression to compute (diff"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cpu/IndexKernel.cpp",
      "line": 107,
      "type": "TODO",
      "content": "// TODO: investigate parallelization of the accumulate kernel. Unlike the non-accumulate case,",
      "context": "  // NOTE: duplicate indices are only supported if accumulate is true.\n  AT_DISPATCH_ALL_TYPES_AND2(at::ScalarType::Half, at::ScalarType::Bool, iter.dtype(), \"index_put\", [&] {\n    if (accumulate) {\n      // TODO: investigate parallelization of the accumulate kernel. Unlike the non-accumulate case,\n      // this needs to be thread-safe.\n      cpu_index_kernel<scalar_t>(iter, index_size, index_stride, [](char* dst, char* src, int64_t offset) {\n        *(scalar_t*)(dst + offset) += *(scalar_t*)src;\n",
      "description": "investigate parallelization of the accumulate kernel. Unlike the non-accumulate case,"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cpu/Activation.cpp",
      "line": 138,
      "type": "TODO",
      "content": "// TODO(yangxm): Add another fast kernel using formula",
      "context": "\n#endif // AT_MKL_ENABLED()\n\n// TODO(yangxm): Add another fast kernel using formula\n// y = 0.5x * (1 + tanh(sqrt(2/Pi) * (x + 0.044715x^3)))\n// and the fast tanh impl from Eigen.\nvoid GeluKernelImpl(TensorIterator& it) {\n",
      "description": "(yangxm): Add another fast kernel using formula"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp",
      "line": 325,
      "type": "TODO",
      "content": "// TODO: Disable cont. branch to test more risky code",
      "context": "  });\n}\n\n// TODO: Disable cont. branch to test more risky code\n\n#define IMPLEMENT_FLOAT_KERNEL(dispatchtypes, op)                             \\\n  static void op##_kernel(TensorIterator& iter) {                             \\\n",
      "description": "Disable cont. branch to test more risky code"
    },
    {
      "file": "atenspace/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp",
      "line": 67,
      "type": "TODO",
      "content": "// TODO: if the divisor is a scalar, rewrite as multiplication by a constant.",
      "context": "void div_kernel(TensorIterator& iter) {\n  if (isIntegralType(iter.dtype(), /*includeBool*/ false)) {\n    // There's no SIMD integer division, so don't try to vectorize it.\n    // TODO: if the divisor is a scalar, rewrite as multiplication by a constant.\n    AT_DISPATCH_INTEGRAL_TYPES(iter.dtype(), \"div_cpu\", [&]() {\n      cpu_kernel(iter, [](scalar_t a, scalar_t b) -> scalar_t {\n        return a / b;\n",
      "description": "if the divisor is a scalar, rewrite as multiplication by a constant."
    },
    {
      "file": "atenspace/aten/src/ATen/native/cpu/DepthwiseConvKernel.cpp",
      "line": 175,
      "type": "TODO",
      "content": "// TODO: avoid spilling W by breaking out the non-padded vs padded case.",
      "context": "  // Non-padded regime.\n\n  // Iterate over non-padded output tiles.\n  // TODO: avoid spilling W by breaking out the non-padded vs padded case.\n  for (int64_t oth = 0; oth < (args.out_rows + 1) / 2; ++oth) {\n    for (int64_t otw = 0; otw < (args.out_cols + 1) / 2; ++otw) {\n      // load input tile for [oth, otw];\n",
      "description": "avoid spilling W by breaking out the non-padded vs padded case."
    },
    {
      "file": "atenspace/aten/src/ATen/native/quantized/TensorCompare.cpp",
      "line": 21,
      "type": "TODO",
      "content": "// TODO: move to TensorMath.cpp",
      "context": "  return std::get<0>(self.reshape({-1}).min(/*dim=*/0));\n}\n\n// TODO: move to TensorMath.cpp\n\nstd::tuple<Tensor, Tensor> sort_quant(\n    const Tensor& self,\n",
      "description": "move to TensorMath.cpp"
    },
    {
      "file": "atenspace/aten/src/ATen/native/quantized/QTensor.cpp",
      "line": 159,
      "type": "TODO",
      "content": "// TODO: add per channel support",
      "context": "}\n\nTensor quantized_clone(const Tensor& self, c10::optional<c10::MemoryFormat> optional_memory_format) {\n  // TODO: add per channel support\n  TORCH_INTERNAL_ASSERT(\n      self.qscheme() == at::kPerTensorAffine,\n      \"clone for quantized Tensor only works for PerTensorAffine scheme right now\");\n",
      "description": "add per channel support"
    },
    {
      "file": "atenspace/aten/src/ATen/native/quantized/QTensor.cpp",
      "line": 167,
      "type": "TODO",
      "content": "// TODO: To support all features of MemoryFormat::Preserve we need to add",
      "context": "  auto memory_format =\n      optional_memory_format.value_or(MemoryFormat::Contiguous);\n\n  // TODO: To support all features of MemoryFormat::Preserve we need to add\n  // _empty_affine_quantized_strided function and use it similarly to\n  // Tensor clone(const Tensor& src, c10::optional<c10::MemoryFormat> optional_memory_format)\n  // if (self.is_non_overlapping_and_dense()) -> _empty_affine_quantized_strided\n",
      "description": "To support all features of MemoryFormat::Preserve we need to add"
    },
    {
      "file": "atenspace/aten/src/ATen/native/quantized/cpu/qpool.cpp",
      "line": 305,
      "type": "TODO",
      "content": "// TODO: change it to contiguous(MemoryFormat::ChannelsLast) once a perf",
      "context": "     int64_t inC = input.size(1);\n     int64_t inH = input.size(2);\n     int64_t inW = input.size(3);\n     // TODO: change it to contiguous(MemoryFormat::ChannelsLast) once a perf\n     // regression of it is fixed.\n     Tensor input_contig = input.permute({0, 2, 3, 1}).contiguous();\n\n",
      "description": "change it to contiguous(MemoryFormat::ChannelsLast) once a perf"
    },
    {
      "file": "atenspace/aten/src/ATen/native/quantized/cpu/qpool.cpp",
      "line": 392,
      "type": "TODO",
      "content": "//TODO: remove permute once MemoryLayout is added above",
      "context": "     TORCH_INTERNAL_ASSERT(\n         runStatus == pytorch_qnnp_status_success,\n         \"failed to run QNNPACK MaxPool operator\");\n     //TODO: remove permute once MemoryLayout is added above\n     return qy.permute({0, 3, 1, 2});\n   }\n   #endif\n",
      "description": "remove permute once MemoryLayout is added above"
    },
    {
      "file": "atenspace/aten/src/ATen/native/quantized/cpu/qlinear_prepack.cpp",
      "line": 63,
      "type": "TODO",
      "content": "// TODO: contiguous is called for further JIT optimizations.",
      "context": "    auto N = weight.size(0);\n    auto K = weight.size(1);\n\n    // TODO: contiguous is called for further JIT optimizations.\n    auto weight_contig = weight.contiguous();\n    const auto qtype = weight.qscheme();\n    std::vector<int32_t> weight_zero_points_int32(1, 0);\n",
      "description": "contiguous is called for further JIT optimizations."
    },
    {
      "file": "atenspace/aten/src/ATen/native/quantized/cpu/qlinear_prepack.cpp",
      "line": 122,
      "type": "TODO",
      "content": "// TODO: we will need to replace this with torchscript classes at a later",
      "context": "        weight_zero_points_int32,\n        qtype});\n\n    // TODO: we will need to replace this with torchscript classes at a later\n    // point.\n    return cpp_custom_type_hack::create(std::move(ret_ptr), weight.options());\n  }\n",
      "description": "we will need to replace this with torchscript classes at a later"
    },
    {
      "file": "atenspace/aten/src/ATen/native/quantized/cpu/q_adaavgpool.cpp",
      "line": 82,
      "type": "TODO",
      "content": "// TODO: add the max/min clip",
      "context": "          }\n\n          /* set output to local average */\n          // TODO: add the max/min clip\n          op->val_ = static_cast<typename scalar_t::underlying>(\n              std::nearbyint(sum * kHWr));\n        }\n",
      "description": "add the max/min clip"
    },
    {
      "file": "atenspace/aten/src/ATen/native/quantized/cpu/fake_quantize_per_channel_affine.cpp",
      "line": 32,
      "type": "TODO",
      "content": "// TODO: Use REGISTER_DISPATCH",
      "context": "    int64_t axis,\n    int64_t quant_min,\n    int64_t quant_max) {\n  // TODO: Use REGISTER_DISPATCH\n  TORCH_CHECK(self.scalar_type() == ScalarType::Float);\n  TORCH_CHECK(\n      scale.dim() == 1, \"scale should be a 1-D tensor\");\n",
      "description": "Use REGISTER_DISPATCH"
    },
    {
      "file": "atenspace/aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp",
      "line": 141,
      "type": "TODO",
      "content": "// TODO: Change this when ChannelsLast3d is ready.",
      "context": "    }\n\n    // FBGEMM expects weights to be in channels last\n    // TODO: Change this when ChannelsLast3d is ready.\n    const Tensor weight_nhwc = kSpatialDim == 2\n        ? weight.contiguous(MemoryFormat::ChannelsLast)\n        : fbgemm_utils::ConvertToChannelsLast3dTensor(weight);\n",
      "description": "Change this when ChannelsLast3d is ready."
    },
    {
      "file": "atenspace/aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp",
      "line": 203,
      "type": "TODO",
      "content": "// TODO: we will need to replace this with torchscript classes at a later",
      "context": "            zero_points,\n            qtype});\n\n    // TODO: we will need to replace this with torchscript classes at a later\n    // point.\n    return cpp_custom_type_hack::create(std::move(ret_ptr), weight.options());\n  }\n",
      "description": "we will need to replace this with torchscript classes at a later"
    },
    {
      "file": "atenspace/aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp",
      "line": 28,
      "type": "TODO",
      "content": "// TODO: contiguous is called for further jit optimizations.",
      "context": "    TORCH_CHECK(\n        fbgemm::fbgemmSupportedCPU(), \"Your CPU does not support FBGEMM.\");\n\n    // TODO: contiguous is called for further jit optimizations.\n    auto input_contig = input.contiguous();\n    const auto* input_ptr = input_contig.data_ptr<float>();\n\n",
      "description": "contiguous is called for further jit optimizations."
    },
    {
      "file": "atenspace/aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp",
      "line": 91,
      "type": "TODO",
      "content": "// TODO: contiguous is called for further jit optimizations.",
      "context": "      TORCH_CHECK(\n          bias_vec.size(0) == N,\n          \"bias should have N elements: \" + std::to_string(N));\n      // TODO: contiguous is called for further jit optimizations.\n      auto bias_contig = bias_vec.contiguous();\n      bias_ptr = bias_contig.data_ptr<float>();\n    }\n",
      "description": "contiguous is called for further jit optimizations."
    },
    {
      "file": "atenspace/aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp",
      "line": 130,
      "type": "TODO",
      "content": "// TODO: Consider a way to pre-allocate and reuse",
      "context": "          /*pmat=*/nullptr, // Currently, packA manages ownership of `pmat`.\n          /*scale=*/q_params.scale,\n          /*zero_pt=*/q_params.zero_point);\n      // TODO: Consider a way to pre-allocate and reuse\n      // pmat buffer.\n\n      // This is the end of the pipeline, pass the resulting matrix through.\n",
      "description": "Consider a way to pre-allocate and reuse"
    },
    {
      "file": "atenspace/aten/src/ATen/native/quantized/cpu/qconv.cpp",
      "line": 523,
      "type": "TODO",
      "content": "// TODO Can be replaced with packB->getOutputChannels() when update pre-pack",
      "context": "\n    const uint32_t kernel_h = kernel[0];\n    const uint32_t kernel_w = kernel[1];\n    // TODO Can be replaced with packB->getOutputChannels() when update pre-pack\n    // to actually do the packing.\n    const auto out_ch = pack_data.bias.size(0);\n    // inputs are in semantic NCHW format\n",
      "description": "Can be replaced with packB->getOutputChannels() when update pre-pack"
    },
    {
      "file": "atenspace/aten/src/ATen/native/quantized/cpu/qlinear.cpp",
      "line": 33,
      "type": "TODO",
      "content": "// TODO: contiguous is called for further jit optimizations.",
      "context": "    TORCH_CHECK(\n        fbgemm::fbgemmSupportedCPU(), \"Your CPU does not support FBGEMM.\");\n\n    // TODO: contiguous is called for further jit optimizations.\n    auto input_contig = input.contiguous();\n    const auto* input_ptr =\n        reinterpret_cast<uint8_t*>(input_contig.data_ptr<c10::quint8>());\n",
      "description": "contiguous is called for further jit optimizations."
    },
    {
      "file": "atenspace/aten/src/ATen/native/quantized/cpu/qlinear.cpp",
      "line": 131,
      "type": "TODO",
      "content": "// TODO: Consider a way to pre-allocate and reuse",
      "context": "            /*smat=*/input_ptr,\n            /*ld=*/K,\n            /*pmat=*/nullptr); // Currently, packA manages ownership of `pmat`.\n                               // TODO: Consider a way to pre-allocate and reuse\n                               // pmat buffer.\n\n        // ReQuantizeOutput requires pointers to the zero point values,\n",
      "description": "Consider a way to pre-allocate and reuse"
    },
    {
      "file": "atenspace/aten/src/ATen/native/quantized/cpu/q_avgpool.cpp",
      "line": 379,
      "type": "TODO",
      "content": "// TODO: remove permute once MemoryLayout is added above",
      "context": "  TORCH_INTERNAL_ASSERT(\n      runStatus == pytorch_qnnp_status_success,\n      \"failed to run QNNPACK Average Pool operator\");\n  // TODO: remove permute once MemoryLayout is added above\n  return output.permute({0, 3, 1, 2});\n}\n#endif\n",
      "description": "remove permute once MemoryLayout is added above"
    },
    {
      "file": "atenspace/aten/src/ATen/native/quantized/cpu/qconv_unpack.cpp",
      "line": 76,
      "type": "TODO",
      "content": "// TODO: Unify 2d and 3d when ChannelsLast3d is ready.",
      "context": "    // Unpacked format would be physical KRS(C/G) but logical KCRS (channels\n    // first) because that's how\n    // ChannelsLast3d is not available now.FBGEMM stores the weights\n    // TODO: Unify 2d and 3d when ChannelsLast3d is ready.\n    Tensor unpacked_weights;\n    if (pack_ptr.q_scheme == kPerTensorAffine) {\n      unpacked_weights = kSpatialDim == 2\n",
      "description": "Unify 2d and 3d when ChannelsLast3d is ready."
    },
    {
      "file": "atenspace/aten/src/ATen/native/quantized/cpu/fbgemm_utils.h",
      "line": 76,
      "type": "TODO",
      "content": "// TODO: Remove functions below when ChannelsLast3d is ready.",
      "context": "    const std::vector<int>& pads,\n    const std::vector<int>& dilations);\n\n// TODO: Remove functions below when ChannelsLast3d is ready.\nTensor MakeStridedQTensorCPU(\n    const IntArrayRef& sizes,\n    const IntArrayRef& strides,\n",
      "description": "Remove functions below when ChannelsLast3d is ready."
    },
    {
      "file": "atenspace/aten/src/ATen/native/quantized/cpu/qreduction.cpp",
      "line": 29,
      "type": "TODO",
      "content": "// TODO: change it to contiguous(MemoryFormat::ChannelsLast) once a perf",
      "context": "  const int64_t inH = input.size(2);\n  const int64_t inW = input.size(3);\n\n  // TODO: change it to contiguous(MemoryFormat::ChannelsLast) once a perf\n  // regression of it is fixed. Today it's equivalent because `input` sizes\n  // are not used below\n  Tensor input_contig = input.permute({0, 2, 3, 1}).contiguous();\n",
      "description": "change it to contiguous(MemoryFormat::ChannelsLast) once a perf"
    },
    {
      "file": "atenspace/aten/src/ATen/native/quantized/cpu/qnnpack/src/conv-run.cc",
      "line": 350,
      "type": "FIXME",
      "content": "// FIXME temporary solution to create a qnnp_op struct for indirection buffer.",
      "context": "      stride_width);\n  const size_t output_size = output_height * output_width;\n\n  // FIXME temporary solution to create a qnnp_op struct for indirection buffer.\n  const bool any_padding =\n      (conv_p.pad[0] | conv_p.pad[1] | conv_p.pad[2] | conv_p.pad[3]) != 0;\n  size_t zero_size = 0, zero_offset = 0;\n",
      "description": "temporary solution to create a qnnp_op struct for indirection buffer."
    },
    {
      "file": "atenspace/aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp",
      "line": 209,
      "type": "TODO",
      "content": "// TODO: we can optimize dequantization by doing a premultiplication",
      "context": "\n  // Broadcast out the parameters here to amortize out that cost across\n  // loop iterations.\n  // TODO: we can optimize dequantization by doing a premultiplication\n  // of the zero point by scale and doing FMA on scale*x_q - (scale*zero_point)\n  auto self_zero_point_vec = Vec256<float>((float)self_zero_point);\n  auto self_scale_vec = Vec256<float>(self_scale);\n",
      "description": "we can optimize dequantization by doing a premultiplication"
    },
    {
      "file": "atenspace/aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp",
      "line": 247,
      "type": "TODO",
      "content": "// TODO: fbgemm::Quantize doesn't support taking in the",
      "context": "            }\n            retvals[i] = c;\n          }\n          // TODO: fbgemm::Quantize doesn't support taking in the\n          // pre-broadcasted parameters. We might be able to save some cycles by\n          // enabling that in the API.\n          // TODO: specialize fbgemm::Quantize for a single vector and make it\n",
      "description": "fbgemm::Quantize doesn't support taking in the"
    },
    {
      "file": "atenspace/aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp",
      "line": 250,
      "type": "TODO",
      "content": "// TODO: specialize fbgemm::Quantize for a single vector and make it",
      "context": "          // TODO: fbgemm::Quantize doesn't support taking in the\n          // pre-broadcasted parameters. We might be able to save some cycles by\n          // enabling that in the API.\n          // TODO: specialize fbgemm::Quantize for a single vector and make it\n          // inlineable. This could help with interleaving as suggested by the\n          // TensorIterator implementations\n          auto rv = Vec::quantize(retvals, scale, zero_point, inv_scale);\n",
      "description": "specialize fbgemm::Quantize for a single vector and make it"
    },
    {
      "file": "atenspace/aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp",
      "line": 444,
      "type": "TODO",
      "content": "// TODO: support 16bit, 32bit, and etc.",
      "context": "        int64_t c = 0;\n        // For int8 or uint8quantization, we implicitly use int32 as\n        // accumulation Or else, it will go to the slow path\n        // TODO: support 16bit, 32bit, and etc.\n        auto* internal_i_p = i_p + istartH * istrideH + istartW * istrideW;\n\n        // TODO: more vectorization with loop interleaving\n",
      "description": "support 16bit, 32bit, and etc."
    },
    {
      "file": "atenspace/aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp",
      "line": 447,
      "type": "TODO",
      "content": "// TODO: more vectorization with loop interleaving",
      "context": "        // TODO: support 16bit, 32bit, and etc.\n        auto* internal_i_p = i_p + istartH * istrideH + istartW * istrideW;\n\n        // TODO: more vectorization with loop interleaving\n        do_avg_pool_on_AVX2<scalar_t>(\n            internal_i_p,\n            o_p,\n",
      "description": "more vectorization with loop interleaving"
    },
    {
      "file": "atenspace/aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp",
      "line": 542,
      "type": "TODO",
      "content": "// TODO: support 16bit, 32bit, and etc.",
      "context": "        int64_t c = 0;\n        // For int8 quantization, we implicitly use int32 as accumulation\n        // Or else, it will go to the slow path\n        // TODO: support 16bit, 32bit, and etc.\n        float multiplier = qx.q_scale() / qy.q_scale() / divide_factor;\n        do_avg_pool_on_AVX2<scalar_t>(\n            i_p,\n",
      "description": "support 16bit, 32bit, and etc."
    },
    {
      "file": "atenspace/aten/src/ATen/templates/OpsAlreadyMovedToC10.cpp",
      "line": 12,
      "type": "TODO",
      "content": "// TODO Once all ATen ops are moved to c10, this file should be removed",
      "context": "\n// ${generated_comment}\n\n// TODO Once all ATen ops are moved to c10, this file should be removed\n\nnamespace at {\n\n",
      "description": "Once all ATen ops are moved to c10, this file should be removed"
    },
    {
      "file": "atenspace/aten/src/ATen/templates/TensorMethods.h",
      "line": 34,
      "type": "TODO",
      "content": "// TODO: The Python version also accepts arguments",
      "context": "  return to(options().device(DeviceType::CPU), /*non_blocking*/ false, /*copy*/ false);\n}\n\n// TODO: The Python version also accepts arguments\ninline Tensor Tensor::cuda() const {\n  return to(options().device(DeviceType::CUDA), /*non_blocking*/ false, /*copy*/ false);\n}\n",
      "description": "The Python version also accepts arguments"
    },
    {
      "file": "atenspace/aten/src/ATen/templates/TensorMethods.h",
      "line": 47,
      "type": "TODO",
      "content": "// TODO: Deprecate me",
      "context": "  return to(options().dtype(t), /*non_blocking*/ false, /*copy*/ false);\n}\n\n// TODO: Deprecate me\ninline Tensor Tensor::toBackend(Backend b) const {\n  return to(options().device(backendToDeviceType(b)).layout(layout_from_backend(b)), /*non_blocking*/ false, /*copy*/ false);\n}\n",
      "description": "Deprecate me"
    },
    {
      "file": "atenspace/aten/src/ATen/templates/TensorBody.h",
      "line": 291,
      "type": "TODO",
      "content": "/// TODO: it's not in native_functions.yaml yet as it's not exposed to python",
      "context": "  bool is_quantized() const;\n\n  /// If a tensor is a quantized tensor, returns its quantizer\n  /// TODO: it's not in native_functions.yaml yet as it's not exposed to python\n  QuantizerPtr quantizer() const;\n\n#ifdef BUILD_NAMEDTENSOR\n",
      "description": "it's not in native_functions.yaml yet as it's not exposed to python"
    },
    {
      "file": "atenspace/aten/src/ATen/templates/TensorBody.h",
      "line": 419,
      "type": "TODO",
      "content": "// TODO: remove following two after at::kDouble and its friends are TypeMeta's.",
      "context": "  // at::kDouble and its friends to be TypeMeta's, but that hasn't happened yet.\n  // Before that change, we make this method to maintain BC for C++ usage like\n  // `x.to(y.dtype)`.\n  // TODO: remove following two after at::kDouble and its friends are TypeMeta's.\n  inline Tensor to(caffe2::TypeMeta type_meta, bool non_blocking=false, bool copy=false) const {\n    return this->to(/*scalar_type=*/typeMetaToScalarType(type_meta), non_blocking, copy);\n  }\n",
      "description": "remove following two after at::kDouble and its friends are TypeMeta's."
    },
    {
      "file": "atenspace/aten/src/ATen/quantized/Quantizer.h",
      "line": 14,
      "type": "TODO",
      "content": "// TODO: move to c10 namespace after we",
      "context": "#include <cmath>\n#include <memory>\n\n// TODO: move to c10 namespace after we\n// unified caffe2::Tensor and at::Tensor\n\nnamespace at {\n",
      "description": "move to c10 namespace after we"
    },
    {
      "file": "atenspace/aten/src/ATen/quantized/Quantizer.cpp",
      "line": 203,
      "type": "TODO",
      "content": "// TODO combine this with quantize_val once the numerics for ARM are aligned with it",
      "context": "  }\n}\n\n// TODO combine this with quantize_val once the numerics for ARM are aligned with it\ninline uint8_t quantize_val_arm(const float scale, const int32_t zero_point, const float value) {\n  const int32_t qmin = std::numeric_limits<uint8_t>::min();\n  const int32_t qmax = std::numeric_limits<uint8_t>::max();\n",
      "description": "combine this with quantize_val once the numerics for ARM are aligned with it"
    },
    {
      "file": "atenspace/aten/src/ATen/quantized/Quantizer.cpp",
      "line": 232,
      "type": "TODO",
      "content": "// TODO Make quantize_tensor_arm work for other datatypes too (int8, int32).",
      "context": "// There may be slight accuracy difference between this and implementation of quantize_val\n// TODO Update quantize_tensor_arm implementation to follow quantize_val,\n// i.e. f = Round(value/scale + zero_point)\n// TODO Make quantize_tensor_arm work for other datatypes too (int8, int32).\ntemplate <>\nvoid quantize_tensor_arm<c10::quint8>(\n    const float* in,\n",
      "description": "Make quantize_tensor_arm work for other datatypes too (int8, int32)."
    },
    {
      "file": "atenspace/aten/src/ATen/quantized/Quantizer.cpp",
      "line": 363,
      "type": "TODO",
      "content": "// TODO: add fbgemm for per channel",
      "context": "template CAFFE2_API quint8 requantize_val<qint32, quint8>(double, int64_t, double, int64_t, qint32);\ntemplate CAFFE2_API qint32 requantize_val<qint32, qint32>(double, int64_t, double, int64_t, qint32);\n\n// TODO: add fbgemm for per channel\ntemplate <typename T>\nTensor quantize_tensor_per_channel_affine(Tensor rtensor,\n                                          Tensor qtensor,\n",
      "description": "add fbgemm for per channel"
    },
    {
      "file": "atenspace/aten/src/ATen/quantized/QTensorImpl.h",
      "line": 23,
      "type": "TODO",
      "content": "// TODO: Expose in PyTorch Frontend",
      "context": "      TensorTypeSet type_set,\n      QuantizerPtr quantizer);\n\n  // TODO: Expose in PyTorch Frontend\n  QuantizerPtr quantizer() {\n    return quantizer_;\n  }\n",
      "description": "Expose in PyTorch Frontend"
    },
    {
      "file": "atenspace/aten/src/TH/vector/VSX.cpp",
      "line": 1345,
      "type": "TODO",
      "content": "//        TODO",
      "context": "//    $ gcc VSX.c -O2 -D RUN_VSX_TESTS -o vsxtest\n//    $ ./vsxtest\n//\n//        TODO\n//\n//\n//    Finished running all tests. All tests PASSED.\n",
      "description": ""
    },
    {
      "file": "atenspace/aten/src/TH/generic/THTensor.cpp",
      "line": 622,
      "type": "FIXME",
      "content": "// FIXME: warn if this is the case",
      "context": "  // to cat empty tensors unless all the other tensors were 1-dimensional, so we allowed these tensors\n  // to be \"skipped\".  We maintain this behavior for backwards compatibility, but only for this specific\n  // size (i.e. other empty sizes are not skipped).\n  // FIXME: warn if this is the case\n  bool allSkipped= true;\n  int64_t nDims = 0;\n  THTensor *notSkippedTensor;  // non-owning reference\n",
      "description": "warn if this is the case"
    },
    {
      "file": "atenspace/aten/src/THC/THCGeneral.cpp",
      "line": 175,
      "type": "TODO",
      "content": "// TODO: delete me",
      "context": "  return &(state->resourcesPerDevice[device]);\n}\n\n// TODO: delete me\ncudaStream_t THCState_getCurrentStreamOnDevice(THCState *state, int device) {\n  return at::cuda::getCurrentCUDAStream(device).stream();\n}\n",
      "description": "delete me"
    },
    {
      "file": "atenspace/aten/src/THC/THCGeneral.cpp",
      "line": 180,
      "type": "TODO",
      "content": "// TODO: delete me",
      "context": "  return at::cuda::getCurrentCUDAStream(device).stream();\n}\n\n// TODO: delete me\ncudaStream_t THCState_getCurrentStream(THCState *state) {\n  return at::cuda::getCurrentCUDAStream().stream();\n}\n",
      "description": "delete me"
    },
    {
      "file": "atenspace/aten/src/THC/THCGeneral.hpp",
      "line": 16,
      "type": "TODO",
      "content": "// TODO: Make this statically obvious",
      "context": "  // NB: These allocators (specifically, cudaHostAllocator) MUST implement\n  // maybeGlobalBoundDeleter, because we have a few use-cases where we need to\n  // do raw allocations with them (for Thrust).\n  // TODO: Make this statically obvious\n  at::Allocator* cudaHostAllocator;\n  at::Allocator* cudaDeviceAllocator;\n\n",
      "description": "Make this statically obvious"
    },
    {
      "file": "atenspace/aten/src/THC/generic/THCTensorMasked.h",
      "line": 16,
      "type": "FIXME",
      "content": "// FIXME: remove now that we have THCudaByteTensor?",
      "context": "                                        THCudaBoolTensor *mask,\n                                        scalar_t value);\n\n// FIXME: remove now that we have THCudaByteTensor?\nTHC_API void THCTensor_(maskedFillByte)(THCState *state,\n                                        THCTensor *tensor,\n                                        THByteTensor *mask,\n",
      "description": "remove now that we have THCudaByteTensor?"
    },
    {
      "file": "atenspace/aten/src/THC/generic/THCTensorMasked.h",
      "line": 32,
      "type": "FIXME",
      "content": "// FIXME: remove now that we have THCudaByteTensor?",
      "context": "                                        THCudaBoolTensor *mask,\n                                        THCTensor *src);\n\n// FIXME: remove now that we have THCudaByteTensor?\nTHC_API void THCTensor_(maskedCopyByte)(THCState *state,\n                                        THCTensor *tensor,\n                                        THByteTensor *mask,\n",
      "description": "remove now that we have THCudaByteTensor?"
    },
    {
      "file": "atenspace/aten/src/THC/generic/THCTensorMasked.h",
      "line": 48,
      "type": "FIXME",
      "content": "// FIXME: remove now that we have THCudaByteTensor?",
      "context": "                                          THCTensor *src,\n                                          THCudaBoolTensor *mask);\n\n// FIXME: remove now that we have THCudaByteTensor?\nTHC_API void THCTensor_(maskedSelectByte)(THCState *state,\n                                          THCTensor *tensor,\n                                          THCTensor *src,\n",
      "description": "remove now that we have THCudaByteTensor?"
    },
    {
      "file": "atomspace-restful/lib/zmq/zhelpers.hpp",
      "line": 31,
      "type": "TODO",
      "content": "// todo: package updated zmq.hpp",
      "context": "\n#include <zmq.hpp>\n//#include <lib/zmq/zmq.hpp>\n// todo: package updated zmq.hpp\n\n#include <iostream>\n#include <iomanip>\n",
      "description": "package updated zmq.hpp"
    },
    {
      "file": "atomspace-restful/opencog/python/web/api/utilities.py",
      "line": 17,
      "type": "FIXME",
      "content": "# FIXME: Should this moved to the atomspace repo and be part",
      "context": "# https://github.com/opencog/opencog/pull/2012 and,\n# https://github.com/opencog/atomspace/pull/611\n# NOTE: This is similar to scheme `cog-node`.\n# FIXME: Should this moved to the atomspace repo and be part\n# of opencog.atomspace module?\ndef get_atoms_by_name(z_type, name, atomspace):\n    return filter(lambda x: x.name == name, atomspace.get_atoms_by_type(z_type))\n",
      "description": "Should this moved to the atomspace repo and be part"
    }
  ]
}